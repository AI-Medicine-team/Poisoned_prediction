{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c9a3cd9",
   "metadata": {},
   "source": [
    "## 1. data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b634970",
   "metadata": {},
   "source": [
    "- Replace new data file (categorical variables have been mapped to values and changed to descriptive): updated albumin wash data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ======================\n",
    "# 1. Read Data\n",
    "# ======================\n",
    "df1 = pd.read_csv('/home/mailiyi/Poisoning_Prediction/all_poisoning_data_wide_clean_albumin_20251106.csv')\n",
    "\n",
    "# ======================\n",
    "# 2. Define Features\n",
    "# ======================\n",
    "x_features_continuous = ['Age',\n",
    " 'Length of Stay',\n",
    " 'Weight',\n",
    " 'Systolic Blood Pressure',\n",
    " 'Diastolic Blood Pressure',\n",
    " 'Respiratory Rate', \n",
    " 'Heart Rate',\n",
    " 'White Blood Cell Count',\n",
    " 'Red Blood Cell Count',\n",
    " 'Hemoglobin Concentration',\n",
    " 'Mean Corpuscular Volume',\n",
    " 'Mean Corpuscular Hemoglobin',\n",
    " 'Mean Corpuscular Hemoglobin Concentration',\n",
    " 'Platelet Count',\n",
    " 'Mean Platelet Volume',\n",
    " 'Alanine Aminotransferase (ALT)',\n",
    " 'Total Bilirubin',\n",
    " 'Direct Bilirubin',\n",
    " 'Lactate Dehydrogenase (LDH)',\n",
    " 'Urea',\n",
    " 'Serum Creatinine',\n",
    " 'Uric Acid',\n",
    " 'Creatine Kinase (CK)',\n",
    " 'Creatine Kinase-MB Isoenzyme',\n",
    " 'Troponin I',\n",
    " 'High-Sensitivity C-Reactive Protein (hs-CRP)',\n",
    " 'Homocysteine',\n",
    " 'Potassium',\n",
    " 'Sodium',\n",
    " 'Chloride',\n",
    " 'Carbon Dioxide',\n",
    " 'Prothrombin Time',\n",
    " 'D-Dimer',\n",
    " 'Lactate',\n",
    " 'Blood Cholinesterase Test Results',\n",
    " 'Albumin (First Measurement)',\n",
    " 'Albumin (Last Measurement)',\n",
    " 'Number of Hemoperfusion Sessions',\n",
    " 'Number of Blood Purification Sessions',\n",
    " 'Hyperbaric Oxygen Therapy Duration and Frequency',\n",
    " 'Atropine Dosage',\n",
    " 'Long-acting Nitroglycerin Dosage',\n",
    " 'Pralidoxime Dosage',\n",
    " ] ## Removed features containing \"increasing\"\n",
    "x_features_categorical = ['Gender','Education Level','Type of Poisoning','Hypertension','Hyperlipidemia','Diabetes Mellitus','Cerebrovascular Disease','Heart Disease','Allergy History','Cancer','Poisoning','degree of poisoning','Smoking Status','Alcohol Consumption Status','Shortness of Breath','Chest Pain','Cough','Pre-syncope','Altered Consciousness or Syncope','Sore Throat','Fever','Fatigue','Lower Limb Edema','Palpitations','Vomiting','Nausea','Weakness','Headache','Residence'] # List of categorical variables\n",
    "\n",
    "## Prediction Target: y\n",
    "y_column = 'Outcome_other' ## Whether the patient died\n",
    "\n",
    "# ======================\n",
    "# 3. Shuffle Data\n",
    "# ======================\n",
    "df2 = df1.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# missing_summary = df2[x_features_continuous].isna().sum()\n",
    "# print(\"Continuous features with missing values in the original data:\")\n",
    "# print(missing_summary[missing_summary > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics Distribution of Outcome_other and Outcome\n",
    "print(\"\\n Outcome_other distribution (death or not)\")\n",
    "print(df2[\"Outcome_other\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n Outcome distribution (not cured):\")\n",
    "print(df2[\"Outcome\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d2db7",
   "metadata": {},
   "source": [
    "- Remove variables with deletion rate>90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f1c3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hospital ID',\n",
       " 'Gender',\n",
       " 'Education Level',\n",
       " 'Type of Poisoning',\n",
       " 'Hypertension',\n",
       " 'Hyperlipidemia',\n",
       " 'Diabetes Mellitus',\n",
       " 'Cerebrovascular Disease',\n",
       " 'Heart Disease',\n",
       " 'Allergy History',\n",
       " 'Cancer',\n",
       " 'Poisoning',\n",
       " 'degree of poisoning',\n",
       " 'Smoking Status',\n",
       " 'Alcohol Consumption Status',\n",
       " 'Shortness of Breath',\n",
       " 'Chest Pain',\n",
       " 'Cough',\n",
       " 'Pre-syncope',\n",
       " 'Altered Consciousness or Syncope',\n",
       " 'Sore Throat',\n",
       " 'Fever',\n",
       " 'Fatigue',\n",
       " 'Lower Limb Edema',\n",
       " 'Palpitations',\n",
       " 'Vomiting',\n",
       " 'Nausea',\n",
       " 'Weakness',\n",
       " 'Headache',\n",
       " 'Residence',\n",
       " 'Age',\n",
       " 'Length of Stay',\n",
       " 'Weight',\n",
       " 'Systolic Blood Pressure',\n",
       " 'Diastolic Blood Pressure',\n",
       " 'Respiratory Rate',\n",
       " 'Heart Rate',\n",
       " 'White Blood Cell Count',\n",
       " 'Red Blood Cell Count',\n",
       " 'Hemoglobin Concentration',\n",
       " 'Mean Corpuscular Volume',\n",
       " 'Mean Corpuscular Hemoglobin',\n",
       " 'Mean Corpuscular Hemoglobin Concentration',\n",
       " 'Platelet Count',\n",
       " 'Mean Platelet Volume',\n",
       " 'Alanine Aminotransferase (ALT)',\n",
       " 'Total Bilirubin',\n",
       " 'Direct Bilirubin',\n",
       " 'Lactate Dehydrogenase (LDH)',\n",
       " 'Urea',\n",
       " 'Serum Creatinine',\n",
       " 'Uric Acid',\n",
       " 'Creatine Kinase (CK)',\n",
       " 'Creatine Kinase-MB Isoenzyme',\n",
       " 'Troponin I',\n",
       " 'High-Sensitivity C-Reactive Protein (hs-CRP)',\n",
       " 'Homocysteine',\n",
       " 'Potassium',\n",
       " 'Sodium',\n",
       " 'Chloride',\n",
       " 'Carbon Dioxide',\n",
       " 'Prothrombin Time',\n",
       " 'D-Dimer',\n",
       " 'Lactate',\n",
       " 'Blood Cholinesterase Test Results',\n",
       " 'Albumin (First Measurement)',\n",
       " 'Albumin (Last Measurement)',\n",
       " 'Number of Hemoperfusion Sessions',\n",
       " 'Number of Blood Purification Sessions',\n",
       " 'Hyperbaric Oxygen Therapy Duration and Frequency',\n",
       " 'Atropine Dosage',\n",
       " 'Long-acting Nitroglycerin Dosage',\n",
       " 'Pralidoxime Dosage',\n",
       " 'White Blood Cell Count_increasing',\n",
       " 'Red Blood Cell Count_increasing',\n",
       " 'Hemoglobin Concentration_increasing',\n",
       " 'Mean Corpuscular Volume_increasing',\n",
       " 'Mean Corpuscular Hemoglobin_increasing',\n",
       " 'Mean Corpuscular Hemoglobin Concentration_increasing',\n",
       " 'Platelet Count_increasing',\n",
       " 'Mean Platelet Volume_increasing',\n",
       " 'Alanine Aminotransferase (ALT)_increasing',\n",
       " 'Total Bilirubin_increasing',\n",
       " 'Direct Bilirubin_increasing',\n",
       " 'Lactate Dehydrogenase (LDH)_increasing',\n",
       " 'Urea_increasing',\n",
       " 'Serum Creatinine_increasing',\n",
       " 'Uric Acid_increasing',\n",
       " 'Creatine Kinase (CK)_increasing',\n",
       " 'Creatine Kinase-MB Isoenzyme_increasing',\n",
       " 'Troponin I_increasing',\n",
       " 'High-Sensitivity C-Reactive Protein (hs-CRP)_increasing',\n",
       " 'Potassium_increasing',\n",
       " 'Chloride_increasing',\n",
       " 'Carbon Dioxide_increasing',\n",
       " 'Prothrombin Time_increasing',\n",
       " 'D-Dimer_increasing',\n",
       " 'Lactate_increasing',\n",
       " 'Blood Cholinesterase Test Results_increasing',\n",
       " 'Albumin (Last Measurement)_increasing',\n",
       " 'Number of Blood Purification Sessions_increasing',\n",
       " 'Atropine Dosage_increasing',\n",
       " 'Long-acting Nitroglycerin Dosage_increasing',\n",
       " 'Outcome_other',\n",
       " 'Outcome',\n",
       " 'Outcome_another']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea1a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the missing ratio of continuous variables\n",
    "\n",
    "# Calculate the missing ratio (column-wise)\n",
    "missing_ratios = df2[x_features_continuous+x_features_categorical].isnull().mean()\n",
    "\n",
    "# Convert to percentage and sort (descending order)\n",
    "missing_summary = (missing_ratios * 100).round(2).sort_values(ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Variable missing ratio (%):\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050aa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select feature names with missing rate > 90%\n",
    "high_missing_features = missing_ratios[missing_ratios > 0.90].index.tolist()\n",
    "\n",
    "# Optional: Print these features\n",
    "print(\"Continuous variables with missing rate > 90%:\")\n",
    "for feat in high_missing_features:\n",
    "    print(f\"{feat}: {missing_ratios[feat]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ac19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "11\n",
      "32\n",
      "(971, 95)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_features_continuous))\n",
    "print(len(high_missing_features))\n",
    "x_features_continuous = [feat for feat in x_features_continuous if feat not in high_missing_features]\n",
    "print(len(x_features_continuous))\n",
    "\n",
    "# df2 Remove variables from high_missing_features\n",
    "df2 = df2.drop(columns=high_missing_features)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 4. Fill missing values with the median of all samples\n",
    "# ======================\n",
    "median_values = df2[x_features_continuous].median()\n",
    "df2[x_features_continuous] = df2[x_features_continuous].fillna(median_values)\n",
    "\n",
    "# Check if there are still missing values\n",
    "missing_summary = df2[x_features_continuous].isna().sum()\n",
    "print(\"Continuous features still containing missing values:\")\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# ======================\n",
    "# 5. Normalize (standardize) each column of continuous variables\n",
    "# ======================\n",
    "scaler = StandardScaler()\n",
    "df2[x_features_continuous] = scaler.fit_transform(df2[x_features_continuous])\n",
    "\n",
    "# Optional: Output the mean and standard deviation of each column to confirm normalization\n",
    "check_means = df2[x_features_continuous].mean().round(3)\n",
    "check_stds = df2[x_features_continuous].std().round(3)\n",
    "print(\"\\nMeans and standard deviations of some continuous features after standardization (should be close to 0 and 1):\")\n",
    "print(pd.DataFrame({'mean': check_means.head(10), 'std': check_stds.head(10)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bbe2f",
   "metadata": {},
   "source": [
    "- one-hot: categorical variables are populated first (missing values are populated as Unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, fill missing values in categorical variables with \"Unknown\"\n",
    "for col in x_features_categorical:\n",
    "    if col in df2.columns:\n",
    "        df2[col] = df2[col].fillna('Unknown')\n",
    "\n",
    "# ======================\n",
    "# 6. One-Hot Encoding\n",
    "# ======================\n",
    "x_columns = x_features_categorical + x_features_continuous\n",
    "datax = df2[x_columns]\n",
    "datay = df2[y_column]\n",
    "\n",
    "datax_encoded = pd.get_dummies(datax, columns=x_features_categorical, drop_first=False)\n",
    "\n",
    "# Explicitly convert boolean values to float (True→1.0, False→0.0)\n",
    "datax_encoded = datax_encoded.astype(float)\n",
    "\n",
    "print(f\"\\nOriginal number of features: {len(x_columns)}\")\n",
    "print(f\"Number of features after One-Hot Encoding: {datax_encoded.shape[1]}\")\n",
    "print(f\"Number of samples: {datax_encoded.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9622ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor 形状：X=torch.Size([971, 107]), y=torch.Size([971, 1])\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 7. conversion tensor\n",
    "# ======================\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"\\nTensor form：X={X_tensor.shape}, y={y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7c29a",
   "metadata": {},
   "source": [
    "## 2.1 Model building (50% cross-validation, 20% test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59bd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed fixed as 42\n",
      "Using device: cuda\n",
      "\n",
      "===== Fold 1 =====\n",
      "Random seed fixed as 43\n",
      "Fold 1: pos_weight = 11.12\n",
      "Epoch 001 | Loss: 1.2873 | Test AUROC: 0.7797\n",
      "Epoch 002 | Loss: 0.9911 | Test AUROC: 0.7994\n",
      "Epoch 003 | Loss: 0.7455 | Test AUROC: 0.7847\n",
      "Epoch 004 | Loss: 0.6295 | Test AUROC: 0.7652\n",
      "Epoch 005 | Loss: 0.5492 | Test AUROC: 0.7370\n",
      "Epoch 006 | Loss: 0.4775 | Test AUROC: 0.7335\n",
      "Epoch 007 | Loss: 0.4169 | Test AUROC: 0.7279\n",
      "Epoch 008 | Loss: 0.4615 | Test AUROC: 0.7542\n",
      "Epoch 009 | Loss: 0.4563 | Test AUROC: 0.7483\n",
      "Epoch 010 | Loss: 0.4114 | Test AUROC: 0.7357\n",
      "Early stopping at epoch 10 (best AUROC=0.7994)\n",
      "[Fold 1] Final AUROC: 0.7994, AUPRC: 0.3490\n",
      "\n",
      "===== Fold 2 =====\n",
      "Random seed fixed as 44\n",
      "Fold 2: pos_weight = 10.60\n",
      "Epoch 001 | Loss: 1.2649 | Test AUROC: 0.9181\n",
      "Epoch 002 | Loss: 1.0273 | Test AUROC: 0.9263\n",
      "Epoch 003 | Loss: 0.8260 | Test AUROC: 0.9240\n",
      "Epoch 004 | Loss: 0.7939 | Test AUROC: 0.9151\n",
      "Epoch 005 | Loss: 0.6931 | Test AUROC: 0.9203\n",
      "Epoch 006 | Loss: 0.4978 | Test AUROC: 0.9236\n",
      "Epoch 007 | Loss: 0.4545 | Test AUROC: 0.8994\n",
      "Epoch 008 | Loss: 0.5359 | Test AUROC: 0.8980\n",
      "Epoch 009 | Loss: 0.4133 | Test AUROC: 0.8909\n",
      "Epoch 010 | Loss: 0.4460 | Test AUROC: 0.9274\n",
      "Epoch 011 | Loss: 0.4488 | Test AUROC: 0.8976\n",
      "Epoch 012 | Loss: 0.4479 | Test AUROC: 0.9047\n",
      "Epoch 013 | Loss: 0.4815 | Test AUROC: 0.8976\n",
      "Epoch 014 | Loss: 0.3873 | Test AUROC: 0.9006\n",
      "Epoch 015 | Loss: 0.3846 | Test AUROC: 0.9114\n",
      "Epoch 016 | Loss: 0.3656 | Test AUROC: 0.9091\n",
      "Epoch 017 | Loss: 0.2779 | Test AUROC: 0.9073\n",
      "Epoch 018 | Loss: 0.4782 | Test AUROC: 0.8834\n",
      "Early stopping at epoch 18 (best AUROC=0.9274)\n",
      "[Fold 2] Final AUROC: 0.9274, AUPRC: 0.5900\n",
      "\n",
      "===== Fold 3 =====\n",
      "Random seed fixed as 45\n",
      "Fold 3: pos_weight = 11.53\n",
      "Epoch 001 | Loss: 1.1736 | Test AUROC: 0.8494\n",
      "Epoch 002 | Loss: 0.8367 | Test AUROC: 0.8569\n",
      "Epoch 003 | Loss: 0.7648 | Test AUROC: 0.8592\n",
      "Epoch 004 | Loss: 0.6086 | Test AUROC: 0.8618\n",
      "Epoch 005 | Loss: 0.4732 | Test AUROC: 0.8618\n",
      "Epoch 006 | Loss: 0.4965 | Test AUROC: 0.8425\n",
      "Epoch 007 | Loss: 0.5477 | Test AUROC: 0.8540\n",
      "Epoch 008 | Loss: 0.4697 | Test AUROC: 0.8425\n",
      "Epoch 009 | Loss: 0.3369 | Test AUROC: 0.8497\n",
      "Epoch 010 | Loss: 0.3594 | Test AUROC: 0.8345\n",
      "Epoch 011 | Loss: 0.3753 | Test AUROC: 0.8216\n",
      "Epoch 012 | Loss: 0.4071 | Test AUROC: 0.8218\n",
      "Epoch 013 | Loss: 0.3784 | Test AUROC: 0.8328\n",
      "Early stopping at epoch 13 (best AUROC=0.8618)\n",
      "[Fold 3] Final AUROC: 0.8618, AUPRC: 0.4740\n",
      "\n",
      "===== Fold 4 =====\n",
      "Random seed fixed as 46\n",
      "Fold 4: pos_weight = 10.26\n",
      "Epoch 001 | Loss: 1.2739 | Test AUROC: 0.7182\n",
      "Epoch 002 | Loss: 0.9748 | Test AUROC: 0.7544\n",
      "Epoch 003 | Loss: 0.8061 | Test AUROC: 0.7510\n",
      "Epoch 004 | Loss: 0.6890 | Test AUROC: 0.7590\n",
      "Epoch 005 | Loss: 0.6254 | Test AUROC: 0.7845\n",
      "Epoch 006 | Loss: 0.5254 | Test AUROC: 0.7692\n",
      "Epoch 007 | Loss: 0.4629 | Test AUROC: 0.7646\n",
      "Epoch 008 | Loss: 0.5632 | Test AUROC: 0.7684\n",
      "Epoch 009 | Loss: 0.4779 | Test AUROC: 0.7561\n",
      "Epoch 010 | Loss: 0.4177 | Test AUROC: 0.7777\n",
      "Epoch 011 | Loss: 0.4275 | Test AUROC: 0.7633\n",
      "Epoch 012 | Loss: 0.3077 | Test AUROC: 0.7072\n",
      "Epoch 013 | Loss: 0.3316 | Test AUROC: 0.7569\n",
      "Early stopping at epoch 13 (best AUROC=0.7845)\n",
      "[Fold 4] Final AUROC: 0.7845, AUPRC: 0.2727\n",
      "\n",
      "===== Fold 5 =====\n",
      "Random seed fixed as 47\n",
      "Fold 5: pos_weight = 10.77\n",
      "Epoch 001 | Loss: 1.2152 | Test AUROC: 0.8669\n",
      "Epoch 002 | Loss: 0.9943 | Test AUROC: 0.8834\n",
      "Epoch 003 | Loss: 0.8084 | Test AUROC: 0.9070\n",
      "Epoch 004 | Loss: 0.7020 | Test AUROC: 0.8845\n",
      "Epoch 005 | Loss: 0.6531 | Test AUROC: 0.8687\n",
      "Epoch 006 | Loss: 0.6623 | Test AUROC: 0.8676\n",
      "Epoch 007 | Loss: 0.5153 | Test AUROC: 0.8750\n",
      "Epoch 008 | Loss: 0.5337 | Test AUROC: 0.8722\n",
      "Epoch 009 | Loss: 0.4567 | Test AUROC: 0.8655\n",
      "Epoch 010 | Loss: 0.5499 | Test AUROC: 0.8624\n",
      "Epoch 011 | Loss: 0.4746 | Test AUROC: 0.8659\n",
      "Early stopping at epoch 11 (best AUROC=0.9070)\n",
      "[Fold 5] Final AUROC: 0.9070, AUPRC: 0.4499\n",
      "\n",
      "===== 五折交叉验证结果 =====\n",
      "AUROC: Mean = 0.8560, 95% CI = (0.8005, 0.9115)\n",
      "AUPRC: Mean = 0.4271, 95% CI = (0.3205, 0.5338)\n",
      "\n",
      "✅ 所有折的预测结果已合并保存为：/home/mailiyi/Poisoning_Prediction/DNN/predict_death/all_folds_results.csv\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# =============== 0. Fix random seed to ensure reproducibility ===============\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # Multi-GPU\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure determinism in convolution operations\n",
    "    torch.backends.cudnn.benchmark = False     # Disable automatic optimization algorithms\n",
    "    print(f\"Random seed fixed as {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# =============== 1. Save path ===============\n",
    "save_path = \"/home/mailiyi/Poisoning_Prediction/DNN/predict_death/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# =============== 2. Device selection ===============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============== 3. Data tensorization ===============\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =============== 4. Define DNN model ===============\n",
    "## Remove the Sigmoid() in the last layer and output only logits (raw scores), \n",
    "## because we will use BCEWithLogitsLoss (this function internally includes Sigmoid)\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),  # Slightly increase dropout to enhance generalization\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "            # nn.Sigmoid()  # ❌ Do not add Sigmoid\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# =============== 5. Five-fold cross-validation ===============\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "auroc_list, auprc_list = [], []\n",
    "all_results = []  # ← Used to aggregate prediction results for all patients\n",
    "\n",
    "for train_index, test_index in kf.split(X_tensor):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "    # Fix random seed again (ensure consistent behavior for each fold)\n",
    "    set_seed(42 + fold)\n",
    "\n",
    "    # Data splitting\n",
    "    X_train, X_test = X_tensor[train_index], X_tensor[test_index]\n",
    "    y_train, y_test = y_tensor[train_index], y_tensor[test_index]\n",
    "\n",
    "    # Calculate the number of positive and negative samples\n",
    "    num_pos = (y_train == 1).sum().item()\n",
    "    num_neg = (y_train == 0).sum().item()\n",
    "    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32).to(device)\n",
    "    print(f\"Fold {fold}: pos_weight = {pos_weight:.2f}\")\n",
    "\n",
    "    # DataLoader (reduce batch_size to 16)\n",
    "    # train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "    # DataLoader: Add worker_init_fn to fix randomness for each worker\n",
    "    def worker_init_fn(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "        random.seed(42 + worker_id)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Recommended to set to 0 or less than 4 for full control\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = DNN(input_dim=X_tensor.shape[1]).to(device)\n",
    "    # criterion = nn.BCELoss()\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # ✅ Use class-weighted loss function\n",
    "    \n",
    "    # Add L2 regularization (weight decay)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # =============== 6. Early Stopping settings ===============\n",
    "    patience = 8\n",
    "    best_auroc = 0\n",
    "    wait = 0\n",
    "    best_model_path = os.path.join(save_path, f\"fold{fold}_best_model.pt\")\n",
    "\n",
    "    # =============== 7. Model training ===============\n",
    "    max_epochs = 100\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # === Evaluate AUROC on the validation set at the end of each epoch ===\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # y_pred_prob = model(X_test.to(device)).squeeze().detach().cpu().numpy()\n",
    "            logits = model(X_test.to(device)).squeeze()\n",
    "            y_pred_prob = torch.sigmoid(logits).cpu().numpy()  # ✅ logits → probabilities\n",
    "            y_true = y_test.squeeze().cpu().numpy()\n",
    "            auroc = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | Loss: {avg_loss:.4f} | Test AUROC: {auroc:.4f}\")\n",
    "\n",
    "        # === Early Stopping check ===\n",
    "        if auroc > best_auroc:\n",
    "            best_auroc = auroc\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (best AUROC={best_auroc:.4f})\")\n",
    "                break\n",
    "\n",
    "    # =============== 8. Load the best model for final validation ===============\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # y_pred_prob = model(X_test.to(device)).squeeze().detach().cpu().numpy()\n",
    "        logits = model(X_test.to(device)).squeeze()\n",
    "        y_pred_prob = torch.sigmoid(logits).cpu().numpy()  # ✅ logits → probabilities\n",
    "        y_true = y_test.squeeze().cpu().numpy()\n",
    "\n",
    "        # Calculate AUROC and AUPRC\n",
    "        auroc = roc_auc_score(y_true, y_pred_prob)\n",
    "        auprc = average_precision_score(y_true, y_pred_prob)\n",
    "        auroc_list.append(auroc)\n",
    "        auprc_list.append(auprc)\n",
    "\n",
    "        print(f\"[Fold {fold}] Final AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "        # Save prediction results\n",
    "        result_df = pd.DataFrame({\n",
    "            \"y_test\": y_true,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        })\n",
    "        result_df.to_csv(os.path.join(save_path, f\"fold{fold}_results.csv\"), index=False)\n",
    "\n",
    "        # Aggregate into overall results\n",
    "        all_results.append(result_df)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# =============== 9. Mean and 95% confidence interval ===============\n",
    "def mean_ci(data, confidence=0.95):\n",
    "    arr = np.array(data)\n",
    "    mean = np.mean(arr)\n",
    "    se = np.std(arr, ddof=1) / np.sqrt(len(arr))\n",
    "    h = 1.96 * se\n",
    "    return mean, (mean - h, mean + h)\n",
    "\n",
    "mean_auroc, ci_auroc = mean_ci(auroc_list)\n",
    "mean_auprc, ci_auprc = mean_ci(auprc_list)\n",
    "\n",
    "print(\"\\n===== Five-fold cross-validation results =====\")\n",
    "print(f\"AUROC: Mean = {mean_auroc:.4f}, 95% CI = ({ci_auroc[0]:.4f}, {ci_auroc[1]:.4f})\")\n",
    "print(f\"AUPRC: Mean = {mean_auprc:.4f}, 95% CI = ({ci_auprc[0]:.4f}, {ci_auprc[1]:.4f})\")\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregate results for all patients and save\n",
    "# -----------------------------\n",
    "all_results_df = pd.concat(all_results, axis=0).reset_index(drop=True)\n",
    "all_results_path = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "all_results_df.to_csv(all_results_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Prediction results for all folds have been merged and saved as: {all_results_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988bad5",
   "metadata": {},
   "source": [
    "## 2.2 Build model: 10% cross validation, 10% validation set, 10% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67fe402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# =============== 0. Fix random seed for reproducibility ===============\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # Multi-GPU\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic convolution operations\n",
    "    torch.backends.cudnn.benchmark = False     # Disable automatic optimization algorithms\n",
    "    print(f\"Random seed fixed as {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# =============== 1. Save path ===============\n",
    "save_path = \"/home/mailiyi/Poisoning_Prediction/DNN/predict_death_valid_test/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# =============== 2. Device selection ===============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============== 3. Tensorize data ===============\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =============== 4. Define DNN model ===============\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# =============== 5. 10-fold cross-validation ===============\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "auroc_list, auprc_list = [], []\n",
    "all_results = []\n",
    "\n",
    "for train_val_index, test_index in kf.split(X_tensor):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "    set_seed(42 + fold)\n",
    "\n",
    "    # Initial split: 90% train_val, 10% test\n",
    "    X_train_val, X_test = X_tensor[train_val_index], X_tensor[test_index]\n",
    "    y_train_val, y_test = y_tensor[train_val_index], y_tensor[test_index]\n",
    "\n",
    "    # Further split train_val: 80% train, 10% validation (overall ratio 80/10/10)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=1/9, random_state=42, stratify=y_train_val\n",
    "    )\n",
    "\n",
    "    # Calculate positive and negative sample ratios\n",
    "    num_pos = (y_train == 1).sum().item()\n",
    "    num_neg = (y_train == 0).sum().item()\n",
    "    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32).to(device)\n",
    "    print(f\"Fold {fold}: pos_weight = {pos_weight:.2f}\")\n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "        random.seed(42 + worker_id)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = DNN(input_dim=X_tensor.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # =============== 6. Early Stopping ===============\n",
    "    patience = 8\n",
    "    best_auroc = 0\n",
    "    wait = 0\n",
    "    best_model_path = os.path.join(save_path, f\"fold{fold}_best_model.pt\")\n",
    "\n",
    "    # =============== 7. Model training ===============\n",
    "    max_epochs = 100\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # === Validate AUROC after each epoch ===\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_val.to(device)).squeeze()\n",
    "            y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            y_true = y_val.squeeze().cpu().numpy()\n",
    "            auroc_val = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | Loss: {avg_loss:.4f} | Val AUROC: {auroc_val:.4f}\")\n",
    "\n",
    "        if auroc_val > best_auroc:\n",
    "            best_auroc = auroc_val\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (best Val AUROC={best_auroc:.4f})\")\n",
    "                break\n",
    "\n",
    "    # =============== 8. Load best model and evaluate on test set ===============\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test.to(device)).squeeze()\n",
    "        y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        y_true = y_test.squeeze().cpu().numpy()\n",
    "        auroc = roc_auc_score(y_true, y_pred_prob)\n",
    "        auprc = average_precision_score(y_true, y_pred_prob)\n",
    "        auroc_list.append(auroc)\n",
    "        auprc_list.append(auprc)\n",
    "\n",
    "        print(f\"[Fold {fold}] Test AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "            \"y_test\": y_true,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        })\n",
    "        result_df.to_csv(os.path.join(save_path, f\"fold{fold}_results.csv\"), index=False)\n",
    "        all_results.append(result_df)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# =============== 9. Calculate mean and 95% confidence interval ===============\n",
    "def mean_ci(data, confidence=0.95):\n",
    "    arr = np.array(data)\n",
    "    mean = np.mean(arr)\n",
    "    se = np.std(arr, ddof=1) / np.sqrt(len(arr))\n",
    "    h = 1.96 * se\n",
    "    return mean, (mean - h, mean + h)\n",
    "\n",
    "mean_auroc, ci_auroc = mean_ci(auroc_list)\n",
    "mean_auprc, ci_auprc = mean_ci(auprc_list)\n",
    "\n",
    "print(\"\\n===== 10-Fold Cross-Validation Results =====\")\n",
    "print(f\"AUROC: Mean = {mean_auroc:.4f}, 95% CI = ({ci_auroc[0]:.4f}, {ci_auroc[1]:.4f})\")\n",
    "print(f\"AUPRC: Mean = {mean_auprc:.4f}, 95% CI = ({ci_auprc[0]:.4f}, {ci_auprc[1]:.4f})\")\n",
    "\n",
    "# -----------------------------\n",
    "# Combine all patient prediction results\n",
    "# -----------------------------\n",
    "all_results_df = pd.concat(all_results, axis=0).reset_index(drop=True)\n",
    "all_results_path = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "all_results_df.to_csv(all_results_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ All fold prediction results have been combined and saved to: {all_results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dccbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38701c95",
   "metadata": {},
   "source": [
    "## 2.3. 5-fold cross validation: Divide 1/8 of the training set into validation sets (i.e. 70% training set, 10% validation set, 20% test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f4330",
   "metadata": {},
   "source": [
    "- 2.3.1. Calculate 95% CI and meanAUC with auc verified by 5-fold cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ffa1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# =============== 0. Fix random seed for reproducibility ===============\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # Multi-GPU\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic convolution operations\n",
    "    torch.backends.cudnn.benchmark = False     # Disable automatic optimization algorithms\n",
    "    print(f\"Random seed fixed as {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# =============== 1. Save path ===============\n",
    "save_path = \"/home/mailiyi/Poisoning_Prediction/DNN/predict_death_valid_test_5cv/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# =============== 2. Device selection ===============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============== 3. Data tensorization ===============\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =============== 4. Define DNN model ===============\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# =============== 5. 5-fold cross-validation ===============\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "auroc_list, auprc_list = [], []\n",
    "all_results = []\n",
    "\n",
    "for train_val_index, test_index in kf.split(X_tensor):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "    set_seed(42 + fold)\n",
    "\n",
    "    # Initial split: 80% train_val, 20% test\n",
    "    X_train_val, X_test = X_tensor[train_val_index], X_tensor[test_index]\n",
    "    y_train_val, y_test = y_tensor[train_val_index], y_tensor[test_index]\n",
    "\n",
    "    # Split train_val: training set 7/8, validation set 1/8 (overall ~70/10/20)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=1/8, random_state=42, stratify=y_train_val\n",
    "    )\n",
    "\n",
    "    # Calculate positive and negative sample ratio\n",
    "    num_pos = (y_train == 1).sum().item()\n",
    "    num_neg = (y_train == 0).sum().item()\n",
    "    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32).to(device)\n",
    "    print(f\"Fold {fold}: pos_weight = {pos_weight:.2f}\")\n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "        random.seed(42 + worker_id)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = DNN(input_dim=X_tensor.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # =============== 6. Early Stopping ===============\n",
    "    patience = 8\n",
    "    best_auroc = 0\n",
    "    wait = 0\n",
    "    best_model_path = os.path.join(save_path, f\"fold{fold}_best_model.pt\")\n",
    "\n",
    "    # =============== 7. Model training ===============\n",
    "    max_epochs = 100\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # === Validate AUROC after each epoch ===\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_val.to(device)).squeeze()\n",
    "            y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            y_true = y_val.squeeze().cpu().numpy()\n",
    "            auroc_val = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | Loss: {avg_loss:.4f} | Val AUROC: {auroc_val:.4f}\")\n",
    "\n",
    "        if auroc_val > best_auroc:\n",
    "            best_auroc = auroc_val\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (best Val AUROC={best_auroc:.4f})\")\n",
    "                break\n",
    "\n",
    "    # =============== 8. Load best model and evaluate on test set ===============\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test.to(device)).squeeze()\n",
    "        y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        y_true = y_test.squeeze().cpu().numpy()\n",
    "        auroc = roc_auc_score(y_true, y_pred_prob)\n",
    "        auprc = average_precision_score(y_true, y_pred_prob)\n",
    "        auroc_list.append(auroc)\n",
    "        auprc_list.append(auprc)\n",
    "\n",
    "        print(f\"[Fold {fold}] Test AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "            \"y_test\": y_true,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        })\n",
    "        result_df.to_csv(os.path.join(save_path, f\"fold{fold}_results.csv\"), index=False)\n",
    "        all_results.append(result_df)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# =============== 9. Calculate mean and 95% confidence interval ===============\n",
    "def mean_ci(data, confidence=0.95):\n",
    "    arr = np.array(data)\n",
    "    mean = np.mean(arr)\n",
    "    se = np.std(arr, ddof=1) / np.sqrt(len(arr))\n",
    "    h = 1.96 * se\n",
    "    return mean, (mean - h, mean + h)\n",
    "\n",
    "mean_auroc, ci_auroc = mean_ci(auroc_list)\n",
    "mean_auprc, ci_auprc = mean_ci(auprc_list)\n",
    "\n",
    "print(\"\\n===== 5-Fold Cross-Validation Results =====\")\n",
    "print(f\"AUROC: Mean = {mean_auroc:.4f}, 95% CI = ({ci_auroc[0]:.4f}, {ci_auroc[1]:.4f})\")\n",
    "print(f\"AUPRC: Mean = {mean_auprc:.4f}, 95% CI = ({ci_auprc[0]:.4f}, {ci_auprc[1]:.4f})\")\n",
    "\n",
    "# -----------------------------\n",
    "# Combine all patient prediction results\n",
    "# -----------------------------\n",
    "all_results_df = pd.concat(all_results, axis=0).reset_index(drop=True)\n",
    "all_results_path = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "all_results_df.to_csv(all_results_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ All fold prediction results have been combined and saved to: {all_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928aa95",
   "metadata": {},
   "source": [
    "- 2.3.2. bootstrap approach to calculate 95% confidence intervals for AUROC and AUPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# =============== 0. Fix random seed to ensure reproducibility ===============\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # Multi-GPU\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic convolution operations\n",
    "    torch.backends.cudnn.benchmark = False     # Disable automatic optimization algorithms\n",
    "    print(f\"Random seed fixed as {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# =============== 1. Save path ===============\n",
    "save_path = \"/home/mailiyi/Poisoning_Prediction/DNN/predict_death_valid_test_5cv/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# =============== 2. Device selection ===============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============== 3. Tensorize data ===============\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =============== 4. Define DNN model ===============\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# =============== 5. 5-fold cross-validation ===============\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "auroc_list, auprc_list = [], []\n",
    "all_results = []\n",
    "\n",
    "for train_val_index, test_index in kf.split(X_tensor):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "    set_seed(42 + fold)\n",
    "\n",
    "    # Initial split: 80% train_val, 20% test\n",
    "    X_train_val, X_test = X_tensor[train_val_index], X_tensor[test_index]\n",
    "    y_train_val, y_test = y_tensor[train_val_index], y_tensor[test_index]\n",
    "\n",
    "    # Split train_val: training set 7/8, validation set 1/8 (overall ~70/10/20)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=1/8, random_state=42, stratify=y_train_val\n",
    "    )\n",
    "\n",
    "    # Calculate positive and negative sample ratio\n",
    "    num_pos = (y_train == 1).sum().item()\n",
    "    num_neg = (y_train == 0).sum().item()\n",
    "    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32).to(device)\n",
    "    print(f\"Fold {fold}: pos_weight = {pos_weight:.2f}\")\n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "        random.seed(42 + worker_id)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = DNN(input_dim=X_tensor.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # =============== 6. Early Stopping ===============\n",
    "    patience = 8\n",
    "    best_auroc = 0\n",
    "    wait = 0\n",
    "    best_model_path = os.path.join(save_path, f\"fold{fold}_best_model.pt\")\n",
    "\n",
    "    # =============== 7. Model training ===============\n",
    "    max_epochs = 100\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # === Validate AUROC after each epoch ===\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_val.to(device)).squeeze()\n",
    "            y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            y_true = y_val.squeeze().cpu().numpy()\n",
    "            auroc_val = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | Loss: {avg_loss:.4f} | Val AUROC: {auroc_val:.4f}\")\n",
    "\n",
    "        if auroc_val > best_auroc:\n",
    "            best_auroc = auroc_val\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (best Val AUROC={best_auroc:.4f})\")\n",
    "                break\n",
    "\n",
    "    # =============== 8. Load the best model and evaluate on the test set ===============\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test.to(device)).squeeze()\n",
    "        y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        y_true = y_test.squeeze().cpu().numpy()\n",
    "        auroc = roc_auc_score(y_true, y_pred_prob)\n",
    "        auprc = average_precision_score(y_true, y_pred_prob)\n",
    "        auroc_list.append(auroc)\n",
    "        auprc_list.append(auprc)\n",
    "\n",
    "        print(f\"[Fold {fold}] Test AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "            \"y_test\": y_true,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        })\n",
    "        result_df.to_csv(os.path.join(save_path, f\"fold{fold}_results.csv\"), index=False)\n",
    "        all_results.append(result_df)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# =============== 9. Bootstrap to calculate overall AUROC and AUPRC with 95% CI ===============\n",
    "from sklearn import metrics\n",
    "\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[indices], y_pred[indices]))\n",
    "    mean_score = np.mean(scores)\n",
    "    lower = np.percentile(scores, 2.5)\n",
    "    upper = np.percentile(scores, 97.5)\n",
    "    return mean_score, lower, upper\n",
    "\n",
    "# Combine predictions from all folds\n",
    "all_results_df = pd.concat(all_results, axis=0).reset_index(drop=True)\n",
    "y_all_true = all_results_df[\"y_test\"].values\n",
    "y_all_pred = all_results_df[\"y_pred\"].values\n",
    "\n",
    "# Use bootstrap to calculate overall AUROC and AUPRC with 95% CI\n",
    "mean_auroc, auc_lower, auc_upper = bootstrap_metric_ci(\n",
    "    y_all_true, y_all_pred, metrics.roc_auc_score, n_bootstrap=2000, seed=42\n",
    ")\n",
    "mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "    y_all_true, y_all_pred, metrics.average_precision_score, n_bootstrap=2000, seed=42\n",
    ")\n",
    "\n",
    "print(\"\\n===== 5-Fold Cross-Validation Results (Bootstrap) =====\")\n",
    "print(f\"AUROC: Mean = {mean_auroc:.4f}, 95% CI = ({auc_lower:.4f}, {auc_upper:.4f})\")\n",
    "print(f\"AUPRC: Mean = {mean_auprc:.4f}, 95% CI = ({auprc_lower:.4f}, {auprc_upper:.4f})\")\n",
    "\n",
    "# -----------------------------\n",
    "# Save all patient prediction results\n",
    "# -----------------------------\n",
    "save_path = \"/home/mailiyi/Poisoning_Prediction/DNN/predict_death_valid_test_5cv/\"\n",
    "all_results_path = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "all_results_df.to_csv(all_results_path, index=False)\n",
    "print(f\"\\n✅ All fold prediction results have been merged and saved to: {all_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d584dbd",
   "metadata": {},
   "source": [
    "- 2.3.3. 减少过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b27e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed fixed as 42\n",
      "Using device: cuda\n",
      "\n",
      "===== Fold 1 =====\n",
      "Random seed fixed as 43\n",
      "Fold 1: pos_weight = 11.12\n",
      "Epoch 001 | Loss: 1.2887 | Val AUROC: 0.4115 | LR=0.000500\n",
      "Epoch 002 | Loss: 1.2559 | Val AUROC: 0.6419 | LR=0.000500\n",
      "Epoch 003 | Loss: 1.1981 | Val AUROC: 0.7402 | LR=0.000500\n",
      "Epoch 004 | Loss: 1.1700 | Val AUROC: 0.7711 | LR=0.000500\n",
      "Epoch 005 | Loss: 1.1492 | Val AUROC: 0.8287 | LR=0.000500\n",
      "Epoch 006 | Loss: 1.0729 | Val AUROC: 0.8469 | LR=0.000500\n",
      "Epoch 007 | Loss: 1.0249 | Val AUROC: 0.8539 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.9427 | Val AUROC: 0.8539 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.8930 | Val AUROC: 0.8638 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.8408 | Val AUROC: 0.8652 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.7568 | Val AUROC: 0.8652 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.7245 | Val AUROC: 0.8750 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.6680 | Val AUROC: 0.8750 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.6280 | Val AUROC: 0.8750 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.5854 | Val AUROC: 0.8581 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.6899 | Val AUROC: 0.8680 | LR=0.000250\n",
      "Epoch 017 | Loss: 0.5844 | Val AUROC: 0.8596 | LR=0.000250\n",
      "Epoch 018 | Loss: 0.6414 | Val AUROC: 0.8610 | LR=0.000250\n",
      "Epoch 019 | Loss: 0.5380 | Val AUROC: 0.8624 | LR=0.000250\n",
      "Epoch 020 | Loss: 0.5214 | Val AUROC: 0.8581 | LR=0.000125\n",
      "Epoch 021 | Loss: 0.5080 | Val AUROC: 0.8483 | LR=0.000125\n",
      "Epoch 022 | Loss: 0.4738 | Val AUROC: 0.8567 | LR=0.000125\n",
      "Epoch 023 | Loss: 0.4903 | Val AUROC: 0.8539 | LR=0.000125\n",
      "Epoch 024 | Loss: 0.4882 | Val AUROC: 0.8567 | LR=0.000063\n",
      "Early stopping at epoch 24 (best Val AUROC=0.8750)\n",
      "[Fold 1] Test AUROC: 0.7793, AUPRC: 0.3763\n",
      "\n",
      "===== Fold 2 =====\n",
      "Random seed fixed as 44\n",
      "Fold 2: pos_weight = 10.51\n",
      "Epoch 001 | Loss: 1.2827 | Val AUROC: 0.3778 | LR=0.000500\n",
      "Epoch 002 | Loss: 1.2370 | Val AUROC: 0.5569 | LR=0.000500\n",
      "Epoch 003 | Loss: 1.2238 | Val AUROC: 0.7306 | LR=0.000500\n",
      "Epoch 004 | Loss: 1.1888 | Val AUROC: 0.7681 | LR=0.000500\n",
      "Epoch 005 | Loss: 1.1778 | Val AUROC: 0.8000 | LR=0.000500\n",
      "Epoch 006 | Loss: 1.1549 | Val AUROC: 0.7944 | LR=0.000500\n",
      "Epoch 007 | Loss: 1.0959 | Val AUROC: 0.7958 | LR=0.000500\n",
      "Epoch 008 | Loss: 1.0365 | Val AUROC: 0.7958 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.9947 | Val AUROC: 0.8042 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.9584 | Val AUROC: 0.8111 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.9273 | Val AUROC: 0.8222 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.8593 | Val AUROC: 0.8458 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.8085 | Val AUROC: 0.8514 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.7832 | Val AUROC: 0.8625 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.7728 | Val AUROC: 0.8764 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.6928 | Val AUROC: 0.8806 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.5969 | Val AUROC: 0.8778 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.5926 | Val AUROC: 0.8750 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.6187 | Val AUROC: 0.8847 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.5823 | Val AUROC: 0.8736 | LR=0.000500\n",
      "Epoch 021 | Loss: 0.5908 | Val AUROC: 0.8833 | LR=0.000500\n",
      "Epoch 022 | Loss: 0.5339 | Val AUROC: 0.8681 | LR=0.000500\n",
      "Epoch 023 | Loss: 0.4875 | Val AUROC: 0.8875 | LR=0.000500\n",
      "Epoch 024 | Loss: 0.5208 | Val AUROC: 0.8708 | LR=0.000500\n",
      "Epoch 025 | Loss: 0.5830 | Val AUROC: 0.8556 | LR=0.000500\n",
      "Epoch 026 | Loss: 0.8043 | Val AUROC: 0.8333 | LR=0.000500\n",
      "Epoch 027 | Loss: 0.4575 | Val AUROC: 0.8458 | LR=0.000250\n",
      "Epoch 028 | Loss: 0.4522 | Val AUROC: 0.8444 | LR=0.000250\n",
      "Epoch 029 | Loss: 0.4288 | Val AUROC: 0.8597 | LR=0.000250\n",
      "Epoch 030 | Loss: 0.4563 | Val AUROC: 0.8556 | LR=0.000250\n",
      "Epoch 031 | Loss: 0.4760 | Val AUROC: 0.8639 | LR=0.000125\n",
      "Epoch 032 | Loss: 0.4855 | Val AUROC: 0.8611 | LR=0.000125\n",
      "Epoch 033 | Loss: 0.4439 | Val AUROC: 0.8653 | LR=0.000125\n",
      "Epoch 034 | Loss: 0.4006 | Val AUROC: 0.8639 | LR=0.000125\n",
      "Epoch 035 | Loss: 0.4070 | Val AUROC: 0.8639 | LR=0.000063\n",
      "Early stopping at epoch 35 (best Val AUROC=0.8875)\n",
      "[Fold 2] Test AUROC: 0.9035, AUPRC: 0.5830\n",
      "\n",
      "===== Fold 3 =====\n",
      "Random seed fixed as 45\n",
      "Fold 3: pos_weight = 11.57\n",
      "Epoch 001 | Loss: 1.2857 | Val AUROC: 0.6542 | LR=0.000500\n",
      "Epoch 002 | Loss: 1.2440 | Val AUROC: 0.6875 | LR=0.000500\n",
      "Epoch 003 | Loss: 1.2148 | Val AUROC: 0.7389 | LR=0.000500\n",
      "Epoch 004 | Loss: 1.1685 | Val AUROC: 0.7486 | LR=0.000500\n",
      "Epoch 005 | Loss: 1.1513 | Val AUROC: 0.7431 | LR=0.000500\n",
      "Epoch 006 | Loss: 1.1128 | Val AUROC: 0.7597 | LR=0.000500\n",
      "Epoch 007 | Loss: 1.0983 | Val AUROC: 0.7542 | LR=0.000500\n",
      "Epoch 008 | Loss: 1.0048 | Val AUROC: 0.7639 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.9671 | Val AUROC: 0.7722 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.8851 | Val AUROC: 0.7750 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.8233 | Val AUROC: 0.7875 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.7387 | Val AUROC: 0.8042 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.7227 | Val AUROC: 0.7972 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.7208 | Val AUROC: 0.8097 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.6393 | Val AUROC: 0.8111 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.6046 | Val AUROC: 0.8139 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.6037 | Val AUROC: 0.8056 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.5774 | Val AUROC: 0.7875 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.5537 | Val AUROC: 0.7986 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.5631 | Val AUROC: 0.7806 | LR=0.000250\n",
      "Epoch 021 | Loss: 0.5537 | Val AUROC: 0.7819 | LR=0.000250\n",
      "Epoch 022 | Loss: 0.4880 | Val AUROC: 0.7819 | LR=0.000250\n",
      "Epoch 023 | Loss: 0.5363 | Val AUROC: 0.7806 | LR=0.000250\n",
      "Epoch 024 | Loss: 0.4522 | Val AUROC: 0.7819 | LR=0.000125\n",
      "Epoch 025 | Loss: 0.4917 | Val AUROC: 0.7875 | LR=0.000125\n",
      "Epoch 026 | Loss: 0.4442 | Val AUROC: 0.7903 | LR=0.000125\n",
      "Epoch 027 | Loss: 0.4726 | Val AUROC: 0.7806 | LR=0.000125\n",
      "Epoch 028 | Loss: 0.4897 | Val AUROC: 0.7819 | LR=0.000063\n",
      "Early stopping at epoch 28 (best Val AUROC=0.8139)\n",
      "[Fold 3] Test AUROC: 0.8359, AUPRC: 0.4482\n",
      "\n",
      "===== Fold 4 =====\n",
      "Random seed fixed as 46\n",
      "Fold 4: pos_weight = 10.32\n",
      "Epoch 001 | Loss: 1.2575 | Val AUROC: 0.7890 | LR=0.000500\n",
      "Epoch 002 | Loss: 1.2641 | Val AUROC: 0.8327 | LR=0.000500\n",
      "Epoch 003 | Loss: 1.1722 | Val AUROC: 0.8439 | LR=0.000500\n",
      "Epoch 004 | Loss: 1.1747 | Val AUROC: 0.8564 | LR=0.000500\n",
      "Epoch 005 | Loss: 1.0836 | Val AUROC: 0.8789 | LR=0.000500\n",
      "Epoch 006 | Loss: 1.0610 | Val AUROC: 0.8801 | LR=0.000500\n",
      "Epoch 007 | Loss: 1.0157 | Val AUROC: 0.8702 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.9307 | Val AUROC: 0.8814 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.8903 | Val AUROC: 0.8826 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.8465 | Val AUROC: 0.8826 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.7891 | Val AUROC: 0.8851 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.8449 | Val AUROC: 0.8901 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.7346 | Val AUROC: 0.9014 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.6324 | Val AUROC: 0.8976 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.6846 | Val AUROC: 0.8976 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.6376 | Val AUROC: 0.9026 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.6179 | Val AUROC: 0.9014 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.5291 | Val AUROC: 0.9051 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.5404 | Val AUROC: 0.9051 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.4683 | Val AUROC: 0.8939 | LR=0.000500\n",
      "Epoch 021 | Loss: 0.4705 | Val AUROC: 0.9001 | LR=0.000500\n",
      "Epoch 022 | Loss: 0.4992 | Val AUROC: 0.8964 | LR=0.000250\n",
      "Epoch 023 | Loss: 0.4206 | Val AUROC: 0.8951 | LR=0.000250\n",
      "Epoch 024 | Loss: 0.4602 | Val AUROC: 0.8964 | LR=0.000250\n",
      "Epoch 025 | Loss: 0.4308 | Val AUROC: 0.8926 | LR=0.000250\n",
      "Epoch 026 | Loss: 0.4954 | Val AUROC: 0.8926 | LR=0.000125\n",
      "Epoch 027 | Loss: 0.4122 | Val AUROC: 0.8901 | LR=0.000125\n",
      "Epoch 028 | Loss: 0.4323 | Val AUROC: 0.8926 | LR=0.000125\n",
      "Epoch 029 | Loss: 0.4379 | Val AUROC: 0.8914 | LR=0.000125\n",
      "Epoch 030 | Loss: 0.4599 | Val AUROC: 0.8926 | LR=0.000063\n",
      "Early stopping at epoch 30 (best Val AUROC=0.9051)\n",
      "[Fold 4] Test AUROC: 0.8075, AUPRC: 0.3722\n",
      "\n",
      "===== Fold 5 =====\n",
      "Random seed fixed as 47\n",
      "Fold 5: pos_weight = 10.71\n",
      "Epoch 001 | Loss: 1.3021 | Val AUROC: 0.6917 | LR=0.000500\n",
      "Epoch 002 | Loss: 1.2487 | Val AUROC: 0.6944 | LR=0.000500\n",
      "Epoch 003 | Loss: 1.2151 | Val AUROC: 0.7250 | LR=0.000500\n",
      "Epoch 004 | Loss: 1.1424 | Val AUROC: 0.7292 | LR=0.000500\n",
      "Epoch 005 | Loss: 1.1317 | Val AUROC: 0.7264 | LR=0.000500\n",
      "Epoch 006 | Loss: 1.1136 | Val AUROC: 0.7403 | LR=0.000500\n",
      "Epoch 007 | Loss: 1.0512 | Val AUROC: 0.7403 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.9864 | Val AUROC: 0.7472 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.9545 | Val AUROC: 0.7528 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.8812 | Val AUROC: 0.7569 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.9163 | Val AUROC: 0.7694 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.8706 | Val AUROC: 0.7875 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.7565 | Val AUROC: 0.7847 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.7314 | Val AUROC: 0.7931 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.7059 | Val AUROC: 0.7806 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.7383 | Val AUROC: 0.7792 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.6387 | Val AUROC: 0.7931 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.6699 | Val AUROC: 0.7944 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.6078 | Val AUROC: 0.7889 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.6098 | Val AUROC: 0.7944 | LR=0.000500\n",
      "Epoch 021 | Loss: 0.5471 | Val AUROC: 0.7917 | LR=0.000500\n",
      "Epoch 022 | Loss: 0.5268 | Val AUROC: 0.7889 | LR=0.000250\n",
      "Epoch 023 | Loss: 0.5369 | Val AUROC: 0.7931 | LR=0.000250\n",
      "Epoch 024 | Loss: 0.4609 | Val AUROC: 0.7806 | LR=0.000250\n",
      "Epoch 025 | Loss: 0.6249 | Val AUROC: 0.7903 | LR=0.000250\n",
      "Epoch 026 | Loss: 0.4891 | Val AUROC: 0.7903 | LR=0.000125\n",
      "Epoch 027 | Loss: 0.4751 | Val AUROC: 0.7903 | LR=0.000125\n",
      "Epoch 028 | Loss: 0.5282 | Val AUROC: 0.7847 | LR=0.000125\n",
      "Epoch 029 | Loss: 0.4843 | Val AUROC: 0.7847 | LR=0.000125\n",
      "Epoch 030 | Loss: 0.4375 | Val AUROC: 0.7861 | LR=0.000063\n",
      "Early stopping at epoch 30 (best Val AUROC=0.7944)\n",
      "[Fold 5] Test AUROC: 0.8961, AUPRC: 0.4266\n",
      "\n",
      "===== 五折交叉验证结果 (Bootstrap) =====\n",
      "AUROC: Mean = 0.8404, 95% CI = (0.7888, 0.8857)\n",
      "AUPRC: Mean = 0.4190, 95% CI = (0.3151, 0.5219)\n",
      "\n",
      "✅ 所有折的预测结果已合并保存为：/home/mailiyi/Poisoning_Prediction/DNN/predict_death_valid_test_5cv/all_folds_results.csv\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn import metrics\n",
    "\n",
    "# =============== 0. Fix random seed ===============\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed fixed as {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# =============== 1. Save path ===============\n",
    "save_path = \"/home/mailiyi/Poisoning_Prediction/DNN/predict_death_valid_test_5cv/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# =============== 2. Device selection ===============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============== 3. Tensorize data ===============\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =============== 4. Improved DNN model ===============\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# =============== 5. 5-fold cross-validation ===============\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "auroc_list, auprc_list = [], []\n",
    "all_results = []\n",
    "\n",
    "for train_val_index, test_index in kf.split(X_tensor):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "    set_seed(42 + fold)\n",
    "\n",
    "    # Split data\n",
    "    X_train_val, X_test = X_tensor[train_val_index], X_tensor[test_index]\n",
    "    y_train_val, y_test = y_tensor[train_val_index], y_tensor[test_index]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=1/8, random_state=42 + fold, stratify=y_train_val\n",
    "    )\n",
    "\n",
    "    # Class weights\n",
    "    num_pos = (y_train == 1).sum().item()\n",
    "    num_neg = (y_train == 0).sum().item()\n",
    "    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32).to(device)\n",
    "    print(f\"Fold {fold}: pos_weight = {pos_weight:.2f}\")\n",
    "\n",
    "    # DataLoader\n",
    "    def worker_init_fn(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "        random.seed(42 + worker_id)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # Model, optimizer, scheduler, loss function\n",
    "    model = DNN(input_dim=X_tensor.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    patience = 12\n",
    "    best_auroc = 0\n",
    "    wait = 0\n",
    "    best_model_path = os.path.join(save_path, f\"fold{fold}_best_model.pt\")\n",
    "\n",
    "    # =============== 6. Training ===============\n",
    "    max_epochs = 100\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_val.to(device)).squeeze()\n",
    "            y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            y_true = y_val.squeeze().cpu().numpy()\n",
    "            auroc_val = roc_auc_score(y_true, y_pred_prob)\n",
    "        scheduler.step(auroc_val)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | Loss: {avg_loss:.4f} | Val AUROC: {auroc_val:.4f} | LR={current_lr:.6f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if auroc_val > best_auroc:\n",
    "            best_auroc = auroc_val\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (best Val AUROC={best_auroc:.4f})\")\n",
    "                break\n",
    "\n",
    "    # =============== 7. Test set evaluation ===============\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test.to(device)).squeeze()\n",
    "        y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        y_true = y_test.squeeze().cpu().numpy()\n",
    "        auroc = roc_auc_score(y_true, y_pred_prob)\n",
    "        auprc = average_precision_score(y_true, y_pred_prob)\n",
    "        auroc_list.append(auroc)\n",
    "        auprc_list.append(auprc)\n",
    "\n",
    "        print(f\"[Fold {fold}] Test AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "        result_df = pd.DataFrame({\"y_test\": y_true, \"y_pred\": y_pred_prob})\n",
    "        result_df.to_csv(os.path.join(save_path, f\"fold{fold}_results.csv\"), index=False)\n",
    "        all_results.append(result_df)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# =============== 8. Bootstrap to calculate overall metrics ===============\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "all_results_df = pd.concat(all_results, axis=0).reset_index(drop=True)\n",
    "y_all_true = all_results_df[\"y_test\"].values\n",
    "y_all_pred = all_results_df[\"y_pred\"].values\n",
    "\n",
    "mean_auroc, auc_lower, auc_upper = bootstrap_metric_ci(y_all_true, y_all_pred, metrics.roc_auc_score)\n",
    "mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(y_all_true, y_all_pred, metrics.average_precision_score)\n",
    "\n",
    "print(\"\\n===== 5-fold cross-validation results (Bootstrap) =====\")\n",
    "print(f\"AUROC: Mean = {mean_auroc:.4f}, 95% CI = ({auc_lower:.4f}, {auc_upper:.4f})\")\n",
    "print(f\"AUPRC: Mean = {mean_auprc:.4f}, 95% CI = ({auprc_lower:.4f}, {auprc_upper:.4f})\")\n",
    "\n",
    "# =============== 9. Save all prediction results ===============\n",
    "all_results_path = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "all_results_df.to_csv(all_results_path, index=False)\n",
    "print(f\"\\n✅ All fold prediction results have been merged and saved to: {all_results_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907e6ec",
   "metadata": {},
   "source": [
    "### finally used 5 CV (70% train, 10% valid, 20% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 增加校准模型，概率校准 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC (bootstrap): 0.4777 (0.4019-0.5507)\n",
      "AUPRC: 0.2562 (0.0924-0.4200)\n",
      "Brier score: 0.0774 (0.0673-0.0874)\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from sklearn.model_selection import KFold, train_test_split\n",
    "# from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import random\n",
    "# from scipy.optimize import minimize\n",
    "\n",
    "# # ===================== 固定随机种子 =====================\n",
    "# def set_seed(seed=42):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set_seed(42)\n",
    "\n",
    "# # ===================== DNN 模型 =====================\n",
    "# class DNN(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(DNN, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 128),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.Linear(32, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# # ===================== Platt scaling（sigmoid 校准） =====================\n",
    "# def platt_calibration(y_val, logits_val):\n",
    "#     \"\"\"\n",
    "#     使用 logits 拟合 Platt scaling（sigmoid 校准），返回校准函数\n",
    "#     \"\"\"\n",
    "#     def sigmoid_loss(params, y_true, logits):\n",
    "#         a, b = params\n",
    "#         p = 1 / (1 + np.exp(-(a * logits + b)))\n",
    "#         eps = 1e-15\n",
    "#         loss = -np.mean(y_true * np.log(p + eps) + (1 - y_true) * np.log(1 - p + eps))\n",
    "#         return loss\n",
    "\n",
    "#     res = minimize(sigmoid_loss, x0=[1.0, 0.0], args=(y_val, logits_val), method='BFGS')\n",
    "#     a, b = res.x\n",
    "#     return lambda logits: 1 / (1 + np.exp(-(a * logits + b)))\n",
    "\n",
    "# # ===================== Bootstrap AUROC =====================\n",
    "# def bootstrap_metric_ci(y_true, y_pred, metric_fn=roc_auc_score, n_bootstrap=2000, seed=42):\n",
    "#     rng = np.random.RandomState(seed)\n",
    "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "#     scores = []\n",
    "#     for _ in range(n_bootstrap):\n",
    "#         idx = rng.randint(0, len(y_true), len(y_true))\n",
    "#         if len(np.unique(y_true[idx])) < 2:\n",
    "#             continue\n",
    "#         scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "#     return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "# # ===================== DNN 五折训练 + Platt 校准 =====================\n",
    "# def train_dnn_5fold_platt(dataX, dataY, save_path='/home/mailiyi/Poisoning_Prediction/DNN/predict_death_calibration/', seed=42):\n",
    "#     os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "#     X_tensor = torch.tensor(dataX.values, dtype=torch.float32)\n",
    "#     y_tensor = torch.tensor(dataY.values, dtype=torch.float32).unsqueeze(1)\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     input_dim = X_tensor.shape[1]\n",
    "\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "#     all_results = []\n",
    "#     auprc_list, brier_list = [], []\n",
    "\n",
    "#     fold = 1\n",
    "#     for train_val_idx, test_idx in kf.split(X_tensor):\n",
    "#         set_seed(seed + fold)\n",
    "#         X_train_val, X_test = X_tensor[train_val_idx], X_tensor[test_idx]\n",
    "#         y_train_val, y_test = y_tensor[train_val_idx], y_tensor[test_idx]\n",
    "\n",
    "#         # 拆分验证集\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(\n",
    "#             X_train_val, y_train_val, test_size=1/8, stratify=y_train_val, random_state=seed+fold\n",
    "#         )\n",
    "\n",
    "#         # 类别权重\n",
    "#         num_pos = (y_train == 1).sum().item()\n",
    "#         num_neg = (y_train == 0).sum().item()\n",
    "#         pos_weight = torch.tensor(num_neg / max(num_pos,1), dtype=torch.float32).to(device)\n",
    "\n",
    "#         # DataLoader\n",
    "#         train_loader = DataLoader(\n",
    "#             TensorDataset(X_train, y_train),\n",
    "#             batch_size=32,\n",
    "#             shuffle=True\n",
    "#         )\n",
    "\n",
    "#         # 模型\n",
    "#         model = DNN(input_dim=input_dim).to(device)\n",
    "#         criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-4)\n",
    "#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "#         # Early stopping\n",
    "#         patience = 12\n",
    "#         best_auroc = 0\n",
    "#         wait = 0\n",
    "#         best_model_path = os.path.join(save_path, f\"fold{fold}_best_model.pt\")\n",
    "\n",
    "#         # ===================== 训练 =====================\n",
    "#         for epoch in range(100):\n",
    "#             model.train()\n",
    "#             total_loss = 0\n",
    "#             for batch_X, batch_y in train_loader:\n",
    "#                 batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = model(batch_X)\n",
    "#                 loss = criterion(outputs, batch_y)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 total_loss += loss.item()\n",
    "#             avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "#             # 验证集 AUROC\n",
    "#             model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 logits_val = model(X_val.to(device)).squeeze()\n",
    "#                 y_val_logits = logits_val.cpu().numpy()\n",
    "#                 y_val_true = y_val.cpu().numpy()\n",
    "#                 auroc_val = roc_auc_score(y_val_true, 1 / (1 + np.exp(-y_val_logits)))\n",
    "\n",
    "#             scheduler.step(auroc_val)\n",
    "\n",
    "#             # Early stopping\n",
    "#             if auroc_val > best_auroc:\n",
    "#                 best_auroc = auroc_val\n",
    "#                 wait = 0\n",
    "#                 torch.save(model.state_dict(), best_model_path)\n",
    "#             else:\n",
    "#                 wait += 1\n",
    "#                 if wait >= patience:\n",
    "#                     break\n",
    "\n",
    "#         # 加载最优模型\n",
    "#         model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "#         model.eval()\n",
    "\n",
    "#         # ===================== Platt 校准 =====================\n",
    "#         with torch.no_grad():\n",
    "#             logits_val = model(X_val.to(device)).squeeze().cpu().numpy()\n",
    "#             y_val_true = y_val.cpu().numpy()\n",
    "#         sigmoid_fn = platt_calibration(y_val_true, logits_val)\n",
    "\n",
    "#         # ===================== 测试集预测 + 校准 =====================\n",
    "#         with torch.no_grad():\n",
    "#             logits_test = model(X_test.to(device)).squeeze().cpu().numpy()\n",
    "#             y_test_pred_calib = sigmoid_fn(logits_test)\n",
    "#             y_test_true = y_test.cpu().numpy().squeeze()\n",
    "\n",
    "#         # 指标\n",
    "#         auprc_list.append(average_precision_score(y_test_true, y_test_pred_calib))\n",
    "#         brier_list.append(brier_score_loss(y_test_true, y_test_pred_calib))\n",
    "\n",
    "#         # 保存折结果\n",
    "#         fold_df = pd.DataFrame({\"y_test\": y_test_true, \"y_pred\": y_test_pred_calib})\n",
    "#         fold_df.to_csv(os.path.join(save_path, f\"fold{fold}_results.csv\"), index=False)\n",
    "#         all_results.append(fold_df)\n",
    "\n",
    "#         fold += 1\n",
    "\n",
    "#     # ===================== 合并所有折 =====================\n",
    "#     all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "#     all_results_df.to_csv(os.path.join(save_path, \"all_folds_results.csv\"), index=False)\n",
    "\n",
    "#     # ===================== Bootstrap AUROC =====================\n",
    "#     mean_auroc, auroc_l, auroc_u = bootstrap_metric_ci(all_results_df[\"y_test\"], all_results_df[\"y_pred\"], roc_auc_score)\n",
    "\n",
    "#     # AUPRC 和 Brier 分数\n",
    "#     def mean_ci(x):\n",
    "#         mean = np.mean(x)\n",
    "#         ci = 1.96 * np.std(x, ddof=1) / np.sqrt(len(x))\n",
    "#         return mean, mean - ci, mean + ci\n",
    "\n",
    "#     mean_auprc, auprc_l, auprc_u = mean_ci(auprc_list)\n",
    "#     mean_brier, brier_l, brier_u = mean_ci(brier_list)\n",
    "\n",
    "#     print(f\"AUROC (bootstrap): {mean_auroc:.4f} ({auroc_l:.4f}-{auroc_u:.4f})\")\n",
    "#     print(f\"AUPRC: {mean_auprc:.4f} ({auprc_l:.4f}-{auprc_u:.4f})\")\n",
    "#     print(f\"Brier score: {mean_brier:.4f} ({brier_l:.4f}-{brier_u:.4f})\")\n",
    "\n",
    "#     return {\n",
    "#         \"AUROC_mean\": mean_auroc, \"AUROC_CI\": (auroc_l, auroc_u),\n",
    "#         \"AUPRC_mean\": mean_auprc, \"AUPRC_CI\": (auprc_l, auprc_u),\n",
    "#         \"Brier_mean\": mean_brier, \"Brier_CI\": (brier_l, brier_u),\n",
    "#         \"AllResults\": all_results_df\n",
    "#     }\n",
    "\n",
    "# # ===================== 使用示例 =====================\n",
    "# results = train_dnn_5fold_platt(datax_encoded, datay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6fea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed fixed as 42\n",
      "Using device: cuda\n",
      "\n",
      "===== Fold 1 =====\n",
      "Random seed fixed as 43\n",
      "Epoch 001 | Loss: 0.6937 | Val AUROC: 0.2430 | LR=0.000500\n",
      "Epoch 002 | Loss: 0.5409 | Val AUROC: 0.3315 | LR=0.000500\n",
      "Epoch 003 | Loss: 0.4296 | Val AUROC: 0.4298 | LR=0.000500\n",
      "Epoch 004 | Loss: 0.3486 | Val AUROC: 0.5281 | LR=0.000500\n",
      "Epoch 005 | Loss: 0.2919 | Val AUROC: 0.6404 | LR=0.000500\n",
      "Epoch 006 | Loss: 0.2752 | Val AUROC: 0.7135 | LR=0.000500\n",
      "Epoch 007 | Loss: 0.2385 | Val AUROC: 0.7458 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.2313 | Val AUROC: 0.7921 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.2161 | Val AUROC: 0.8244 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.2032 | Val AUROC: 0.8497 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.1952 | Val AUROC: 0.8581 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.1904 | Val AUROC: 0.8708 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.1696 | Val AUROC: 0.8778 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.1621 | Val AUROC: 0.8876 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.1679 | Val AUROC: 0.8834 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.1881 | Val AUROC: 0.8848 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.1552 | Val AUROC: 0.8834 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.1657 | Val AUROC: 0.8862 | LR=0.000250\n",
      "Epoch 019 | Loss: 0.1419 | Val AUROC: 0.8933 | LR=0.000250\n",
      "Epoch 020 | Loss: 0.1543 | Val AUROC: 0.8890 | LR=0.000250\n",
      "Epoch 021 | Loss: 0.1351 | Val AUROC: 0.8919 | LR=0.000250\n",
      "Epoch 022 | Loss: 0.1335 | Val AUROC: 0.9073 | LR=0.000250\n",
      "Epoch 023 | Loss: 0.1322 | Val AUROC: 0.9031 | LR=0.000250\n",
      "Epoch 024 | Loss: 0.1366 | Val AUROC: 0.8975 | LR=0.000250\n",
      "Epoch 025 | Loss: 0.1285 | Val AUROC: 0.9031 | LR=0.000250\n",
      "Epoch 026 | Loss: 0.1415 | Val AUROC: 0.8933 | LR=0.000125\n",
      "Epoch 027 | Loss: 0.1251 | Val AUROC: 0.8989 | LR=0.000125\n",
      "Epoch 028 | Loss: 0.1447 | Val AUROC: 0.8947 | LR=0.000125\n",
      "Epoch 029 | Loss: 0.1298 | Val AUROC: 0.8919 | LR=0.000125\n",
      "Epoch 030 | Loss: 0.1240 | Val AUROC: 0.8989 | LR=0.000063\n",
      "Epoch 031 | Loss: 0.1166 | Val AUROC: 0.9017 | LR=0.000063\n",
      "Epoch 032 | Loss: 0.1263 | Val AUROC: 0.9017 | LR=0.000063\n",
      "Epoch 033 | Loss: 0.1246 | Val AUROC: 0.8989 | LR=0.000063\n",
      "Epoch 034 | Loss: 0.1138 | Val AUROC: 0.8961 | LR=0.000031\n",
      "Early stopping at epoch 34 (best Val AUROC=0.9073)\n",
      "[Fold 1] Test AUROC: 0.7552, AUPRC: 0.3313\n",
      "\n",
      "===== Fold 2 =====\n",
      "Random seed fixed as 44\n",
      "Epoch 001 | Loss: 0.5610 | Val AUROC: 0.2875 | LR=0.000500\n",
      "Epoch 002 | Loss: 0.4548 | Val AUROC: 0.4778 | LR=0.000500\n",
      "Epoch 003 | Loss: 0.3777 | Val AUROC: 0.6153 | LR=0.000500\n",
      "Epoch 004 | Loss: 0.3304 | Val AUROC: 0.7583 | LR=0.000500\n",
      "Epoch 005 | Loss: 0.2891 | Val AUROC: 0.8222 | LR=0.000500\n",
      "Epoch 006 | Loss: 0.2686 | Val AUROC: 0.8403 | LR=0.000500\n",
      "Epoch 007 | Loss: 0.2456 | Val AUROC: 0.8722 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.2300 | Val AUROC: 0.8792 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.2393 | Val AUROC: 0.8875 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.2234 | Val AUROC: 0.8861 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.2068 | Val AUROC: 0.8889 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.1968 | Val AUROC: 0.8917 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.1911 | Val AUROC: 0.8958 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.1894 | Val AUROC: 0.8889 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.1856 | Val AUROC: 0.8944 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.1857 | Val AUROC: 0.9000 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.1702 | Val AUROC: 0.8917 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.1703 | Val AUROC: 0.8847 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.1704 | Val AUROC: 0.8889 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.1542 | Val AUROC: 0.8778 | LR=0.000250\n",
      "Epoch 021 | Loss: 0.1658 | Val AUROC: 0.8750 | LR=0.000250\n",
      "Epoch 022 | Loss: 0.1607 | Val AUROC: 0.8847 | LR=0.000250\n",
      "Epoch 023 | Loss: 0.1531 | Val AUROC: 0.8875 | LR=0.000250\n",
      "Epoch 024 | Loss: 0.1612 | Val AUROC: 0.8833 | LR=0.000125\n",
      "Epoch 025 | Loss: 0.1915 | Val AUROC: 0.8875 | LR=0.000125\n",
      "Epoch 026 | Loss: 0.1721 | Val AUROC: 0.8694 | LR=0.000125\n",
      "Epoch 027 | Loss: 0.1403 | Val AUROC: 0.8736 | LR=0.000125\n",
      "Epoch 028 | Loss: 0.1569 | Val AUROC: 0.8750 | LR=0.000063\n",
      "Early stopping at epoch 28 (best Val AUROC=0.9000)\n",
      "[Fold 2] Test AUROC: 0.8924, AUPRC: 0.5918\n",
      "\n",
      "===== Fold 3 =====\n",
      "Random seed fixed as 45\n",
      "Epoch 001 | Loss: 0.5758 | Val AUROC: 0.5486 | LR=0.000500\n",
      "Epoch 002 | Loss: 0.4598 | Val AUROC: 0.5736 | LR=0.000500\n",
      "Epoch 003 | Loss: 0.3650 | Val AUROC: 0.6014 | LR=0.000500\n",
      "Epoch 004 | Loss: 0.2992 | Val AUROC: 0.6611 | LR=0.000500\n",
      "Epoch 005 | Loss: 0.2643 | Val AUROC: 0.7000 | LR=0.000500\n",
      "Epoch 006 | Loss: 0.2422 | Val AUROC: 0.7264 | LR=0.000500\n",
      "Epoch 007 | Loss: 0.2426 | Val AUROC: 0.7403 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.2162 | Val AUROC: 0.7417 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.2250 | Val AUROC: 0.7597 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.2029 | Val AUROC: 0.7611 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.1900 | Val AUROC: 0.7764 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.1706 | Val AUROC: 0.7764 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.1853 | Val AUROC: 0.7639 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.1850 | Val AUROC: 0.7833 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.1679 | Val AUROC: 0.7903 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.1544 | Val AUROC: 0.7931 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.1561 | Val AUROC: 0.8000 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.1551 | Val AUROC: 0.7806 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.1595 | Val AUROC: 0.7861 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.1456 | Val AUROC: 0.7764 | LR=0.000500\n",
      "Epoch 021 | Loss: 0.1545 | Val AUROC: 0.7889 | LR=0.000250\n",
      "Epoch 022 | Loss: 0.1362 | Val AUROC: 0.7847 | LR=0.000250\n",
      "Epoch 023 | Loss: 0.1438 | Val AUROC: 0.7792 | LR=0.000250\n",
      "Epoch 024 | Loss: 0.1262 | Val AUROC: 0.7764 | LR=0.000250\n",
      "Epoch 025 | Loss: 0.1466 | Val AUROC: 0.7861 | LR=0.000125\n",
      "Epoch 026 | Loss: 0.1274 | Val AUROC: 0.7792 | LR=0.000125\n",
      "Epoch 027 | Loss: 0.1294 | Val AUROC: 0.7806 | LR=0.000125\n",
      "Epoch 028 | Loss: 0.1286 | Val AUROC: 0.7792 | LR=0.000125\n",
      "Epoch 029 | Loss: 0.1388 | Val AUROC: 0.7778 | LR=0.000063\n",
      "Early stopping at epoch 29 (best Val AUROC=0.8000)\n",
      "[Fold 3] Test AUROC: 0.8132, AUPRC: 0.4600\n",
      "\n",
      "===== Fold 4 =====\n",
      "Random seed fixed as 46\n",
      "Epoch 001 | Loss: 0.5122 | Val AUROC: 0.7528 | LR=0.000500\n",
      "Epoch 002 | Loss: 0.4087 | Val AUROC: 0.7953 | LR=0.000500\n",
      "Epoch 003 | Loss: 0.3331 | Val AUROC: 0.8115 | LR=0.000500\n",
      "Epoch 004 | Loss: 0.2822 | Val AUROC: 0.8190 | LR=0.000500\n",
      "Epoch 005 | Loss: 0.2493 | Val AUROC: 0.8240 | LR=0.000500\n",
      "Epoch 006 | Loss: 0.2461 | Val AUROC: 0.8215 | LR=0.000500\n",
      "Epoch 007 | Loss: 0.2301 | Val AUROC: 0.8252 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.2092 | Val AUROC: 0.8302 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.2157 | Val AUROC: 0.8477 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.2023 | Val AUROC: 0.8477 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.1887 | Val AUROC: 0.8602 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.2065 | Val AUROC: 0.8689 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.1789 | Val AUROC: 0.8727 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.1590 | Val AUROC: 0.8764 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.1780 | Val AUROC: 0.8826 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.1746 | Val AUROC: 0.8839 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.1587 | Val AUROC: 0.8851 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.1557 | Val AUROC: 0.8801 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.1503 | Val AUROC: 0.8839 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.1314 | Val AUROC: 0.8851 | LR=0.000500\n",
      "Epoch 021 | Loss: 0.1442 | Val AUROC: 0.8901 | LR=0.000500\n",
      "Epoch 022 | Loss: 0.1426 | Val AUROC: 0.8976 | LR=0.000500\n",
      "Epoch 023 | Loss: 0.1314 | Val AUROC: 0.8864 | LR=0.000500\n",
      "Epoch 024 | Loss: 0.1226 | Val AUROC: 0.8976 | LR=0.000500\n",
      "Epoch 025 | Loss: 0.1241 | Val AUROC: 0.8839 | LR=0.000500\n",
      "Epoch 026 | Loss: 0.1353 | Val AUROC: 0.8826 | LR=0.000250\n",
      "Epoch 027 | Loss: 0.1196 | Val AUROC: 0.8801 | LR=0.000250\n",
      "Epoch 028 | Loss: 0.1123 | Val AUROC: 0.8777 | LR=0.000250\n",
      "Epoch 029 | Loss: 0.1201 | Val AUROC: 0.8764 | LR=0.000250\n",
      "Epoch 030 | Loss: 0.1238 | Val AUROC: 0.8764 | LR=0.000125\n",
      "Epoch 031 | Loss: 0.1102 | Val AUROC: 0.8702 | LR=0.000125\n",
      "Epoch 032 | Loss: 0.1246 | Val AUROC: 0.8764 | LR=0.000125\n",
      "Epoch 033 | Loss: 0.1065 | Val AUROC: 0.8764 | LR=0.000125\n",
      "Epoch 034 | Loss: 0.1169 | Val AUROC: 0.8839 | LR=0.000063\n",
      "Early stopping at epoch 34 (best Val AUROC=0.8976)\n",
      "[Fold 4] Test AUROC: 0.8100, AUPRC: 0.2988\n",
      "\n",
      "===== Fold 5 =====\n",
      "Random seed fixed as 47\n",
      "Epoch 001 | Loss: 0.6400 | Val AUROC: 0.4111 | LR=0.000500\n",
      "Epoch 002 | Loss: 0.5041 | Val AUROC: 0.4653 | LR=0.000500\n",
      "Epoch 003 | Loss: 0.4048 | Val AUROC: 0.5542 | LR=0.000500\n",
      "Epoch 004 | Loss: 0.3295 | Val AUROC: 0.6417 | LR=0.000500\n",
      "Epoch 005 | Loss: 0.2998 | Val AUROC: 0.7056 | LR=0.000500\n",
      "Epoch 006 | Loss: 0.2694 | Val AUROC: 0.7139 | LR=0.000500\n",
      "Epoch 007 | Loss: 0.2458 | Val AUROC: 0.7208 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.2402 | Val AUROC: 0.7361 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.2201 | Val AUROC: 0.7458 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.2086 | Val AUROC: 0.7514 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.2244 | Val AUROC: 0.7611 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.2013 | Val AUROC: 0.7750 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.1891 | Val AUROC: 0.7750 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.1902 | Val AUROC: 0.7708 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.1776 | Val AUROC: 0.7667 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.2091 | Val AUROC: 0.7667 | LR=0.000250\n",
      "Epoch 017 | Loss: 0.1661 | Val AUROC: 0.7722 | LR=0.000250\n",
      "Epoch 018 | Loss: 0.1672 | Val AUROC: 0.7750 | LR=0.000250\n",
      "Epoch 019 | Loss: 0.1673 | Val AUROC: 0.7750 | LR=0.000250\n",
      "Epoch 020 | Loss: 0.1680 | Val AUROC: 0.7833 | LR=0.000250\n",
      "Epoch 021 | Loss: 0.1658 | Val AUROC: 0.7875 | LR=0.000250\n",
      "Epoch 022 | Loss: 0.1618 | Val AUROC: 0.7861 | LR=0.000250\n",
      "Epoch 023 | Loss: 0.1757 | Val AUROC: 0.7806 | LR=0.000250\n",
      "Epoch 024 | Loss: 0.1525 | Val AUROC: 0.7792 | LR=0.000250\n",
      "Epoch 025 | Loss: 0.1876 | Val AUROC: 0.7917 | LR=0.000250\n",
      "Epoch 026 | Loss: 0.1621 | Val AUROC: 0.7889 | LR=0.000250\n",
      "Epoch 027 | Loss: 0.1411 | Val AUROC: 0.7889 | LR=0.000250\n",
      "Epoch 028 | Loss: 0.1562 | Val AUROC: 0.7792 | LR=0.000250\n",
      "Epoch 029 | Loss: 0.1403 | Val AUROC: 0.7819 | LR=0.000125\n",
      "Epoch 030 | Loss: 0.1624 | Val AUROC: 0.7875 | LR=0.000125\n",
      "Epoch 031 | Loss: 0.1426 | Val AUROC: 0.7806 | LR=0.000125\n",
      "Epoch 032 | Loss: 0.1363 | Val AUROC: 0.7778 | LR=0.000125\n",
      "Epoch 033 | Loss: 0.1400 | Val AUROC: 0.7861 | LR=0.000063\n",
      "Epoch 034 | Loss: 0.1486 | Val AUROC: 0.7792 | LR=0.000063\n",
      "Epoch 035 | Loss: 0.1493 | Val AUROC: 0.7736 | LR=0.000063\n",
      "Epoch 036 | Loss: 0.1497 | Val AUROC: 0.7861 | LR=0.000063\n",
      "Epoch 037 | Loss: 0.1592 | Val AUROC: 0.7750 | LR=0.000031\n",
      "Early stopping at epoch 37 (best Val AUROC=0.7917)\n",
      "[Fold 5] Test AUROC: 0.9126, AUPRC: 0.5030\n",
      "\n",
      "===== 五折交叉验证结果 (Bootstrap) =====\n",
      "AUROC: Mean = 0.8292, 95% CI = (0.7764, 0.8741)\n",
      "AUPRC: Mean = 0.3759, 95% CI = (0.2756, 0.4770)\n",
      "\n",
      "✅ 所有折的预测结果已合并保存为：/home/mailiyi/Poisoning_Prediction/DNN/predict_death_calibration/all_folds_results.csv\n"
     ]
    }
   ],
   "source": [
    "import torch   \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn import metrics\n",
    "\n",
    "# ===================== 0. Fix random seed =====================\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed fixed as {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ===================== 1. Save path =====================\n",
    "save_path = \"/home/mailiyi/Poisoning_Prediction/DNN/predict_death_calibration/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# ===================== 2. Device selection =====================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ===================== 3. Tensorize data =====================\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# ===================== 4. DNN model =====================\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, 1)  # Output logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ===================== 5. 5-fold cross-validation =====================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "auroc_list, auprc_list = [], []\n",
    "all_results = []\n",
    "\n",
    "for train_val_index, test_index in kf.split(X_tensor):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "    set_seed(42 + fold)\n",
    "\n",
    "    # Split data\n",
    "    X_train_val, X_test = X_tensor[train_val_index], X_tensor[test_index]\n",
    "    y_train_val, y_test = y_tensor[train_val_index], y_tensor[test_index]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=1/8, random_state=42 + fold, stratify=y_train_val\n",
    "    )\n",
    "\n",
    "    # DataLoader\n",
    "    def worker_init_fn(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "        random.seed(42 + worker_id)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # Model, optimizer, loss function\n",
    "    model = DNN(input_dim=X_tensor.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Without using pos_weight\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3\n",
    "    )\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 12\n",
    "    best_auroc = 0\n",
    "    wait = 0\n",
    "    best_model_path = os.path.join(save_path, f\"fold{fold}_best_model.pt\")\n",
    "\n",
    "    # ===================== 6. Training =====================\n",
    "    max_epochs = 100\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)           # logits\n",
    "            loss = criterion(outputs, batch_y) # BCEWithLogitsLoss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_val.to(device)).squeeze()\n",
    "            y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            y_true = y_val.squeeze().cpu().numpy()\n",
    "            auroc_val = roc_auc_score(y_true, y_pred_prob)\n",
    "        scheduler.step(auroc_val)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | Loss: {avg_loss:.4f} | Val AUROC: {auroc_val:.4f} | LR={current_lr:.6f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if auroc_val > best_auroc:\n",
    "            best_auroc = auroc_val\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (best Val AUROC={best_auroc:.4f})\")\n",
    "                break\n",
    "\n",
    "    # ===================== 7. Test set evaluation =====================\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test.to(device)).squeeze()\n",
    "        y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        y_true = y_test.squeeze().cpu().numpy()\n",
    "        auroc = roc_auc_score(y_true, y_pred_prob)\n",
    "        auprc = average_precision_score(y_true, y_pred_prob)\n",
    "        auroc_list.append(auroc)\n",
    "        auprc_list.append(auprc)\n",
    "\n",
    "        print(f\"[Fold {fold}] Test AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "        result_df = pd.DataFrame({\"y_test\": y_true, \"y_pred\": y_pred_prob})\n",
    "        result_df.to_csv(os.path.join(save_path, f\"fold{fold}_results.csv\"), index=False)\n",
    "        all_results.append(result_df)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# ===================== 8. Bootstrap to calculate overall metrics =====================\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "all_results_df = pd.concat(all_results, axis=0).reset_index(drop=True)\n",
    "y_all_true = all_results_df[\"y_test\"].values\n",
    "y_all_pred = all_results_df[\"y_pred\"].values\n",
    "\n",
    "mean_auroc, auc_lower, auc_upper = bootstrap_metric_ci(y_all_true, y_all_pred, metrics.roc_auc_score)\n",
    "mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(y_all_true, y_all_pred, metrics.average_precision_score)\n",
    "\n",
    "print(\"\\n===== 5-fold cross-validation results (Bootstrap) =====\")\n",
    "print(f\"AUROC: Mean = {mean_auroc:.4f}, 95% CI = ({auc_lower:.4f}, {auc_upper:.4f})\")\n",
    "print(f\"AUPRC: Mean = {mean_auprc:.4f}, 95% CI = ({auprc_lower:.4f}, {auprc_upper:.4f})\")\n",
    "\n",
    "# ===================== 9. Save all prediction results =====================\n",
    "all_results_path = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "all_results_df.to_csv(all_results_path, index=False)\n",
    "print(f\"\\n✅ All fold prediction results have been merged and saved to: {all_results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1704be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
