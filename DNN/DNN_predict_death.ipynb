{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c9a3cd9",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ======================\n",
    "# 1. Read Data\n",
    "# ======================\n",
    "df1 = pd.read_csv('./Poisoning_Prediction/all_poisoning_data_wide_clean_albumin_20251106.csv')\n",
    "\n",
    "# ======================\n",
    "# 2. Define Features\n",
    "# ======================\n",
    "x_features_continuous = ['Age',\n",
    " 'Length of Stay',\n",
    " 'Weight',\n",
    " 'Systolic Blood Pressure',\n",
    " 'Diastolic Blood Pressure',\n",
    " 'Respiratory Rate', \n",
    " 'Heart Rate',\n",
    " 'White Blood Cell Count',\n",
    " 'Red Blood Cell Count',\n",
    " 'Hemoglobin Concentration',\n",
    " 'Mean Corpuscular Volume',\n",
    " 'Mean Corpuscular Hemoglobin',\n",
    " 'Mean Corpuscular Hemoglobin Concentration',\n",
    " 'Platelet Count',\n",
    " 'Mean Platelet Volume',\n",
    " 'Alanine Aminotransferase (ALT)',\n",
    " 'Total Bilirubin',\n",
    " 'Direct Bilirubin',\n",
    " 'Lactate Dehydrogenase (LDH)',\n",
    " 'Urea',\n",
    " 'Serum Creatinine',\n",
    " 'Uric Acid',\n",
    " 'Creatine Kinase (CK)',\n",
    " 'Creatine Kinase-MB Isoenzyme',\n",
    " 'Troponin I',\n",
    " 'High-Sensitivity C-Reactive Protein (hs-CRP)',\n",
    " 'Homocysteine',\n",
    " 'Potassium',\n",
    " 'Sodium',\n",
    " 'Chloride',\n",
    " 'Carbon Dioxide',\n",
    " 'Prothrombin Time',\n",
    " 'D-Dimer',\n",
    " 'Lactate',\n",
    " 'Blood Cholinesterase Test Results',\n",
    " 'Albumin (First Measurement)',\n",
    " 'Albumin (Last Measurement)',\n",
    " 'Number of Hemoperfusion Sessions',\n",
    " 'Number of Blood Purification Sessions',\n",
    " 'Hyperbaric Oxygen Therapy Duration and Frequency',\n",
    " 'Atropine Dosage',\n",
    " 'Long-acting Nitroglycerin Dosage',\n",
    " 'Pralidoxime Dosage',\n",
    " ] ## Removed features containing \"increasing\"\n",
    "x_features_categorical = ['Gender','Education Level','Type of Poisoning','Hypertension','Hyperlipidemia','Diabetes Mellitus','Cerebrovascular Disease','Heart Disease','Allergy History','Cancer','Poisoning','degree of poisoning','Smoking Status','Alcohol Consumption Status','Shortness of Breath','Chest Pain','Cough','Pre-syncope','Altered Consciousness or Syncope','Sore Throat','Fever','Fatigue','Lower Limb Edema','Palpitations','Vomiting','Nausea','Weakness','Headache','Residence'] # List of categorical variables\n",
    "\n",
    "## Prediction Target: y\n",
    "y_column = 'Outcome_other' ## Whether the patient died\n",
    "\n",
    "# ======================\n",
    "# 3. Shuffle Data\n",
    "# ======================\n",
    "df2 = df1.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# missing_summary = df2[x_features_continuous].isna().sum()\n",
    "# print(\"Continuous features with missing values in the original data:\")\n",
    "# print(missing_summary[missing_summary > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics Distribution of Outcome_other and Outcome\n",
    "print(\"\\n Outcome_other distribution (death or not)\")\n",
    "print(df2[\"Outcome_other\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n Outcome distribution (not cured):\")\n",
    "print(df2[\"Outcome\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d2db7",
   "metadata": {},
   "source": [
    "- Remove variables with deletion rate>90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f1c3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hospital ID',\n",
       " 'Gender',\n",
       " 'Education Level',\n",
       " 'Type of Poisoning',\n",
       " 'Hypertension',\n",
       " 'Hyperlipidemia',\n",
       " 'Diabetes Mellitus',\n",
       " 'Cerebrovascular Disease',\n",
       " 'Heart Disease',\n",
       " 'Allergy History',\n",
       " 'Cancer',\n",
       " 'Poisoning',\n",
       " 'degree of poisoning',\n",
       " 'Smoking Status',\n",
       " 'Alcohol Consumption Status',\n",
       " 'Shortness of Breath',\n",
       " 'Chest Pain',\n",
       " 'Cough',\n",
       " 'Pre-syncope',\n",
       " 'Altered Consciousness or Syncope',\n",
       " 'Sore Throat',\n",
       " 'Fever',\n",
       " 'Fatigue',\n",
       " 'Lower Limb Edema',\n",
       " 'Palpitations',\n",
       " 'Vomiting',\n",
       " 'Nausea',\n",
       " 'Weakness',\n",
       " 'Headache',\n",
       " 'Residence',\n",
       " 'Age',\n",
       " 'Length of Stay',\n",
       " 'Weight',\n",
       " 'Systolic Blood Pressure',\n",
       " 'Diastolic Blood Pressure',\n",
       " 'Respiratory Rate',\n",
       " 'Heart Rate',\n",
       " 'White Blood Cell Count',\n",
       " 'Red Blood Cell Count',\n",
       " 'Hemoglobin Concentration',\n",
       " 'Mean Corpuscular Volume',\n",
       " 'Mean Corpuscular Hemoglobin',\n",
       " 'Mean Corpuscular Hemoglobin Concentration',\n",
       " 'Platelet Count',\n",
       " 'Mean Platelet Volume',\n",
       " 'Alanine Aminotransferase (ALT)',\n",
       " 'Total Bilirubin',\n",
       " 'Direct Bilirubin',\n",
       " 'Lactate Dehydrogenase (LDH)',\n",
       " 'Urea',\n",
       " 'Serum Creatinine',\n",
       " 'Uric Acid',\n",
       " 'Creatine Kinase (CK)',\n",
       " 'Creatine Kinase-MB Isoenzyme',\n",
       " 'Troponin I',\n",
       " 'High-Sensitivity C-Reactive Protein (hs-CRP)',\n",
       " 'Homocysteine',\n",
       " 'Potassium',\n",
       " 'Sodium',\n",
       " 'Chloride',\n",
       " 'Carbon Dioxide',\n",
       " 'Prothrombin Time',\n",
       " 'D-Dimer',\n",
       " 'Lactate',\n",
       " 'Blood Cholinesterase Test Results',\n",
       " 'Albumin (First Measurement)',\n",
       " 'Albumin (Last Measurement)',\n",
       " 'Number of Hemoperfusion Sessions',\n",
       " 'Number of Blood Purification Sessions',\n",
       " 'Hyperbaric Oxygen Therapy Duration and Frequency',\n",
       " 'Atropine Dosage',\n",
       " 'Long-acting Nitroglycerin Dosage',\n",
       " 'Pralidoxime Dosage',\n",
       " 'White Blood Cell Count_increasing',\n",
       " 'Red Blood Cell Count_increasing',\n",
       " 'Hemoglobin Concentration_increasing',\n",
       " 'Mean Corpuscular Volume_increasing',\n",
       " 'Mean Corpuscular Hemoglobin_increasing',\n",
       " 'Mean Corpuscular Hemoglobin Concentration_increasing',\n",
       " 'Platelet Count_increasing',\n",
       " 'Mean Platelet Volume_increasing',\n",
       " 'Alanine Aminotransferase (ALT)_increasing',\n",
       " 'Total Bilirubin_increasing',\n",
       " 'Direct Bilirubin_increasing',\n",
       " 'Lactate Dehydrogenase (LDH)_increasing',\n",
       " 'Urea_increasing',\n",
       " 'Serum Creatinine_increasing',\n",
       " 'Uric Acid_increasing',\n",
       " 'Creatine Kinase (CK)_increasing',\n",
       " 'Creatine Kinase-MB Isoenzyme_increasing',\n",
       " 'Troponin I_increasing',\n",
       " 'High-Sensitivity C-Reactive Protein (hs-CRP)_increasing',\n",
       " 'Potassium_increasing',\n",
       " 'Chloride_increasing',\n",
       " 'Carbon Dioxide_increasing',\n",
       " 'Prothrombin Time_increasing',\n",
       " 'D-Dimer_increasing',\n",
       " 'Lactate_increasing',\n",
       " 'Blood Cholinesterase Test Results_increasing',\n",
       " 'Albumin (Last Measurement)_increasing',\n",
       " 'Number of Blood Purification Sessions_increasing',\n",
       " 'Atropine Dosage_increasing',\n",
       " 'Long-acting Nitroglycerin Dosage_increasing',\n",
       " 'Outcome_other',\n",
       " 'Outcome',\n",
       " 'Outcome_another']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea1a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the missing ratio of continuous variables\n",
    "\n",
    "# Calculate the missing ratio (column-wise)\n",
    "missing_ratios = df2[x_features_continuous+x_features_categorical].isnull().mean()\n",
    "\n",
    "# Convert to percentage and sort (descending order)\n",
    "missing_summary = (missing_ratios * 100).round(2).sort_values(ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Variable missing ratio (%):\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050aa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select feature names with missing rate > 90%\n",
    "high_missing_features = missing_ratios[missing_ratios > 0.90].index.tolist()\n",
    "\n",
    "# Optional: Print these features\n",
    "print(\"Continuous variables with missing rate > 90%:\")\n",
    "for feat in high_missing_features:\n",
    "    print(f\"{feat}: {missing_ratios[feat]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ac19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "11\n",
      "32\n",
      "(971, 95)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_features_continuous))\n",
    "print(len(high_missing_features))\n",
    "x_features_continuous = [feat for feat in x_features_continuous if feat not in high_missing_features]\n",
    "print(len(x_features_continuous))\n",
    "\n",
    "# df2 Remove variables from high_missing_features\n",
    "df2 = df2.drop(columns=high_missing_features)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 4. Fill missing values with the median of all samples\n",
    "# ======================\n",
    "median_values = df2[x_features_continuous].median()\n",
    "df2[x_features_continuous] = df2[x_features_continuous].fillna(median_values)\n",
    "\n",
    "# Check if there are still missing values\n",
    "missing_summary = df2[x_features_continuous].isna().sum()\n",
    "print(\"Continuous features still containing missing values:\")\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# ======================\n",
    "# 5. Normalize (standardize) each column of continuous variables\n",
    "# ======================\n",
    "scaler = StandardScaler()\n",
    "df2[x_features_continuous] = scaler.fit_transform(df2[x_features_continuous])\n",
    "\n",
    "# Optional: Output the mean and standard deviation of each column to confirm normalization\n",
    "check_means = df2[x_features_continuous].mean().round(3)\n",
    "check_stds = df2[x_features_continuous].std().round(3)\n",
    "print(\"\\nMeans and standard deviations of some continuous features after standardization (should be close to 0 and 1):\")\n",
    "print(pd.DataFrame({'mean': check_means.head(10), 'std': check_stds.head(10)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bbe2f",
   "metadata": {},
   "source": [
    "- one-hot: categorical variables are populated first (missing values are populated as Unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, fill missing values in categorical variables with \"Unknown\"\n",
    "for col in x_features_categorical:\n",
    "    if col in df2.columns:\n",
    "        df2[col] = df2[col].fillna('Unknown')\n",
    "\n",
    "# ======================\n",
    "# 6. One-Hot Encoding\n",
    "# ======================\n",
    "x_columns = x_features_categorical + x_features_continuous\n",
    "datax = df2[x_columns]\n",
    "datay = df2[y_column]\n",
    "\n",
    "datax_encoded = pd.get_dummies(datax, columns=x_features_categorical, drop_first=False)\n",
    "\n",
    "# Explicitly convert boolean values to float (True→1.0, False→0.0)\n",
    "datax_encoded = datax_encoded.astype(float)\n",
    "\n",
    "print(f\"\\nOriginal number of features: {len(x_columns)}\")\n",
    "print(f\"Number of features after One-Hot Encoding: {datax_encoded.shape[1]}\")\n",
    "print(f\"Number of samples: {datax_encoded.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9622ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor 形状：X=torch.Size([971, 107]), y=torch.Size([971, 1])\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 7. conversion tensor\n",
    "# ======================\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"\\nTensor form：X={X_tensor.shape}, y={y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dccbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38701c95",
   "metadata": {},
   "source": [
    "## 2. Five-fold cross validation: Divide 1/8 of the training set into validation sets (i.e. 70% training set, 10% validation set, 20% test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b27e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed fixed as 42\n",
      "Using device: cuda\n",
      "\n",
      "===== Fold 1 =====\n",
      "Random seed fixed as 43\n",
      "Fold 1: pos_weight = 11.12\n",
      "Epoch 001 | Loss: 1.2887 | Val AUROC: 0.4115 | LR=0.000500\n",
      "Epoch 002 | Loss: 1.2559 | Val AUROC: 0.6419 | LR=0.000500\n",
      "Epoch 003 | Loss: 1.1981 | Val AUROC: 0.7402 | LR=0.000500\n",
      "Epoch 004 | Loss: 1.1700 | Val AUROC: 0.7711 | LR=0.000500\n",
      "Epoch 005 | Loss: 1.1492 | Val AUROC: 0.8287 | LR=0.000500\n",
      "Epoch 006 | Loss: 1.0729 | Val AUROC: 0.8469 | LR=0.000500\n",
      "Epoch 007 | Loss: 1.0249 | Val AUROC: 0.8539 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.9427 | Val AUROC: 0.8539 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.8930 | Val AUROC: 0.8638 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.8408 | Val AUROC: 0.8652 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.7568 | Val AUROC: 0.8652 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.7245 | Val AUROC: 0.8750 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.6680 | Val AUROC: 0.8750 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.6280 | Val AUROC: 0.8750 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.5854 | Val AUROC: 0.8581 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.6899 | Val AUROC: 0.8680 | LR=0.000250\n",
      "Epoch 017 | Loss: 0.5844 | Val AUROC: 0.8596 | LR=0.000250\n",
      "Epoch 018 | Loss: 0.6414 | Val AUROC: 0.8610 | LR=0.000250\n",
      "Epoch 019 | Loss: 0.5380 | Val AUROC: 0.8624 | LR=0.000250\n",
      "Epoch 020 | Loss: 0.5214 | Val AUROC: 0.8581 | LR=0.000125\n",
      "Epoch 021 | Loss: 0.5080 | Val AUROC: 0.8483 | LR=0.000125\n",
      "Epoch 022 | Loss: 0.4738 | Val AUROC: 0.8567 | LR=0.000125\n",
      "Epoch 023 | Loss: 0.4903 | Val AUROC: 0.8539 | LR=0.000125\n",
      "Epoch 024 | Loss: 0.4882 | Val AUROC: 0.8567 | LR=0.000063\n",
      "Early stopping at epoch 24 (best Val AUROC=0.8750)\n",
      "[Fold 1] Test AUROC: 0.7793, AUPRC: 0.3763\n",
      "\n",
      "===== Fold 2 =====\n",
      "Random seed fixed as 44\n",
      "Fold 2: pos_weight = 10.51\n",
      "Epoch 001 | Loss: 1.2827 | Val AUROC: 0.3778 | LR=0.000500\n",
      "Epoch 002 | Loss: 1.2370 | Val AUROC: 0.5569 | LR=0.000500\n",
      "Epoch 003 | Loss: 1.2238 | Val AUROC: 0.7306 | LR=0.000500\n",
      "Epoch 004 | Loss: 1.1888 | Val AUROC: 0.7681 | LR=0.000500\n",
      "Epoch 005 | Loss: 1.1778 | Val AUROC: 0.8000 | LR=0.000500\n",
      "Epoch 006 | Loss: 1.1549 | Val AUROC: 0.7944 | LR=0.000500\n",
      "Epoch 007 | Loss: 1.0959 | Val AUROC: 0.7958 | LR=0.000500\n",
      "Epoch 008 | Loss: 1.0365 | Val AUROC: 0.7958 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.9947 | Val AUROC: 0.8042 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.9584 | Val AUROC: 0.8111 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.9273 | Val AUROC: 0.8222 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.8593 | Val AUROC: 0.8458 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.8085 | Val AUROC: 0.8514 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.7832 | Val AUROC: 0.8625 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.7728 | Val AUROC: 0.8764 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.6928 | Val AUROC: 0.8806 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.5969 | Val AUROC: 0.8778 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.5926 | Val AUROC: 0.8750 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.6187 | Val AUROC: 0.8847 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.5823 | Val AUROC: 0.8736 | LR=0.000500\n",
      "Epoch 021 | Loss: 0.5908 | Val AUROC: 0.8833 | LR=0.000500\n",
      "Epoch 022 | Loss: 0.5339 | Val AUROC: 0.8681 | LR=0.000500\n",
      "Epoch 023 | Loss: 0.4875 | Val AUROC: 0.8875 | LR=0.000500\n",
      "Epoch 024 | Loss: 0.5208 | Val AUROC: 0.8708 | LR=0.000500\n",
      "Epoch 025 | Loss: 0.5830 | Val AUROC: 0.8556 | LR=0.000500\n",
      "Epoch 026 | Loss: 0.8043 | Val AUROC: 0.8333 | LR=0.000500\n",
      "Epoch 027 | Loss: 0.4575 | Val AUROC: 0.8458 | LR=0.000250\n",
      "Epoch 028 | Loss: 0.4522 | Val AUROC: 0.8444 | LR=0.000250\n",
      "Epoch 029 | Loss: 0.4288 | Val AUROC: 0.8597 | LR=0.000250\n",
      "Epoch 030 | Loss: 0.4563 | Val AUROC: 0.8556 | LR=0.000250\n",
      "Epoch 031 | Loss: 0.4760 | Val AUROC: 0.8639 | LR=0.000125\n",
      "Epoch 032 | Loss: 0.4855 | Val AUROC: 0.8611 | LR=0.000125\n",
      "Epoch 033 | Loss: 0.4439 | Val AUROC: 0.8653 | LR=0.000125\n",
      "Epoch 034 | Loss: 0.4006 | Val AUROC: 0.8639 | LR=0.000125\n",
      "Epoch 035 | Loss: 0.4070 | Val AUROC: 0.8639 | LR=0.000063\n",
      "Early stopping at epoch 35 (best Val AUROC=0.8875)\n",
      "[Fold 2] Test AUROC: 0.9035, AUPRC: 0.5830\n",
      "\n",
      "===== Fold 3 =====\n",
      "Random seed fixed as 45\n",
      "Fold 3: pos_weight = 11.57\n",
      "Epoch 001 | Loss: 1.2857 | Val AUROC: 0.6542 | LR=0.000500\n",
      "Epoch 002 | Loss: 1.2440 | Val AUROC: 0.6875 | LR=0.000500\n",
      "Epoch 003 | Loss: 1.2148 | Val AUROC: 0.7389 | LR=0.000500\n",
      "Epoch 004 | Loss: 1.1685 | Val AUROC: 0.7486 | LR=0.000500\n",
      "Epoch 005 | Loss: 1.1513 | Val AUROC: 0.7431 | LR=0.000500\n",
      "Epoch 006 | Loss: 1.1128 | Val AUROC: 0.7597 | LR=0.000500\n",
      "Epoch 007 | Loss: 1.0983 | Val AUROC: 0.7542 | LR=0.000500\n",
      "Epoch 008 | Loss: 1.0048 | Val AUROC: 0.7639 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.9671 | Val AUROC: 0.7722 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.8851 | Val AUROC: 0.7750 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.8233 | Val AUROC: 0.7875 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.7387 | Val AUROC: 0.8042 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.7227 | Val AUROC: 0.7972 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.7208 | Val AUROC: 0.8097 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.6393 | Val AUROC: 0.8111 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.6046 | Val AUROC: 0.8139 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.6037 | Val AUROC: 0.8056 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.5774 | Val AUROC: 0.7875 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.5537 | Val AUROC: 0.7986 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.5631 | Val AUROC: 0.7806 | LR=0.000250\n",
      "Epoch 021 | Loss: 0.5537 | Val AUROC: 0.7819 | LR=0.000250\n",
      "Epoch 022 | Loss: 0.4880 | Val AUROC: 0.7819 | LR=0.000250\n",
      "Epoch 023 | Loss: 0.5363 | Val AUROC: 0.7806 | LR=0.000250\n",
      "Epoch 024 | Loss: 0.4522 | Val AUROC: 0.7819 | LR=0.000125\n",
      "Epoch 025 | Loss: 0.4917 | Val AUROC: 0.7875 | LR=0.000125\n",
      "Epoch 026 | Loss: 0.4442 | Val AUROC: 0.7903 | LR=0.000125\n",
      "Epoch 027 | Loss: 0.4726 | Val AUROC: 0.7806 | LR=0.000125\n",
      "Epoch 028 | Loss: 0.4897 | Val AUROC: 0.7819 | LR=0.000063\n",
      "Early stopping at epoch 28 (best Val AUROC=0.8139)\n",
      "[Fold 3] Test AUROC: 0.8359, AUPRC: 0.4482\n",
      "\n",
      "===== Fold 4 =====\n",
      "Random seed fixed as 46\n",
      "Fold 4: pos_weight = 10.32\n",
      "Epoch 001 | Loss: 1.2575 | Val AUROC: 0.7890 | LR=0.000500\n",
      "Epoch 002 | Loss: 1.2641 | Val AUROC: 0.8327 | LR=0.000500\n",
      "Epoch 003 | Loss: 1.1722 | Val AUROC: 0.8439 | LR=0.000500\n",
      "Epoch 004 | Loss: 1.1747 | Val AUROC: 0.8564 | LR=0.000500\n",
      "Epoch 005 | Loss: 1.0836 | Val AUROC: 0.8789 | LR=0.000500\n",
      "Epoch 006 | Loss: 1.0610 | Val AUROC: 0.8801 | LR=0.000500\n",
      "Epoch 007 | Loss: 1.0157 | Val AUROC: 0.8702 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.9307 | Val AUROC: 0.8814 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.8903 | Val AUROC: 0.8826 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.8465 | Val AUROC: 0.8826 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.7891 | Val AUROC: 0.8851 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.8449 | Val AUROC: 0.8901 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.7346 | Val AUROC: 0.9014 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.6324 | Val AUROC: 0.8976 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.6846 | Val AUROC: 0.8976 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.6376 | Val AUROC: 0.9026 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.6179 | Val AUROC: 0.9014 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.5291 | Val AUROC: 0.9051 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.5404 | Val AUROC: 0.9051 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.4683 | Val AUROC: 0.8939 | LR=0.000500\n",
      "Epoch 021 | Loss: 0.4705 | Val AUROC: 0.9001 | LR=0.000500\n",
      "Epoch 022 | Loss: 0.4992 | Val AUROC: 0.8964 | LR=0.000250\n",
      "Epoch 023 | Loss: 0.4206 | Val AUROC: 0.8951 | LR=0.000250\n",
      "Epoch 024 | Loss: 0.4602 | Val AUROC: 0.8964 | LR=0.000250\n",
      "Epoch 025 | Loss: 0.4308 | Val AUROC: 0.8926 | LR=0.000250\n",
      "Epoch 026 | Loss: 0.4954 | Val AUROC: 0.8926 | LR=0.000125\n",
      "Epoch 027 | Loss: 0.4122 | Val AUROC: 0.8901 | LR=0.000125\n",
      "Epoch 028 | Loss: 0.4323 | Val AUROC: 0.8926 | LR=0.000125\n",
      "Epoch 029 | Loss: 0.4379 | Val AUROC: 0.8914 | LR=0.000125\n",
      "Epoch 030 | Loss: 0.4599 | Val AUROC: 0.8926 | LR=0.000063\n",
      "Early stopping at epoch 30 (best Val AUROC=0.9051)\n",
      "[Fold 4] Test AUROC: 0.8075, AUPRC: 0.3722\n",
      "\n",
      "===== Fold 5 =====\n",
      "Random seed fixed as 47\n",
      "Fold 5: pos_weight = 10.71\n",
      "Epoch 001 | Loss: 1.3021 | Val AUROC: 0.6917 | LR=0.000500\n",
      "Epoch 002 | Loss: 1.2487 | Val AUROC: 0.6944 | LR=0.000500\n",
      "Epoch 003 | Loss: 1.2151 | Val AUROC: 0.7250 | LR=0.000500\n",
      "Epoch 004 | Loss: 1.1424 | Val AUROC: 0.7292 | LR=0.000500\n",
      "Epoch 005 | Loss: 1.1317 | Val AUROC: 0.7264 | LR=0.000500\n",
      "Epoch 006 | Loss: 1.1136 | Val AUROC: 0.7403 | LR=0.000500\n",
      "Epoch 007 | Loss: 1.0512 | Val AUROC: 0.7403 | LR=0.000500\n",
      "Epoch 008 | Loss: 0.9864 | Val AUROC: 0.7472 | LR=0.000500\n",
      "Epoch 009 | Loss: 0.9545 | Val AUROC: 0.7528 | LR=0.000500\n",
      "Epoch 010 | Loss: 0.8812 | Val AUROC: 0.7569 | LR=0.000500\n",
      "Epoch 011 | Loss: 0.9163 | Val AUROC: 0.7694 | LR=0.000500\n",
      "Epoch 012 | Loss: 0.8706 | Val AUROC: 0.7875 | LR=0.000500\n",
      "Epoch 013 | Loss: 0.7565 | Val AUROC: 0.7847 | LR=0.000500\n",
      "Epoch 014 | Loss: 0.7314 | Val AUROC: 0.7931 | LR=0.000500\n",
      "Epoch 015 | Loss: 0.7059 | Val AUROC: 0.7806 | LR=0.000500\n",
      "Epoch 016 | Loss: 0.7383 | Val AUROC: 0.7792 | LR=0.000500\n",
      "Epoch 017 | Loss: 0.6387 | Val AUROC: 0.7931 | LR=0.000500\n",
      "Epoch 018 | Loss: 0.6699 | Val AUROC: 0.7944 | LR=0.000500\n",
      "Epoch 019 | Loss: 0.6078 | Val AUROC: 0.7889 | LR=0.000500\n",
      "Epoch 020 | Loss: 0.6098 | Val AUROC: 0.7944 | LR=0.000500\n",
      "Epoch 021 | Loss: 0.5471 | Val AUROC: 0.7917 | LR=0.000500\n",
      "Epoch 022 | Loss: 0.5268 | Val AUROC: 0.7889 | LR=0.000250\n",
      "Epoch 023 | Loss: 0.5369 | Val AUROC: 0.7931 | LR=0.000250\n",
      "Epoch 024 | Loss: 0.4609 | Val AUROC: 0.7806 | LR=0.000250\n",
      "Epoch 025 | Loss: 0.6249 | Val AUROC: 0.7903 | LR=0.000250\n",
      "Epoch 026 | Loss: 0.4891 | Val AUROC: 0.7903 | LR=0.000125\n",
      "Epoch 027 | Loss: 0.4751 | Val AUROC: 0.7903 | LR=0.000125\n",
      "Epoch 028 | Loss: 0.5282 | Val AUROC: 0.7847 | LR=0.000125\n",
      "Epoch 029 | Loss: 0.4843 | Val AUROC: 0.7847 | LR=0.000125\n",
      "Epoch 030 | Loss: 0.4375 | Val AUROC: 0.7861 | LR=0.000063\n",
      "Early stopping at epoch 30 (best Val AUROC=0.7944)\n",
      "[Fold 5] Test AUROC: 0.8961, AUPRC: 0.4266\n",
      "\n",
      "===== 五折交叉验证结果 (Bootstrap) =====\n",
      "AUROC: Mean = 0.8404, 95% CI = (0.7888, 0.8857)\n",
      "AUPRC: Mean = 0.4190, 95% CI = (0.3151, 0.5219)\n",
      "\n",
      "✅ 所有折的预测结果已合并保存为：/home/mailiyi/Poisoning_Prediction/DNN/predict_death_valid_test_5cv/all_folds_results.csv\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn import metrics\n",
    "\n",
    "# =============== 0. Fix random seed ===============\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed fixed as {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# =============== 1. Save path ===============\n",
    "save_path = \"./Poisoning_Prediction/DNN/predict_death_valid_test_5cv/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# =============== 2. Device selection ===============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============== 3. Tensorize data ===============\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =============== 4. Improved DNN model ===============\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# =============== 5. 5-fold cross-validation ===============\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "auroc_list, auprc_list = [], []\n",
    "all_results = []\n",
    "\n",
    "for train_val_index, test_index in kf.split(X_tensor):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "    set_seed(42 + fold)\n",
    "\n",
    "    # Split data\n",
    "    X_train_val, X_test = X_tensor[train_val_index], X_tensor[test_index]\n",
    "    y_train_val, y_test = y_tensor[train_val_index], y_tensor[test_index]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=1/8, random_state=42 + fold, stratify=y_train_val\n",
    "    )\n",
    "\n",
    "    # Class weights\n",
    "    num_pos = (y_train == 1).sum().item()\n",
    "    num_neg = (y_train == 0).sum().item()\n",
    "    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32).to(device)\n",
    "    print(f\"Fold {fold}: pos_weight = {pos_weight:.2f}\")\n",
    "\n",
    "    # DataLoader\n",
    "    def worker_init_fn(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "        random.seed(42 + worker_id)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # Model, optimizer, scheduler, loss function\n",
    "    model = DNN(input_dim=X_tensor.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    patience = 12\n",
    "    best_auroc = 0\n",
    "    wait = 0\n",
    "    best_model_path = os.path.join(save_path, f\"fold{fold}_best_model.pt\")\n",
    "\n",
    "    # =============== 6. Training ===============\n",
    "    max_epochs = 100\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_val.to(device)).squeeze()\n",
    "            y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            y_true = y_val.squeeze().cpu().numpy()\n",
    "            auroc_val = roc_auc_score(y_true, y_pred_prob)\n",
    "        scheduler.step(auroc_val)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | Loss: {avg_loss:.4f} | Val AUROC: {auroc_val:.4f} | LR={current_lr:.6f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if auroc_val > best_auroc:\n",
    "            best_auroc = auroc_val\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (best Val AUROC={best_auroc:.4f})\")\n",
    "                break\n",
    "\n",
    "    # =============== 7. Test set evaluation ===============\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test.to(device)).squeeze()\n",
    "        y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        y_true = y_test.squeeze().cpu().numpy()\n",
    "        auroc = roc_auc_score(y_true, y_pred_prob)\n",
    "        auprc = average_precision_score(y_true, y_pred_prob)\n",
    "        auroc_list.append(auroc)\n",
    "        auprc_list.append(auprc)\n",
    "\n",
    "        print(f\"[Fold {fold}] Test AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "        result_df = pd.DataFrame({\"y_test\": y_true, \"y_pred\": y_pred_prob})\n",
    "        result_df.to_csv(os.path.join(save_path, f\"fold{fold}_results.csv\"), index=False)\n",
    "        all_results.append(result_df)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# =============== 8. Bootstrap to calculate overall metrics ===============\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "all_results_df = pd.concat(all_results, axis=0).reset_index(drop=True)\n",
    "y_all_true = all_results_df[\"y_test\"].values\n",
    "y_all_pred = all_results_df[\"y_pred\"].values\n",
    "\n",
    "mean_auroc, auc_lower, auc_upper = bootstrap_metric_ci(y_all_true, y_all_pred, metrics.roc_auc_score)\n",
    "mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(y_all_true, y_all_pred, metrics.average_precision_score)\n",
    "\n",
    "print(\"\\n===== 5-fold cross-validation results (Bootstrap) =====\")\n",
    "print(f\"AUROC: Mean = {mean_auroc:.4f}, 95% CI = ({auc_lower:.4f}, {auc_upper:.4f})\")\n",
    "print(f\"AUPRC: Mean = {mean_auprc:.4f}, 95% CI = ({auprc_lower:.4f}, {auprc_upper:.4f})\")\n",
    "\n",
    "# =============== 9. Save all prediction results ===============\n",
    "all_results_path = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "all_results_df.to_csv(all_results_path, index=False)\n",
    "print(f\"\\n✅ All fold prediction results have been merged and saved to: {all_results_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
