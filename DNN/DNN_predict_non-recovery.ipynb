{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1024e15e",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f573e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ======================\n",
    "# 1. Read Data\n",
    "# ======================\n",
    "df1 = pd.read_csv('./Poisoning_Prediction/all_poisoning_data_wide_clean_albumin_20251106.csv')\n",
    "\n",
    "# ======================\n",
    "# 2. Define Features\n",
    "# ======================\n",
    "x_features_continuous = ['Age',\n",
    " 'Length of Stay',\n",
    " 'Weight',\n",
    " 'Systolic Blood Pressure',\n",
    " 'Diastolic Blood Pressure',\n",
    " 'Respiratory Rate', \n",
    " 'Heart Rate',\n",
    " 'White Blood Cell Count',\n",
    " 'Red Blood Cell Count',\n",
    " 'Hemoglobin Concentration',\n",
    " 'Mean Corpuscular Volume',\n",
    " 'Mean Corpuscular Hemoglobin',\n",
    " 'Mean Corpuscular Hemoglobin Concentration',\n",
    " 'Platelet Count',\n",
    " 'Mean Platelet Volume',\n",
    " 'Alanine Aminotransferase (ALT)',\n",
    " 'Total Bilirubin',\n",
    " 'Direct Bilirubin',\n",
    " 'Lactate Dehydrogenase (LDH)',\n",
    " 'Urea',\n",
    " 'Serum Creatinine',\n",
    " 'Uric Acid',\n",
    " 'Creatine Kinase (CK)',\n",
    " 'Creatine Kinase-MB Isoenzyme',\n",
    " 'Troponin I',\n",
    " 'High-Sensitivity C-Reactive Protein (hs-CRP)',\n",
    " 'Homocysteine',\n",
    " 'Potassium',\n",
    " 'Sodium',\n",
    " 'Chloride',\n",
    " 'Carbon Dioxide',\n",
    " 'Prothrombin Time',\n",
    " 'D-Dimer',\n",
    " 'Lactate',\n",
    " 'Blood Cholinesterase Test Results',\n",
    " 'Albumin (First Measurement)',\n",
    " 'Albumin (Last Measurement)',\n",
    " 'Number of Hemoperfusion Sessions',\n",
    " 'Number of Blood Purification Sessions',\n",
    " 'Hyperbaric Oxygen Therapy Duration and Frequency',\n",
    " 'Atropine Dosage',\n",
    " 'Long-acting Nitroglycerin Dosage',\n",
    " 'Pralidoxime Dosage',\n",
    " ] \n",
    "x_features_categorical = ['Gender','Education Level','Type of Poisoning','Hypertension','Hyperlipidemia','Diabetes Mellitus','Cerebrovascular Disease','Heart Disease','Allergy History','Cancer','Poisoning','degree of poisoning','Smoking Status','Alcohol Consumption Status','Shortness of Breath','Chest Pain','Cough','Pre-syncope','Altered Consciousness or Syncope','Sore Throat','Fever','Fatigue','Lower Limb Edema','Palpitations','Vomiting','Nausea','Weakness','Headache','Residence'] # 分类变量列表\n",
    "\n",
    "\n",
    "# y_column = 'Outcome_other' \n",
    "y_column = 'Outcome' \n",
    "\n",
    "# ======================\n",
    "# 3. Shuffle Data\n",
    "# ======================\n",
    "df2 = df1.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# missing_summary = df2[x_features_continuous].isna().sum()\n",
    "# print(\"原始数据含缺失值的连续特征：\")\n",
    "# print(missing_summary[missing_summary > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a36ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outcome_other 分布（是否死亡）：\n",
      "Outcome_other\n",
      "0    889\n",
      "1     82\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Outcome 分布（是否未治愈）：\n",
      "Outcome\n",
      "0    731\n",
      "1    240\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Statistics Distribution of Outcome_other and Outcome\n",
    "\n",
    "print(df2[\"Outcome_other\"].value_counts(dropna=False))\n",
    "\n",
    "print(df2[\"Outcome\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85870d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变量缺失比例（%）:\n",
      "Lactate             96.81\n",
      "Carbon Dioxide      94.95\n",
      "Sodium              94.75\n",
      "Potassium           94.75\n",
      "Chloride            94.75\n",
      "                    ...  \n",
      "Lower Limb Edema     0.00\n",
      "Vomiting             0.00\n",
      "Nausea               0.00\n",
      "Weakness             0.00\n",
      "Headache             0.00\n",
      "Length: 72, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Calculate the missing ratio of continuous variables\n",
    "\n",
    "missing_ratios = df2[x_features_continuous+x_features_categorical].isnull().mean()\n",
    "\n",
    "missing_summary = (missing_ratios * 100).round(2).sort_values(ascending=False)\n",
    "\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e75e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失率 > 90% 的连续变量:\n",
      "Potassium: 94.75%\n",
      "Sodium: 94.75%\n",
      "Chloride: 94.75%\n",
      "Carbon Dioxide: 94.95%\n",
      "Prothrombin Time: 94.64%\n",
      "D-Dimer: 94.64%\n",
      "Lactate: 96.81%\n",
      "Hyperbaric Oxygen Therapy Duration and Frequency: 92.38%\n",
      "Atropine Dosage: 93.92%\n",
      "Long-acting Nitroglycerin Dosage: 92.48%\n",
      "Pralidoxime Dosage: 92.17%\n"
     ]
    }
   ],
   "source": [
    "# Select feature names with missing rate > 90%\n",
    "high_missing_features = missing_ratios[missing_ratios > 0.90].index.tolist()\n",
    "\n",
    "for feat in high_missing_features:\n",
    "    print(f\"{feat}: {missing_ratios[feat]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66acbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "11\n",
      "32\n",
      "(971, 95)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_features_continuous))\n",
    "print(len(high_missing_features))\n",
    "x_features_continuous = [feat for feat in x_features_continuous if feat not in high_missing_features]\n",
    "print(len(x_features_continuous))\n",
    "\n",
    "df2 = df2.drop(columns=high_missing_features)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ba8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "仍含缺失值的连续特征：\n",
      "Series([], dtype: int64)\n",
      "\n",
      "标准化后部分连续特征均值与标准差（应接近0与1）:\n",
      "                          mean    std\n",
      "Age                        0.0  1.001\n",
      "Length of Stay            -0.0  1.001\n",
      "Weight                     0.0  1.001\n",
      "Systolic Blood Pressure   -0.0  1.001\n",
      "Diastolic Blood Pressure   0.0  1.001\n",
      "Respiratory Rate           0.0  1.001\n",
      "Heart Rate                -0.0  1.001\n",
      "White Blood Cell Count     0.0  1.001\n",
      "Red Blood Cell Count      -0.0  1.001\n",
      "Hemoglobin Concentration   0.0  1.001\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 4. Fill missing values with the median of all samples\n",
    "# ======================\n",
    "median_values = df2[x_features_continuous].median()\n",
    "df2[x_features_continuous] = df2[x_features_continuous].fillna(median_values)\n",
    "\n",
    "missing_summary = df2[x_features_continuous].isna().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# ======================\n",
    "# 5. Normalize (standardize) each column of continuous variables\n",
    "# ======================\n",
    "scaler = StandardScaler()\n",
    "df2[x_features_continuous] = scaler.fit_transform(df2[x_features_continuous])\n",
    "\n",
    "check_means = df2[x_features_continuous].mean().round(3)\n",
    "check_stds = df2[x_features_continuous].std().round(3)\n",
    "print(pd.DataFrame({'mean': check_means.head(10), 'std': check_stds.head(10)}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf8714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原始特征数: 61\n",
      "One-Hot 后特征数: 107\n",
      "样本量: 971\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## First, fill missing values in categorical variables with \"Unknown\"\n",
    "for col in x_features_categorical:\n",
    "    if col in df2.columns:\n",
    "        df2[col] = df2[col].fillna('Unknown')\n",
    "\n",
    "# ======================\n",
    "# 6. One-Hot Encoding\n",
    "# ======================\n",
    "x_columns = x_features_categorical + x_features_continuous\n",
    "datax = df2[x_columns]\n",
    "datay = df2[y_column]\n",
    "\n",
    "datax_encoded = pd.get_dummies(datax, columns=x_features_categorical, drop_first=False)\n",
    "\n",
    "datax_encoded = datax_encoded.astype(float)\n",
    "print(f\"\\nOriginal number of features: {len(x_columns)}\")\n",
    "print(f\"Number of features after One-Hot Encoding: {datax_encoded.shape[1]}\")\n",
    "print(f\"Number of samples: {datax_encoded.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2942b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor 形状：X=torch.Size([971, 107]), y=torch.Size([971, 1])\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 7. conversion tensor\n",
    "# ======================\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"\\nTensor form：X={X_tensor.shape}, y={y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d7068",
   "metadata": {},
   "source": [
    "## 2.  Five-fold cross validation: Divide 1/8 of the training set into validation sets (i.e. 70% training set, 10% validation set, 20% test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn import metrics\n",
    "\n",
    "# =============== 0. Fix random seeds for reproducibility ===============\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed fixed as {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# =============== 1. Output directory ===============\n",
    "save_path = \"./Poisoning_Prediction/DNN/predict_non-recovery_valid_test_5cv/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# =============== 2. Device selection (CUDA / MPS / CPU) ===============\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============== 3. Convert data to tensors ===============\n",
    "X_tensor = torch.tensor(datax_encoded.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(datay.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =============== 4. Improved DNN architecture ===============\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# =============== 5. Five-fold cross-validation ===============\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "auroc_list, auprc_list = [], []\n",
    "all_results = []\n",
    "\n",
    "for train_val_index, test_index in kf.split(X_tensor):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "    set_seed(42 + fold)\n",
    "\n",
    "    # Split data into train/validation/test sets\n",
    "    X_train_val, X_test = X_tensor[train_val_index], X_tensor[test_index]\n",
    "    y_train_val, y_test = y_tensor[train_val_index], y_tensor[test_index]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        test_size=1/8,\n",
    "        random_state=42 + fold,\n",
    "        stratify=y_train_val\n",
    "    )\n",
    "\n",
    "    # Compute class imbalance weight\n",
    "    num_pos = (y_train == 1).sum().item()\n",
    "    num_neg = (y_train == 0).sum().item()\n",
    "    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32).to(device)\n",
    "    print(f\"Fold {fold}: pos_weight = {pos_weight:.2f}\")\n",
    "\n",
    "    # DataLoader with deterministic worker initialization\n",
    "    def worker_init_fn(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "        random.seed(42 + worker_id)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # Model, optimizer, scheduler, and loss function\n",
    "    model = DNN(input_dim=X_tensor.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3\n",
    "    )\n",
    "\n",
    "    # Early stopping configuration\n",
    "    patience = 12\n",
    "    best_auroc = 0\n",
    "    wait = 0\n",
    "    best_model_path = os.path.join(save_path, f\"fold{fold}_best_model.pt\")\n",
    "\n",
    "    # =============== 6. Model training ===============\n",
    "    max_epochs = 100\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_val.to(device)).squeeze()\n",
    "            y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            y_true = y_val.squeeze().cpu().numpy()\n",
    "            auroc_val = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "        scheduler.step(auroc_val)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:03d} | \"\n",
    "            f\"Loss: {avg_loss:.4f} | \"\n",
    "            f\"Val AUROC: {auroc_val:.4f} | \"\n",
    "            f\"LR={current_lr:.6f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping criterion\n",
    "        if auroc_val > best_auroc:\n",
    "            best_auroc = auroc_val\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\n",
    "                    f\"Early stopping at epoch {epoch+1} \"\n",
    "                    f\"(best Val AUROC={best_auroc:.4f})\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "    # =============== 7. Test set evaluation ===============\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test.to(device)).squeeze()\n",
    "        y_pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        y_true = y_test.squeeze().cpu().numpy()\n",
    "\n",
    "        auroc = roc_auc_score(y_true, y_pred_prob)\n",
    "        auprc = average_precision_score(y_true, y_pred_prob)\n",
    "\n",
    "        auroc_list.append(auroc)\n",
    "        auprc_list.append(auprc)\n",
    "\n",
    "        print(f\"[Fold {fold}] Test AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "            \"y_test\": y_true,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        })\n",
    "        result_df.to_csv(\n",
    "            os.path.join(save_path, f\"fold{fold}_results.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "        all_results.append(result_df)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# =============== 8. Bootstrap estimation of overall performance ===============\n",
    "def bootstrap_metric_ci(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    metric_fn,\n",
    "    n_bootstrap=2000,\n",
    "    seed=42\n",
    "):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "\n",
    "    return (\n",
    "        np.mean(scores),\n",
    "        np.percentile(scores, 2.5),\n",
    "        np.percentile(scores, 97.5)\n",
    "    )\n",
    "\n",
    "all_results_df = pd.concat(all_results, axis=0).reset_index(drop=True)\n",
    "y_all_true = all_results_df[\"y_test\"].values\n",
    "y_all_pred = all_results_df[\"y_pred\"].values\n",
    "\n",
    "mean_auroc, auc_lower, auc_upper = bootstrap_metric_ci(\n",
    "    y_all_true,\n",
    "    y_all_pred,\n",
    "    metrics.roc_auc_score\n",
    ")\n",
    "mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "    y_all_true,\n",
    "    y_all_pred,\n",
    "    metrics.average_precision_score\n",
    ")\n",
    "\n",
    "print(\"\\n===== Five-fold cross-validation results (Bootstrap) =====\")\n",
    "print(\n",
    "    f\"AUROC: Mean = {mean_auroc:.4f}, \"\n",
    "    f\"95% CI = ({auc_lower:.4f}, {auc_upper:.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"AUPRC: Mean = {mean_auprc:.4f}, \"\n",
    "    f\"95% CI = ({auprc_lower:.4f}, {auprc_upper:.4f})\"\n",
    ")\n",
    "\n",
    "# =============== 9. Save merged predictions from all folds ===============\n",
    "all_results_path = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "all_results_df.to_csv(all_results_path, index=False)\n",
    "print(f\"\\n✅ All fold predictions have been merged and saved to: {all_results_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
