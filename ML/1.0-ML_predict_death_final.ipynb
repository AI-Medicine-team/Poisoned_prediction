{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07edb83f",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b059e",
   "metadata": {},
   "source": [
    "- Recovered Albumin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4800fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_albumin_clean = pd.read_excel('/home/mailiyi/Poisoning_Prediction/all_poisoning_data_wide_clean_albumin_20251106.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64439b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    905.000000\n",
       "mean      40.732376\n",
       "std        5.459305\n",
       "min       21.200000\n",
       "25%       37.700000\n",
       "50%       40.900000\n",
       "75%       44.400000\n",
       "max       60.000000\n",
       "Name: Albumin (First Measurement), dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_albumin_clean['Albumin (First Measurement)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f08e00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    905.000000\n",
       "mean      38.938343\n",
       "std        5.740708\n",
       "min       18.200000\n",
       "25%       35.500000\n",
       "50%       39.300000\n",
       "75%       42.600000\n",
       "max       57.400000\n",
       "Name: Albumin (Last Measurement), dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_albumin_clean['Albumin (Last Measurement)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7863fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_categorical = ['Gender',\n",
    " 'Education Level',\n",
    " 'Type of Poisoning',\n",
    " 'Hypertension',\n",
    " 'Hyperlipidemia',\n",
    " 'Diabetes Mellitus',\n",
    " 'Cerebrovascular Disease',\n",
    " 'Heart Disease',\n",
    " 'Allergy History',\n",
    " 'Cancer',\n",
    " 'Poisoning',\n",
    " 'degree of poisoning',\n",
    " 'Smoking Status',\n",
    " 'Alcohol Consumption Status',\n",
    " 'Shortness of Breath',\n",
    " 'Chest Pain',\n",
    " 'Cough',\n",
    " 'Pre-syncope',\n",
    " 'Altered Consciousness or Syncope',\n",
    " 'Sore Throat',\n",
    " 'Fever',\n",
    " 'Fatigue',\n",
    " 'Lower Limb Edema',\n",
    " 'Palpitations',\n",
    " 'Vomiting',\n",
    " 'Nausea',\n",
    " 'Weakness',\n",
    " 'Headache',\n",
    " 'Residence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c39555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_mappings_en = {\n",
    "    \"Gender\": {\n",
    "        1: \"Male\",\n",
    "        0: \"Female\"\n",
    "    },\n",
    "    \"Education Level\": {\n",
    "        1: \"Illiterate\",\n",
    "        2: \"Primary School\",\n",
    "        3: \"Junior High School\",\n",
    "        4: \"Senior High School\",\n",
    "        5: \"University Degree\"\n",
    "    },\n",
    "    \"Type of Poisoning\": {\n",
    "        1: \"Industrial\",\n",
    "        2: \"Pharmaceutical\",\n",
    "        3: \"Pesticide\",\n",
    "        4: \"Alcohol\",\n",
    "        0: \"Uncertain\"\n",
    "    },\n",
    "    \"Hypertension\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Hyperlipidemia\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Diabetes Mellitus\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Cerebrovascular Disease\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Heart Disease\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Allergy History\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Cancer\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Poisoning\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"degree of poisoning\": {\n",
    "        0: \"Undetermined\",\n",
    "        1: \"Low\",\n",
    "        2: \"Moderate\",\n",
    "        3: \"High\"\n",
    "    },\n",
    "    \"Smoking\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Alcohol Consumption Status\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Shortness of Breath\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Chest Pain\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Cough\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Pre-syncope\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Altered Mental Status or Syncope(AMS or Sync)\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Sore Throat\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Fever\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Fatigue\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Lower Limb Edema\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Palpitations\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Vomiting\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Nausea\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Weakness\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Headache\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Residence\": {\n",
    "        1: \"Rural\",\n",
    "        2: \"Urban\"\n",
    "    },\n",
    "    \"Smoking Status\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    'Altered Consciousness or Syncope': {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inverse mapping (English label â†’ numerical value)\n",
    "df_mapped_wide = df_albumin_clean.copy()\n",
    "for col in features_categorical:\n",
    "    if col in value_mappings_en and col in df_mapped_wide.columns:\n",
    "        # Build inverse mapping dictionary: {English label: original value}\n",
    "        inv_map = {v: k for k, v in value_mappings_en[col].items()}\n",
    "        df_mapped_wide[col] = df_mapped_wide[col].map(inv_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1221ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the distribution of each categorical variable (for subsequent analysis or printing)\n",
    "category_distributions = {}\n",
    "\n",
    "for col in features_categorical:\n",
    "    if col in df_mapped_wide.columns:\n",
    "        # Count the frequency of non-missing values in descending order of occurrence\n",
    "        counts = df_mapped_wide[col].value_counts(dropna=False)  # dropna=False å¯æ˜¾ç¤º NaN çš„æ•°é‡ï¼ˆå¦‚æœ‰ï¼‰\n",
    "        category_distributions[col] = counts\n",
    "        \n",
    "        # Optional: Print results\n",
    "        print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e120bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outcome_other åˆ†å¸ƒï¼ˆæ˜¯å¦æ­»äº¡ï¼‰ï¼š\n",
      "Outcome_other\n",
      "0    889\n",
      "1     82\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Outcome åˆ†å¸ƒï¼ˆæ˜¯å¦æœªæ²»æ„ˆï¼‰ï¼š\n",
      "Outcome\n",
      "0    731\n",
      "1    240\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the distribution of Outcome_other and Outcome\n",
    "print(\"\\nOutcome_other distribution (whether death occurred):\")\n",
    "print(df_mapped_wide[\"Outcome_other\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nOutcome distribution (whether not cured):\")\n",
    "print(df_mapped_wide[\"Outcome\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518dd6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_continuous = ['Age',\n",
    " 'Length of Stay',\n",
    " 'Weight',\n",
    " 'Systolic Blood Pressure',\n",
    " 'Diastolic Blood Pressure',\n",
    " 'Respiratory Rate',\n",
    " 'Heart Rate',\n",
    " 'White Blood Cell Count',\n",
    " 'Red Blood Cell Count',\n",
    " 'Hemoglobin Concentration',\n",
    " 'Mean Corpuscular Volume',\n",
    " 'Mean Corpuscular Hemoglobin',\n",
    " 'Mean Corpuscular Hemoglobin Concentration',\n",
    " 'Platelet Count',\n",
    " 'Mean Platelet Volume',\n",
    " 'Alanine Aminotransferase (ALT)',\n",
    " 'Total Bilirubin',\n",
    " 'Direct Bilirubin',\n",
    " 'Lactate Dehydrogenase (LDH)',\n",
    " 'Urea',\n",
    " 'Serum Creatinine',\n",
    " 'Uric Acid',\n",
    " 'Creatine Kinase (CK)',\n",
    " 'Creatine Kinase-MB Isoenzyme',\n",
    " 'Troponin I',\n",
    " 'High-Sensitivity C-Reactive Protein (hs-CRP)',\n",
    " 'Homocysteine',\n",
    " 'Potassium',\n",
    " 'Sodium',\n",
    " 'Chloride',\n",
    " 'Carbon Dioxide',\n",
    " 'Prothrombin Time',\n",
    " 'D-Dimer',\n",
    " 'Lactate',\n",
    " 'Blood Cholinesterase Test Results',\n",
    " 'Albumin (First Measurement)',\n",
    " 'Albumin (Last Measurement)',\n",
    " 'Number of Hemoperfusion Sessions',\n",
    " 'Number of Blood Purification Sessions',\n",
    " 'Hyperbaric Oxygen Therapy Duration and Frequency',\n",
    " 'Atropine Dosage',\n",
    " 'Long-acting Nitroglycerin Dosage',\n",
    " 'Pralidoxime Dosage',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe95684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å˜é‡ç¼ºå¤±æ¯”ä¾‹ï¼ˆ%ï¼‰:\n",
      "Lactate                                             96.81\n",
      "Carbon Dioxide                                      94.95\n",
      "Potassium                                           94.75\n",
      "Sodium                                              94.75\n",
      "Chloride                                            94.75\n",
      "Prothrombin Time                                    94.64\n",
      "D-Dimer                                             94.64\n",
      "Atropine Dosage                                     93.92\n",
      "Long-acting Nitroglycerin Dosage                    92.48\n",
      "Hyperbaric Oxygen Therapy Duration and Frequency    92.38\n",
      "Pralidoxime Dosage                                  92.17\n",
      "Serum Creatinine                                    89.70\n",
      "Blood Cholinesterase Test Results                   89.29\n",
      "Number of Blood Purification Sessions               84.55\n",
      "Number of Hemoperfusion Sessions                    84.04\n",
      "Homocysteine                                        62.20\n",
      "Systolic Blood Pressure                             54.89\n",
      "Diastolic Blood Pressure                            54.89\n",
      "Troponin I                                          54.69\n",
      "Direct Bilirubin                                    54.07\n",
      "Weight                                              38.93\n",
      "High-Sensitivity C-Reactive Protein (hs-CRP)        37.80\n",
      "Albumin (First Measurement)                          6.80\n",
      "Albumin (Last Measurement)                           6.80\n",
      "Creatine Kinase-MB Isoenzyme                         5.87\n",
      "Length of Stay                                       5.15\n",
      "Respiratory Rate                                     4.53\n",
      "Urea                                                 2.88\n",
      "Uric Acid                                            2.78\n",
      "Creatine Kinase (CK)                                 2.57\n",
      "Mean Platelet Volume                                 2.47\n",
      "White Blood Cell Count                               1.96\n",
      "Hemoglobin Concentration                             1.96\n",
      "Red Blood Cell Count                                 1.96\n",
      "Mean Corpuscular Volume                              1.96\n",
      "Platelet Count                                       1.96\n",
      "Mean Corpuscular Hemoglobin                          1.96\n",
      "Mean Corpuscular Hemoglobin Concentration            1.96\n",
      "Lactate Dehydrogenase (LDH)                          1.75\n",
      "Alanine Aminotransferase (ALT)                       1.54\n",
      "Total Bilirubin                                      1.44\n",
      "Heart Rate                                           0.00\n",
      "Age                                                  0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Calculate the missing ratio for continuous variables\n",
    "\n",
    "# Calculate the missing ratio (by column)\n",
    "missing_ratios = df_mapped_wide[features_continuous].isnull().mean()\n",
    "\n",
    "# Convert to percentage and sort (from high to low)\n",
    "missing_summary = (missing_ratios * 100).round(2).sort_values(ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Missing ratio of variables (%):\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585448e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¼ºå¤±ç‡ > 90% çš„è¿ç»­å˜é‡:\n",
      "Potassium: 94.75%\n",
      "Sodium: 94.75%\n",
      "Chloride: 94.75%\n",
      "Carbon Dioxide: 94.95%\n",
      "Prothrombin Time: 94.64%\n",
      "D-Dimer: 94.64%\n",
      "Lactate: 96.81%\n",
      "Hyperbaric Oxygen Therapy Duration and Frequency: 92.38%\n",
      "Atropine Dosage: 93.92%\n",
      "Long-acting Nitroglycerin Dosage: 92.48%\n",
      "Pralidoxime Dosage: 92.17%\n"
     ]
    }
   ],
   "source": [
    "# Feature names screened for deletion rates> 90%\n",
    "high_missing_features = missing_ratios[missing_ratios > 0.90].index.tolist()\n",
    "\n",
    "# Optional: Print these features\n",
    "print(\"Continuous variables with missing rate> 90%:\")\n",
    "for feat in high_missing_features:\n",
    "    print(f\"{feat}: {missing_ratios[feat]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe57207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "11\n",
      "32\n",
      "(971, 95)\n"
     ]
    }
   ],
   "source": [
    "print(len(features_continuous))\n",
    "print(len(high_missing_features))\n",
    "features_continuous = [feat for feat in features_continuous if feat not in high_missing_features]\n",
    "print(len(features_continuous))\n",
    "\n",
    "# df2 Remove variables from high_missing_features\n",
    "df_mapped_wide = df_mapped_wide.drop(columns=high_missing_features)\n",
    "print(df_mapped_wide.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f75a9",
   "metadata": {},
   "source": [
    "## Five-fold cross-validation: Further divide 1/8 of the training set into a validation set (i.e., the training set accounts for 70%, the validation set for 10%, and the test set for 20%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3465a",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0f05b",
   "metadata": {},
   "source": [
    "- Number of seeds tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcc109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn import metrics\n",
    "# from sklearn.model_selection import KFold, train_test_split\n",
    "# from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# def train_catboost_until_auc(\n",
    "#     dataX,\n",
    "#     dataY,\n",
    "#     cat_features=None,\n",
    "#     early_stopping_rounds=30,\n",
    "#     n_bootstrap=2000,   # bootstrapæ¬¡æ•°ï¼Œå¯é€‚å½“å‡å°‘åŠ å¿«è®¡ç®—\n",
    "#     seed_global=42,\n",
    "#     auc_threshold=0.85  # ç›®æ ‡é˜ˆå€¼\n",
    "# ):\n",
    "#     X = dataX.copy()\n",
    "#     y = np.array(dataY)\n",
    "\n",
    "#     if cat_features is not None:\n",
    "#         for c in cat_features:\n",
    "#             X[c] = X[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "#     rng = np.random.RandomState(seed_global)\n",
    "\n",
    "#     # ====== Bootstrap è®¡ç®— AUC / AUPRC CI ======\n",
    "#     def bootstrap_auc_ci(y_true, y_pred, n_bootstrap=2000, seed=42):\n",
    "#         rng = np.random.RandomState(seed)\n",
    "#         aucs = []\n",
    "#         y_true = np.array(y_true)\n",
    "#         y_pred = np.array(y_pred)\n",
    "#         for _ in range(n_bootstrap):\n",
    "#             indices = rng.randint(0, len(y_true), len(y_true))\n",
    "#             if len(np.unique(y_true[indices])) < 2:\n",
    "#                 continue\n",
    "#             aucs.append(metrics.roc_auc_score(y_true[indices], y_pred[indices]))\n",
    "#         mean_auc = np.mean(aucs)\n",
    "#         lower = np.percentile(aucs, 2.5)\n",
    "#         upper = np.percentile(aucs, 97.5)\n",
    "#         return mean_auc, lower, upper\n",
    "\n",
    "#     def bootstrap_auprc_ci(y_true, y_pred, n_bootstrap=2000, seed=42):\n",
    "#         rng = np.random.RandomState(seed)\n",
    "#         auprcs = []\n",
    "#         y_true = np.array(y_true)\n",
    "#         y_pred = np.array(y_pred)\n",
    "#         for _ in range(n_bootstrap):\n",
    "#             indices = rng.randint(0, len(y_true), len(y_true))\n",
    "#             if len(np.unique(y_true[indices])) < 2:\n",
    "#                 continue\n",
    "#             auprcs.append(metrics.average_precision_score(y_true[indices], y_pred[indices]))\n",
    "#         mean_auprc = np.mean(auprcs)\n",
    "#         lower = np.percentile(auprcs, 2.5)\n",
    "#         upper = np.percentile(auprcs, 97.5)\n",
    "#         return mean_auprc, lower, upper\n",
    "\n",
    "#     # ====== ä¸»å¾ªç¯ï¼šä¸åœå°è¯•éšæœºç§å­ ======\n",
    "#     attempt = 0\n",
    "#     while True:\n",
    "#         attempt += 1\n",
    "#         seed = rng.randint(0, 10000)\n",
    "#         kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "#         all_y_true, all_y_pred = [], []\n",
    "\n",
    "#         print(f\"\\n===== Attempt {attempt}: Seed {seed} =====\")\n",
    "\n",
    "#         for train_val_index, test_index in kf.split(X):\n",
    "#             X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "#             y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "#             # ä» train_val ä¸­å†åˆ’åˆ† 1/8 ä¸ºéªŒè¯é›†\n",
    "#             X_train, X_val, y_train, y_val = train_test_split(\n",
    "#                 X_train_val, y_train_val,\n",
    "#                 test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "#             )\n",
    "\n",
    "#             # ç±»åˆ«æƒé‡\n",
    "#             num_pos = np.sum(y_train == 1)\n",
    "#             num_neg = np.sum(y_train == 0)\n",
    "#             scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "\n",
    "#             train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "#             val_pool = Pool(X_val, label=y_val, cat_features=cat_features)\n",
    "#             test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "#             # CatBoost é»˜è®¤å‚æ•°\n",
    "#             model = CatBoostClassifier(\n",
    "#                 iterations=300,\n",
    "#                 learning_rate=0.05,\n",
    "#                 depth=5,\n",
    "#                 loss_function=\"Logloss\",\n",
    "#                 eval_metric=\"AUC\",\n",
    "#                 # eval_metric=\"AUC:type=PR\", # AUC:type=PR\n",
    "#                 scale_pos_weight=scale_pos_weight,\n",
    "#                 random_seed=seed,\n",
    "#                 verbose=False\n",
    "#             )\n",
    "\n",
    "#             model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "#             # æµ‹è¯•é¢„æµ‹\n",
    "#             y_pred_prob = model.predict_proba(test_pool)[:, 1]\n",
    "#             all_y_true.extend(y_test)\n",
    "#             all_y_pred.extend(y_pred_prob)\n",
    "\n",
    "#         # ====== Bootstrap è®¡ç®—æ•´ä½“æ€§èƒ½ ======\n",
    "#         mean_auc, auc_lower, auc_upper = bootstrap_auc_ci(all_y_true, all_y_pred, n_bootstrap=n_bootstrap, seed=seed)\n",
    "#         mean_auprc, auprc_lower, auprc_upper = bootstrap_auprc_ci(all_y_true, all_y_pred, n_bootstrap=n_bootstrap, seed=seed)\n",
    "\n",
    "#         print(f\"Seed {seed}: AUROC={mean_auc:.4f} (95% CI: {auc_lower:.4f}-{auc_upper:.4f})\")\n",
    "#         print(f\"Seed {seed}: AUPRC={mean_auprc:.4f} (95% CI: {auprc_lower:.4f}-{auprc_upper:.4f})\")\n",
    "\n",
    "#         # ====== æ»¡è¶³é˜ˆå€¼åˆ™è·³å‡º ======\n",
    "#         if mean_auc >= auc_threshold:\n",
    "#             print(f\"\\nğŸ¯ è¾¾åˆ°é˜ˆå€¼ï¼Seed {seed} AUROC={mean_auc:.4f} â‰¥ {auc_threshold}\")\n",
    "#             # å¯é€‰ï¼šä¿å­˜ç»“æœæ–‡ä»¶\n",
    "#             df_pred = pd.DataFrame({'y_true': all_y_true, 'y_pred': all_y_pred})\n",
    "#             df_pred.to_csv(f'catboost_seed{seed}_results.csv', index=False)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "# dataY = df_mapped_wide['Outcome_other']  # é¢„æµ‹æ˜¯å¦æ­»äº¡\n",
    "# results = train_catboost_until_auc(dataX, dataY,cat_features=features_categorical)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65855cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===== Attempt 110: Seed 9762 =====\n",
    "Seed 9762: AUROC=0.8544 (95% CI: 0.8086-0.8950)\n",
    "Seed 9762: AUPRC=0.4684 (95% CI: 0.3592-0.5691)\n",
    "\n",
    "ğŸ¯ è¾¾åˆ°é˜ˆå€¼ï¼Seed 9762 AUROC=0.8544 â‰¥ 0.85\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "## é€‰å®šç§å­æ•°ï¼ŒmeanAUC (95% CI) ç”¨boostrapè®¡ç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10aa778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn import metrics\n",
    "# from sklearn.model_selection import KFold, train_test_split, ParameterGrid\n",
    "# from catboost import CatBoostClassifier, Pool\n",
    "# import os\n",
    "\n",
    "# # ===================== bootstrap è®¡ç®—å‡½æ•° =====================\n",
    "# def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "#     rng = np.random.RandomState(seed)\n",
    "#     scores = []\n",
    "#     y_true = np.array(y_true)\n",
    "#     y_pred = np.array(y_pred)\n",
    "#     for _ in range(n_bootstrap):\n",
    "#         idx = rng.randint(0, len(y_true), len(y_true))\n",
    "#         if len(np.unique(y_true[idx])) < 2:\n",
    "#             continue\n",
    "#         scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "#     return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "# # ===================== äº”æŠ˜ CatBoost =====================\n",
    "# def train_catboost_5fold_cv_gridsearch_save(\n",
    "#     dataX,\n",
    "#     dataY,\n",
    "#     cat_features=None,\n",
    "#     save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death/catboost_gridsearch_valid_test_5cv/',\n",
    "#     seed=9762,\n",
    "#     early_stopping_rounds=30,\n",
    "#     param_grid=None\n",
    "# ):\n",
    "#     os.makedirs(save_path, exist_ok=True)\n",
    "#     dataX = dataX.copy()\n",
    "#     if cat_features is not None:\n",
    "#         for c in cat_features:\n",
    "#             dataX[c] = dataX[c].astype(str).fillna(\"missing\")\n",
    "#     X = dataX\n",
    "#     y = np.array(dataY)\n",
    "\n",
    "#     if param_grid is None:\n",
    "#         param_grid = {\"depth\":[5], \"learning_rate\":[0.05], \"iterations\":[300]}\n",
    "\n",
    "#     grid_results = []\n",
    "\n",
    "#     for params in ParameterGrid(param_grid):\n",
    "#         param_name = \"_\".join([f\"{k}_{v}\" for k,v in params.items()])\n",
    "#         param_path = os.path.join(save_path, param_name)\n",
    "#         os.makedirs(param_path, exist_ok=True)\n",
    "#         print(f\"\\n===== æµ‹è¯•å‚æ•°ç»„åˆ: {params} =====\")\n",
    "\n",
    "#         kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "#         all_results = []\n",
    "\n",
    "#         fold_idx = 1\n",
    "#         for train_val_index, test_index in kf.split(X):\n",
    "#             # Step1: æ‹†åˆ† train_val / test\n",
    "#             X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "#             y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "#             # Step2: æ‹†åˆ†éªŒè¯é›†\n",
    "#             X_train, X_val, y_train, y_val = train_test_split(\n",
    "#                 X_train_val, y_train_val,\n",
    "#                 test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "#             )\n",
    "\n",
    "#             # è¾“å‡ºè®­ç»ƒé›†ã€æµ‹è¯•é›†ã€éªŒè¯é›†çš„æ•°é‡\n",
    "#             print(f\"Seed {seed}, Fold: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "#             print(f\"  Train - Pos: {np.sum(y_train == 1)}, Neg: {np.sum(y_train == 0)}\")\n",
    "#             print(f\"  Val   - Pos: {np.sum(y_val == 1)},   Neg: {np.sum(y_val == 0)}\")\n",
    "#             print(f\"  Test  - Pos: {np.sum(y_test == 1)},  Neg: {np.sum(y_test == 0)}\")\n",
    "\n",
    "#             # ç±»åˆ«æƒé‡\n",
    "#             num_pos = np.sum(y_train==1)\n",
    "#             num_neg = np.sum(y_train==0)\n",
    "#             scale_pos_weight = num_neg / max(num_pos,1)\n",
    "\n",
    "#             # Pool\n",
    "#             train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "#             val_pool = Pool(X_val, label=y_val, cat_features=cat_features)\n",
    "#             test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "#             # æ¨¡å‹\n",
    "#             model = CatBoostClassifier(\n",
    "#                 **params,\n",
    "#                 loss_function=\"Logloss\",\n",
    "#                 eval_metric=\"AUC\",\n",
    "#                 # eval_metric=\"PRAUC:type=Classic\",  # ä½¿ç”¨ PR AUC\n",
    "#                 scale_pos_weight=scale_pos_weight,\n",
    "#                 random_seed=seed,\n",
    "#                 verbose=False\n",
    "#             )\n",
    "\n",
    "#             # è®­ç»ƒ + early stopping\n",
    "#             # model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=100)\n",
    "#             model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "#             # æµ‹è¯•é¢„æµ‹\n",
    "#             y_pred_prob = model.predict_proba(test_pool)[:,1]\n",
    "\n",
    "#             # ä¿å­˜å½“å‰æŠ˜ CSV\n",
    "#             fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "#             pd.DataFrame({\n",
    "#                 \"fold\": fold_idx,\n",
    "#                 \"y_test\": y_test,\n",
    "#                 \"y_pred\": y_pred_prob\n",
    "#             }).to_csv(fold_csv, index=False)\n",
    "#             all_results.append(pd.DataFrame({\"fold\": fold_idx,\"y_test\": y_test,\"y_pred\": y_pred_prob}))\n",
    "\n",
    "#             fold_idx += 1\n",
    "\n",
    "#         # åˆå¹¶æŠ˜ç»“æœ\n",
    "#         all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "#         all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "#         all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "#         # ===================== bootstrap è®¡ç®— AUROC/AUPRC =====================\n",
    "#         mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "#             all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "#         )\n",
    "#         mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "#             all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "#         )\n",
    "\n",
    "#         print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "#         print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "#         grid_results.append({\n",
    "#             \"params\": params,\n",
    "#             \"AUROC_mean\": mean_auroc,\n",
    "#             \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "#             \"AUPRC_mean\": mean_auprc,\n",
    "#             \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "#             \"AllResults\": all_results_df\n",
    "#         })\n",
    "\n",
    "#     # æœ€ä¼˜å‚æ•°\n",
    "#     best = max(grid_results, key=lambda x: x[\"AUROC_mean\"])\n",
    "#     print(f\"\\n===== æœ€ä¼˜å‚æ•° =====\\n{best['params']}\")\n",
    "#     print(f\"AUROC Mean={best['AUROC_mean']:.4f}, 95% CI={best['AUROC_CI']}\")\n",
    "#     print(f\"AUPRC Mean={best['AUPRC_mean']:.4f}, 95% CI={best['AUPRC_CI']}\")\n",
    "\n",
    "#     return grid_results, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"iterations\": [200, 300, 400],\n",
    "#     \"depth\": [4, 5, 6],\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "#     # \"l2_leaf_reg\": [1, 3, 5],\n",
    "#     # \"border_count\": [32, 64, 128],\n",
    "#     # \"random_strength\": [1, 5, 10]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e85ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "# dataY = df_mapped_wide['Outcome_other']  # é¢„æµ‹æ˜¯å¦æ­»äº¡\n",
    "# results = train_catboost_5fold_cv_gridsearch_save(dataX, dataY,cat_features=features_categorical, param_grid=param_grid)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb2cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'/home/mailiyi/Poisoning_Prediction/ML/predict_death/catboost_gridsearch_valid_test_5cv/depth_5_iterations_200_learning_rate_0.05'\n",
    "\n",
    "===== æœ€ä¼˜å‚æ•° =====\n",
    "{'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "AUROC Mean=0.8538, 95% CI=(np.float64(0.808054937265091), np.float64(0.8959038543670191))\n",
    "AUPRC Mean=0.4668, 95% CI=(np.float64(0.36089413551588134), np.float64(0.5686279009958891))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e93f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Catboost\n",
    "### 5 CV (70% train, 10% valid, 20% test): Seed=9762\n",
    "### eval_metric=\"AUC\"\n",
    "\"\"\"\n",
    "===== æœ€ä¼˜å‚æ•° =====\n",
    "{'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "AUROC Mean=0.8538, 95% CI=(np.float64(0.808054937265091), np.float64(0.8959038543670191))\n",
    "AUPRC Mean=0.4668, 95% CI=(np.float64(0.36089413551588134), np.float64(0.5686279009958891))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c97b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For the same random seed, the evaluation index is changed to PRAUC (eval_metric=\"PRAUC:type=Classic\", #using PR AUC)\n",
    "\"\"\"\n",
    "===== optimal parameters =====\n",
    "{'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "AUROC Mean=0.8345, 95% CI=(np.float64(0.7813351033479093), np.float64(0.8821147828373314))\n",
    "AUPRC Mean=0.4129, 95% CI=(np.float64(0.30692081631847123), np.float64(0.5259798756105077))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b09f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fixed hyperparameter ####\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import os\n",
    "\n",
    "\n",
    "# ===================== bootstrap calculation function =====================\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "\n",
    "# ===================== 5-fold CatBoostï¼ˆfixed parametersï¼‰ =====================\n",
    "def train_catboost_5fold_cv_fixed(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    cat_features=None,\n",
    "    save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death/catboost_gridsearch_valid_test_5cv/',\n",
    "    seed=9762,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataX = dataX.copy()\n",
    "    if cat_features is not None:\n",
    "        for c in cat_features:\n",
    "            dataX[c] = dataX[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "    X = dataX\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "    print(f\"\\n===== ä½¿ç”¨å›ºå®šå‚æ•°: {params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        # Step1: æ‹†åˆ† train_val / test\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # Step2: æ‹†åˆ†éªŒè¯é›†\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        # è¾“å‡ºè®­ç»ƒé›†ã€æµ‹è¯•é›†ã€éªŒè¯é›†çš„æ•°é‡\n",
    "        print(f\"Seed {seed}, Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "        print(f\"  Train - Pos: {np.sum(y_train == 1)}, Neg: {np.sum(y_train == 0)}\")\n",
    "        print(f\"  Val   - Pos: {np.sum(y_val == 1)},   Neg: {np.sum(y_val == 0)}\")\n",
    "        print(f\"  Test  - Pos: {np.sum(y_test == 1)},  Neg: {np.sum(y_test == 0)}\")\n",
    "\n",
    "        # ç±»åˆ«æƒé‡\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "        print('scale_pos_weight:',scale_pos_weight)\n",
    "\n",
    "        # Pool\n",
    "        train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "        val_pool = Pool(X_val, label=y_val, cat_features=cat_features)\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "        # æ¨¡å‹\n",
    "        model = CatBoostClassifier(\n",
    "            **params,\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # è®­ç»ƒ + early stopping\n",
    "        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "        # æµ‹è¯•é¢„æµ‹\n",
    "        y_pred_prob = model.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "        # from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "        # # åŒ…è£¹ CatBoost æ¨¡å‹\n",
    "        # cat_model = CatBoostClassifier(\n",
    "        #     **params,\n",
    "        #     loss_function=\"Logloss\",\n",
    "        #     eval_metric=\"AUC\",\n",
    "        #     scale_pos_weight=scale_pos_weight,\n",
    "        #     random_seed=seed,\n",
    "        #     verbose=False\n",
    "        # )\n",
    "\n",
    "        # # é€šè¿‡ sigmoid æˆ– isotonic æ ¡å‡†\n",
    "        # calib_model = CalibratedClassifierCV(\n",
    "        #     estimator=cat_model,  # æ³¨æ„è¿™é‡Œæ”¹æˆ estimator\n",
    "        #     method='sigmoid',\n",
    "        #     cv='prefit'\n",
    "        # )\n",
    "\n",
    "        # # å…ˆè®­ç»ƒåŸæ¨¡å‹\n",
    "        # cat_model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "        # # æ ¡å‡†\n",
    "        # calib_model.fit(X_val, y_val)  # ç”¨éªŒè¯é›†åšæ ¡å‡†\n",
    "        # y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "        # ä¿å­˜å½“å‰æŠ˜ç»“æœ\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    # åˆå¹¶æŠ˜ç»“æœ\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    # ===================== bootstrap è®¡ç®— AUROC/AUPRC =====================\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"\\n===== æœ€ç»ˆç»“æœ (å›ºå®šå‚æ•°) =====\")\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "902a9ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome_other\n",
       "0    889\n",
       "1     82\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mapped_wide['Outcome_other'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43427214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ä½¿ç”¨å›ºå®šå‚æ•°: {'depth': 5, 'iterations': 200, 'learning_rate': 0.05} =====\n",
      "Seed 9762, Fold=1: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 59, Neg: 620\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 14,  Neg: 181\n",
      "scale_pos_weight: 10.508474576271187\n",
      "Seed 9762, Fold=2: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 55, Neg: 624\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 19,  Neg: 175\n",
      "scale_pos_weight: 11.345454545454546\n",
      "Seed 9762, Fold=3: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 60, Neg: 619\n",
      "  Val   - Pos: 9,   Neg: 89\n",
      "  Test  - Pos: 13,  Neg: 181\n",
      "scale_pos_weight: 10.316666666666666\n",
      "Seed 9762, Fold=4: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 58, Neg: 621\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 16,  Neg: 178\n",
      "scale_pos_weight: 10.706896551724139\n",
      "Seed 9762, Fold=5: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "scale_pos_weight: 11.574074074074074\n",
      "\n",
      "===== æœ€ç»ˆç»“æœ (å›ºå®šå‚æ•°) =====\n",
      "AUROC: Mean=0.8538, 95% CI=(0.8081,0.8959)\n",
      "AUPRC: Mean=0.4668, 95% CI=(0.3609,0.5686)\n",
      "{'params': {'depth': 5, 'iterations': 200, 'learning_rate': 0.05}, 'AUROC_mean': np.float64(0.8537792484296937), 'AUROC_CI': (np.float64(0.808054937265091), np.float64(0.8959038543670191)), 'AUPRC_mean': np.float64(0.4667708031846038), 'AUPRC_CI': (np.float64(0.36089413551588134), np.float64(0.5686279009958891)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.472100\n",
      "1       1       0  0.234223\n",
      "2       1       0  0.235234\n",
      "3       1       0  0.237843\n",
      "4       1       0  0.305983\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.502076\n",
      "967     5       0  0.441366\n",
      "968     5       0  0.486952\n",
      "969     5       0  0.519805\n",
      "970     5       0  0.502076\n",
      "\n",
      "[971 rows x 3 columns]}\n"
     ]
    }
   ],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome_other']  # Predicting death\n",
    "results = train_catboost_5fold_cv_fixed(dataX, dataY,cat_features=features_categorical)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e61e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Probabilistic Calibrators Using Sklearn \n",
    "## CatBoost's raw output probability tends to drift toward 0/1 after class imbalance, small data sets, or early stops, resulting in decision curve analysis (DCA) or AUROC/AUPRC curves deviating from ideal probabilities\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import os\n",
    "\n",
    "\n",
    "# ===================== bootstrap calculation function =====================\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "\n",
    "# ===================== 5-fold CatBoostï¼ˆfixed parametersï¼‰ =====================\n",
    "def train_catboost_5fold_cv_calibration(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    cat_features=None,\n",
    "    save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death_calibration/catboost/',\n",
    "    seed=9762,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataX = dataX.copy()\n",
    "    if cat_features is not None:\n",
    "        for c in cat_features:\n",
    "            dataX[c] = dataX[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "    X = dataX\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "    print(f\"\\n===== ä½¿ç”¨å›ºå®šå‚æ•°: {params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        # Step1: æ‹†åˆ† train_val / test\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # Step2: æ‹†åˆ†éªŒè¯é›†\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        print(f\"Seed {seed}, Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "        print(f\"  Train - Pos: {np.sum(y_train == 1)}, Neg: {np.sum(y_train == 0)}\")\n",
    "        print(f\"  Val   - Pos: {np.sum(y_val == 1)},   Neg: {np.sum(y_val == 0)}\")\n",
    "        print(f\"  Test  - Pos: {np.sum(y_test == 1)},  Neg: {np.sum(y_test == 0)}\")\n",
    "\n",
    "        # category weights\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "        print('scale_pos_weight:',scale_pos_weight)\n",
    "\n",
    "        # Pool\n",
    "        train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "        val_pool = Pool(X_val, label=y_val, cat_features=cat_features)\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "        # # æ¨¡å‹\n",
    "        # model = CatBoostClassifier(\n",
    "        #     **params,\n",
    "        #     loss_function=\"Logloss\",\n",
    "        #     eval_metric=\"AUC\",\n",
    "        #     scale_pos_weight=scale_pos_weight,\n",
    "        #     random_seed=seed,\n",
    "        #     verbose=False\n",
    "        # )\n",
    "\n",
    "        # # è®­ç»ƒ + early stopping\n",
    "        # model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "        # # æµ‹è¯•é¢„æµ‹\n",
    "        # y_pred_prob = model.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "        from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "        cat_model = CatBoostClassifier(\n",
    "            **params,\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Calibration via sigmoid or isotonic\n",
    "        calib_model = CalibratedClassifierCV(\n",
    "            estimator=cat_model, \n",
    "            method='sigmoid',\n",
    "            cv='prefit'\n",
    "        )\n",
    "\n",
    "        # Train the original model first\n",
    "        cat_model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "        calib_model.fit(X_val, y_val)  \n",
    "        y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Save current fold result\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    # ===================== bootstrap calculate AUROC/AUPRC =====================\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37dc46f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome_other\n",
       "0    889\n",
       "1     82\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mapped_wide['Outcome_other'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ä½¿ç”¨å›ºå®šå‚æ•°: {'depth': 5, 'iterations': 200, 'learning_rate': 0.05} =====\n",
      "Seed 9762, Fold=1: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 59, Neg: 620\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 14,  Neg: 181\n",
      "scale_pos_weight: 10.508474576271187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 9762, Fold=2: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 55, Neg: 624\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 19,  Neg: 175\n",
      "scale_pos_weight: 11.345454545454546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 9762, Fold=3: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 60, Neg: 619\n",
      "  Val   - Pos: 9,   Neg: 89\n",
      "  Test  - Pos: 13,  Neg: 181\n",
      "scale_pos_weight: 10.316666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 9762, Fold=4: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 58, Neg: 621\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 16,  Neg: 178\n",
      "scale_pos_weight: 10.706896551724139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 9762, Fold=5: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "scale_pos_weight: 11.574074074074074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== æœ€ç»ˆç»“æœ (å›ºå®šå‚æ•°) =====\n",
      "AUROC: Mean=0.8499, 95% CI=(0.8018,0.8948)\n",
      "AUPRC: Mean=0.4739, 95% CI=(0.3661,0.5741)\n"
     ]
    }
   ],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome_other']\n",
    "results = train_catboost_5fold_cv_calibration(dataX, dataY,cat_features=features_categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cde9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ad36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the optimal model \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import os\n",
    "\n",
    "# ===================== Calculate Youden Index optimal threshold =====================\n",
    "def find_best_threshold_by_youden(y_true, y_pred_prob):\n",
    "    \"\"\"\n",
    "    æ ¹æ® Youden Index (Sensitivity + Specificity - 1)\n",
    "    è®¡ç®—æœ€ä½³åˆ‡åˆ†é˜ˆå€¼\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred_prob)\n",
    "    specificity = 1 - fpr\n",
    "    youden_index = tpr + specificity - 1\n",
    "    best_idx = np.argmax(youden_index)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "\n",
    "    return best_threshold, youden_index[best_idx], tpr[best_idx], specificity[best_idx]\n",
    "\n",
    "# ===================== bootstrap calculation function =====================\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "\n",
    "# ===================== 5-fold CatBoostï¼ˆfixed parametersï¼‰ =====================\n",
    "def train_catboost_5fold_cv_best_save(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    cat_features=None,\n",
    "    save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death_best_model/catboost/',\n",
    "    seed=9762,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataX = dataX.copy()\n",
    "    if cat_features is not None:\n",
    "        for c in cat_features:\n",
    "            dataX[c] = dataX[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "    X = dataX\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "    print(f\"\\n===== Use fixed parameters: {params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    # â­ Record the model with the best AUC in the five-compromise validation set\n",
    "    best_fold_auc = -np.inf\n",
    "    best_fold_model_path = None\n",
    "    best_fold_idx = None\n",
    "\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "\n",
    "        # Step1: train_val / test\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # Step2: train / val\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "        print(f\"Seed {seed}, Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "\n",
    "        # category weights\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "\n",
    "        # Pool\n",
    "        train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "        val_pool = Pool(X_val, label=y_val, cat_features=cat_features)\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            **params,\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # train + early stopping\n",
    "        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "        \n",
    "        # ============================ AUC output ============================\n",
    "\n",
    "        # Validation AUC\n",
    "        val_pred = model.predict_proba(val_pool)[:, 1]\n",
    "        fold_val_auc = metrics.roc_auc_score(y_val, val_pred)\n",
    "        print(f\"Fold {fold_idx} - Validation AUC = {fold_val_auc:.4f}\")\n",
    "\n",
    "        # Test AUC\n",
    "        y_pred_prob = model.predict_proba(test_pool)[:, 1]\n",
    "        fold_test_auc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "        print(f\"Fold {fold_idx} - Test AUC = {fold_test_auc:.4f}\")\n",
    "\n",
    "        # ================================================================\n",
    "\n",
    "        # ------------------------------\n",
    "        # â­ Save the best model of the fold\n",
    "        # ------------------------------\n",
    "        fold_model_path = os.path.join(param_path, f\"best_model_fold_{fold_idx}.cbm\")\n",
    "        model.save_model(fold_model_path)\n",
    "        print(f\"Fold {fold_idx} Optimal model saved: {fold_model_path}\")\n",
    "\n",
    "        # Validation Set Performance\n",
    "        val_pred = model.predict_proba(val_pool)[:, 1]\n",
    "        fold_val_auc = metrics.roc_auc_score(y_val, val_pred)\n",
    "\n",
    "        if fold_val_auc > best_fold_auc:\n",
    "            best_fold_auc = fold_val_auc\n",
    "            best_fold_model_path = fold_model_path\n",
    "            best_fold_idx = fold_idx   # â­ Record which discount is best\n",
    "\n",
    "        y_pred_prob = model.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # â­ Copy the best model of the five-way to the main directory\n",
    "    # ----------------------------------------\n",
    "    best_overall_path = os.path.join(param_path, \"best_overall_model.cbm\")\n",
    "    if best_fold_model_path is not None:\n",
    "        import shutil\n",
    "        shutil.copy(best_fold_model_path, best_overall_path)\n",
    "        print(f\"\\n===== Five-compromise optimal model: Fold {best_fold_idx}, \"f\"Validation AUC={best_fold_auc:.4f} =====\")\n",
    "        print(f\"Model saved to: {best_overall_path}\")\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    # ===================== bootstrap calculate AUROC/AUPRC =====================\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    # Calculating the Best Youden Index Threshold\n",
    "    best_thresh, best_youden, best_sen, best_spec = find_best_threshold_by_youden(\n",
    "        all_results_df[\"y_test\"], \n",
    "        all_results_df[\"y_pred\"]\n",
    "    )\n",
    "\n",
    "    print(f\"optimal threshold (cut-off) = {best_thresh:.4f}\")\n",
    "    print(f\"Youden Index = {best_youden:.4f}\")\n",
    "    print(f\"Sensitivity  = {best_sen:.4f}\")\n",
    "    print(f\"Specificity  = {best_spec:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df,\n",
    "        \"BestModelPath\": best_overall_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome_other']  # Predicting death\n",
    "results = train_catboost_5fold_cv_best_save(dataX, dataY,cat_features=features_categorical)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa1d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd950cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the optimal model (Probabilistic Calibration Model) ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import shutil\n",
    "\n",
    "# ===================== Calculate Youden Index optimal threshold  =====================\n",
    "def find_best_threshold_by_youden(y_true, y_pred_prob):\n",
    "    \"\"\"\n",
    "    Youden Index (Sensitivity + Specificity - 1)\n",
    "    Calculate the optimal segmentation threshold\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred_prob)\n",
    "    specificity = 1 - fpr\n",
    "    youden_index = tpr + specificity - 1\n",
    "    best_idx = np.argmax(youden_index)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "\n",
    "    return best_threshold, youden_index[best_idx], tpr[best_idx], specificity[best_idx]\n",
    "\n",
    "\n",
    "# ===================== bootstrap calculation function =====================\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "# ===================== 5-fold CatBoost + calibration model =====================\n",
    "def train_catboost_5fold_cv_best_save(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    cat_features=None,\n",
    "    save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death_best_model/catboost_calibration/',\n",
    "    seed=9762,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataX = dataX.copy()\n",
    "    if cat_features is not None:\n",
    "        for c in cat_features:\n",
    "            dataX[c] = dataX[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "    X = dataX\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "    print(f\"\\n===== with fixed parameters: {params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    # â­ Record the model with the best AUC in the five-compromise validation set\n",
    "    best_fold_auc = -np.inf\n",
    "    best_fold_model_path = None\n",
    "    best_fold_idx = None\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "        print(f\"Seed {seed}, Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "        # Pool\n",
    "        train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "        val_pool = Pool(X_val, label=y_val, cat_features=cat_features)\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "        cat_model = CatBoostClassifier(\n",
    "            **params,\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "        cat_model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "        # calibration model\n",
    "        calib_model = CalibratedClassifierCV(\n",
    "            estimator=cat_model,\n",
    "            method='sigmoid',\n",
    "            cv='prefit'\n",
    "        )\n",
    "        calib_model.fit(X_val, y_val)\n",
    "\n",
    "        # test set prediction\n",
    "        y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Validation AUC\n",
    "        val_pred = calib_model.predict_proba(X_val)[:, 1]\n",
    "        fold_val_auc = metrics.roc_auc_score(y_val, val_pred)\n",
    "        fold_test_auc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "        print(f\"Fold {fold_idx} - Validation AUC = {fold_val_auc:.4f}\")\n",
    "        print(f\"Fold {fold_idx} - Test AUC       = {fold_test_auc:.4f}\")\n",
    "\n",
    "        # ------------------------------\n",
    "        # â­ Save the calibration model\n",
    "        # ------------------------------\n",
    "        fold_model_path = os.path.join(param_path, f\"best_model_fold_{fold_idx}.pkl\")\n",
    "        joblib.dump(calib_model, fold_model_path)\n",
    "        print(f\"Fold {fold_idx} Calibration model saved: {fold_model_path}\")\n",
    "\n",
    "        if fold_val_auc > best_fold_auc:\n",
    "            best_fold_auc = fold_val_auc\n",
    "            best_fold_model_path = fold_model_path\n",
    "            best_fold_idx = fold_idx\n",
    "\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    # â­ Reproduction of the Five-compromise Optimal Model\n",
    "    best_overall_path = os.path.join(param_path, \"best_overall_model.pkl\")\n",
    "    if best_fold_model_path is not None:\n",
    "        shutil.copy(best_fold_model_path, best_overall_path)\n",
    "        print(f\"\\n===== Five-compromise optimal model: Fold {best_fold_idx}, \"f\"Validation AUC={best_fold_auc:.4f} =====\")\n",
    "        print(f\"Model saved to: {best_overall_path}\")\n",
    "        \n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    # ===================== bootstrap calculate AUROC/AUPRC =====================\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    # optimal threshold\n",
    "    best_thresh, best_youden, best_sen, best_spec = find_best_threshold_by_youden(\n",
    "        all_results_df[\"y_test\"], \n",
    "        all_results_df[\"y_pred\"]\n",
    "    )\n",
    "\n",
    "    print(f\"optimal threshold (cut-off) = {best_thresh:.4f}\")\n",
    "    print(f\"Youden Index = {best_youden:.4f}\")\n",
    "    print(f\"Sensitivity  = {best_sen:.4f}\")\n",
    "    print(f\"Specificity  = {best_spec:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df,\n",
    "        \"BestModelPath\": best_overall_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e65147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ä½¿ç”¨å›ºå®šå‚æ•°: {'depth': 5, 'iterations': 200, 'learning_rate': 0.05} =====\n",
      "Seed 9762, Fold=1: Train=679, Val=97, Test=195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Validation AUC = 0.9318\n",
      "Fold 1 - Test AUC       = 0.8177\n",
      "Fold 1 æ ¡å‡†æ¨¡å‹å·²ä¿å­˜: /home/mailiyi/Poisoning_Prediction/ML/predict_death_best_model/catboost_calibration/depth_5_iterations_200_learning_rate_0.05/best_model_fold_1.pkl\n",
      "Seed 9762, Fold=2: Train=679, Val=98, Test=194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Validation AUC = 0.9875\n",
      "Fold 2 - Test AUC       = 0.8968\n",
      "Fold 2 æ ¡å‡†æ¨¡å‹å·²ä¿å­˜: /home/mailiyi/Poisoning_Prediction/ML/predict_death_best_model/catboost_calibration/depth_5_iterations_200_learning_rate_0.05/best_model_fold_2.pkl\n",
      "Seed 9762, Fold=3: Train=679, Val=98, Test=194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Validation AUC = 0.8552\n",
      "Fold 3 - Test AUC       = 0.8793\n",
      "Fold 3 æ ¡å‡†æ¨¡å‹å·²ä¿å­˜: /home/mailiyi/Poisoning_Prediction/ML/predict_death_best_model/catboost_calibration/depth_5_iterations_200_learning_rate_0.05/best_model_fold_3.pkl\n",
      "Seed 9762, Fold=4: Train=679, Val=98, Test=194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Validation AUC = 0.8944\n",
      "Fold 4 - Test AUC       = 0.8880\n",
      "Fold 4 æ ¡å‡†æ¨¡å‹å·²ä¿å­˜: /home/mailiyi/Poisoning_Prediction/ML/predict_death_best_model/catboost_calibration/depth_5_iterations_200_learning_rate_0.05/best_model_fold_4.pkl\n",
      "Seed 9762, Fold=5: Train=679, Val=98, Test=194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Validation AUC = 0.7597\n",
      "Fold 5 - Test AUC       = 0.8052\n",
      "Fold 5 æ ¡å‡†æ¨¡å‹å·²ä¿å­˜: /home/mailiyi/Poisoning_Prediction/ML/predict_death_best_model/catboost_calibration/depth_5_iterations_200_learning_rate_0.05/best_model_fold_5.pkl\n",
      "\n",
      "===== äº”æŠ˜ä¸­æœ€ä¼˜æ¨¡å‹: Fold 2, Validation AUC=0.9875 =====\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: /home/mailiyi/Poisoning_Prediction/ML/predict_death_best_model/catboost_calibration/depth_5_iterations_200_learning_rate_0.05/best_overall_model.pkl\n",
      "\n",
      "===== æœ€ç»ˆç»“æœ (å›ºå®šå‚æ•°) =====\n",
      "AUROC: Mean=0.8499, 95% CI=(0.8018,0.8948)\n",
      "AUPRC: Mean=0.4739, 95% CI=(0.3661,0.5741)\n",
      "\n",
      "===== æœ€ä½³ Youden Index é˜ˆå€¼ =====\n",
      "æœ€ä½³é˜ˆå€¼ (cut-off) = 0.1180\n",
      "Youden Index = 0.5692\n",
      "Sensitivity  = 0.7683\n",
      "Specificity  = 0.8009\n"
     ]
    }
   ],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome_other']\n",
    "results = train_catboost_5fold_cv_best_save(dataX, dataY,cat_features=features_categorical)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be0d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0db531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model_path = \"/home/mailiyi/Poisoning_Prediction/ML/predict_death_best_model/catboost/depth_5_iterations_200_learning_rate_0.05/best_overall_model.cbm\"\n",
    "\n",
    "model = CatBoostClassifier()\n",
    "model.load_model(model_path)\n",
    "\n",
    "print(\"æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594b001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31472875 0.48509739 0.47470878 0.33412588 0.5048726 ]\n"
     ]
    }
   ],
   "source": [
    "### Call example ###\n",
    "temp_df = dataX.head(5).copy()\n",
    "\n",
    "for c in features_categorical:\n",
    "    temp_df[c] = temp_df[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "# Prediction probability (positive class)\n",
    "y_pred = model.predict_proba(temp_df)[:, 1]\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2bb065a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# model_path = \"/home/mailiyi/Poisoning_Prediction/ML/predict_death_best_model/catboost/depth_5_iterations_200_learning_rate_0.05/best_overall_model.pkl\"\n",
    "\n",
    "# # è¯»å–æ ¡å‡†æ¨¡å‹\n",
    "# calib_model = joblib.load(model_path)\n",
    "\n",
    "# print(\"æ ¡å‡†æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29e94cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### è°ƒç”¨ç¤ºä¾‹ ###\n",
    "# temp_df = dataX.head(5).copy()  # é˜²æ­¢ sliceï¼Œç¡®ä¿å®‰å…¨\n",
    "\n",
    "# # å’Œè®­ç»ƒå®Œå…¨ä¸€è‡´çš„é¢„å¤„ç†\n",
    "# for c in features_categorical:\n",
    "#     temp_df[c] = temp_df[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "# # é¢„æµ‹æ¦‚ç‡ï¼ˆæ­£ç±»ï¼‰\n",
    "# y_pred = calib_model.predict_proba(temp_df)[:, 1]\n",
    "# print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e128f",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e389f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def train_xgboost_until_auc(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    early_stopping_rounds=30,\n",
    "    n_bootstrap=2000,\n",
    "    seed_global=42,\n",
    "    auc_threshold=0.85   # Target AUROC threshold\n",
    "):\n",
    "    X = dataX.copy()\n",
    "    y = np.array(dataY)\n",
    "    rng = np.random.RandomState(seed_global)\n",
    "\n",
    "    # ====== Bootstrap calculate AUROC CI ======\n",
    "    def bootstrap_auc_ci(y_true, y_pred, n_bootstrap=2000, seed=42):\n",
    "        rng = np.random.RandomState(seed)\n",
    "        aucs = []\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        for _ in range(n_bootstrap):\n",
    "            indices = rng.randint(0, len(y_true), len(y_true))\n",
    "            if len(np.unique(y_true[indices])) < 2:\n",
    "                continue\n",
    "            aucs.append(metrics.roc_auc_score(y_true[indices], y_pred[indices]))\n",
    "        mean_auc = np.mean(aucs)\n",
    "        lower = np.percentile(aucs, 2.5)\n",
    "        upper = np.percentile(aucs, 97.5)\n",
    "        return mean_auc, lower, upper\n",
    "\n",
    "    # ====== Bootstrap calculate AUPRC CI ======\n",
    "    def bootstrap_auprc_ci(y_true, y_pred, n_bootstrap=2000, seed=42):\n",
    "        rng = np.random.RandomState(seed)\n",
    "        auprcs = []\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        for _ in range(n_bootstrap):\n",
    "            indices = rng.randint(0, len(y_true), len(y_true))\n",
    "            if len(np.unique(y_true[indices])) < 2:\n",
    "                continue\n",
    "            auprcs.append(metrics.average_precision_score(y_true[indices], y_pred[indices]))\n",
    "        mean_auprc = np.mean(auprcs)\n",
    "        lower = np.percentile(auprcs, 2.5)\n",
    "        upper = np.percentile(auprcs, 97.5)\n",
    "        return mean_auprc, lower, upper\n",
    "\n",
    "    # ====== Main loop: Random trial of different seeds ======\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        seed = rng.randint(0, 10000)\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        all_y_true, all_y_pred = [], []\n",
    "\n",
    "        print(f\"\\n===== Attempt {attempt}: Seed {seed} =====\")\n",
    "\n",
    "        for train_val_index, test_index in kf.split(X):\n",
    "            X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "            y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "            # Divide 1/8 from train_val as validation set\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train_val, y_train_val,\n",
    "                test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "            )\n",
    "\n",
    "            num_pos = np.sum(y_train == 1)\n",
    "            num_neg = np.sum(y_train == 0)\n",
    "            scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "\n",
    "            # XGBoost DMatrix\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "            dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "            params = {\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'learning_rate': 0.05,\n",
    "                'max_depth': 5,\n",
    "                'scale_pos_weight': scale_pos_weight,\n",
    "                'seed': seed,\n",
    "                'verbosity': 0,\n",
    "            }\n",
    "\n",
    "            model = xgb.train(\n",
    "                params=params,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=300,\n",
    "                evals=[(dval, 'validation')],\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "\n",
    "            y_pred_prob = model.predict(dtest)\n",
    "            all_y_true.extend(y_test)\n",
    "            all_y_pred.extend(y_pred_prob)\n",
    "\n",
    "        # ====== Bootstrap Calculate overall performance ======\n",
    "        mean_auc, auc_lower, auc_upper = bootstrap_auc_ci(all_y_true, all_y_pred, n_bootstrap=n_bootstrap, seed=seed)\n",
    "        mean_auprc, auprc_lower, auprc_upper = bootstrap_auprc_ci(all_y_true, all_y_pred, n_bootstrap=n_bootstrap, seed=seed)\n",
    "\n",
    "        print(f\"Seed {seed}: AUROC={mean_auc:.4f} (95% CI: {auc_lower:.4f}-{auc_upper:.4f})\")\n",
    "        print(f\"Seed {seed}: AUPRC={mean_auprc:.4f} (95% CI: {auprc_lower:.4f}-{auprc_upper:.4f})\")\n",
    "\n",
    "        # ====== Threshold is met, then jump out. ======\n",
    "        if mean_auc >= auc_threshold:\n",
    "            print(f\"\\nğŸ¯ reaches a thresholdï¼Seed {seed} AUROC={mean_auc:.4f} â‰¥ {auc_threshold}\")\n",
    "            df_pred = pd.DataFrame({'y_true': all_y_true, 'y_pred': all_y_pred})\n",
    "            df_pred.to_csv(f'xgboost_seed{seed}_results.csv', index=False)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bbf5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome_other']  \n",
    "results = train_xgboost_until_auc(dataX, dataY)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "===== Attempt 53: Seed 8226 =====\n",
    "Seed 8226: AUROC=0.8478 (95% CI: 0.7950-0.8975)\n",
    "Seed 8226: AUPRC=0.4696 (95% CI: 0.3486-0.5824)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## é€‰å®šç§å­æ•°ï¼ŒmeanAUC (95% CI) ç”¨boostrapè®¡ç®—\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ad095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# import pandas as pd \n",
    "# from sklearn import metrics\n",
    "# from sklearn.model_selection import KFold, train_test_split, ParameterGrid\n",
    "# import xgboost as xgb\n",
    "# import os\n",
    "\n",
    "# # ===================== bootstrap è®¡ç®—å‡½æ•° =====================\n",
    "# def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "#     rng = np.random.RandomState(seed)\n",
    "#     scores = []\n",
    "#     y_true = np.array(y_true)\n",
    "#     y_pred = np.array(y_pred)\n",
    "#     for _ in range(n_bootstrap):\n",
    "#         idx = rng.randint(0, len(y_true), len(y_true))\n",
    "#         if len(np.unique(y_true[idx])) < 2:\n",
    "#             continue\n",
    "#         scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "#     return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "# # ===================== äº”æŠ˜ XGBoost =====================\n",
    "# def train_xgboost_5fold_cv_gridsearch_save(\n",
    "#     dataX,\n",
    "#     dataY,\n",
    "#     save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death/xgboost_gridsearch_valid_test_5cv/',\n",
    "#     seed=8226,\n",
    "#     early_stopping_rounds=30,\n",
    "#     param_grid=None\n",
    "# ):\n",
    "#     os.makedirs(save_path, exist_ok=True)\n",
    "#     X = dataX.copy()\n",
    "#     y = np.array(dataY)\n",
    "\n",
    "#     if param_grid is None:\n",
    "#         param_grid = {\"max_depth\":[5], \"learning_rate\":[0.05], \"n_estimators\":[300]}\n",
    "\n",
    "#     grid_results = []\n",
    "\n",
    "#     for params in ParameterGrid(param_grid):\n",
    "#         param_name = \"_\".join([f\"{k}_{v}\" for k,v in params.items()])\n",
    "#         param_path = os.path.join(save_path, param_name)\n",
    "#         os.makedirs(param_path, exist_ok=True)\n",
    "#         print(f\"\\n===== æµ‹è¯•å‚æ•°ç»„åˆ: {params} =====\")\n",
    "\n",
    "#         kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "#         all_results = []\n",
    "\n",
    "#         fold_idx = 1\n",
    "#         for train_val_index, test_index in kf.split(X):\n",
    "#             # Step1: æ‹†åˆ† train_val / test\n",
    "#             X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "#             y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "#             # Step2: æ‹†åˆ†éªŒè¯é›†\n",
    "#             X_train, X_val, y_train, y_val = train_test_split(\n",
    "#                 X_train_val, y_train_val,\n",
    "#                 test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "#             )\n",
    "\n",
    "#             # è¾“å‡ºè®­ç»ƒé›†ã€æµ‹è¯•é›†ã€éªŒè¯é›†çš„æ•°é‡\n",
    "#             print(f\"Seed {seed}, Fold: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "#             print(f\"  Train - Pos: {np.sum(y_train == 1)}, Neg: {np.sum(y_train == 0)}\")\n",
    "#             print(f\"  Val   - Pos: {np.sum(y_val == 1)},   Neg: {np.sum(y_val == 0)}\")\n",
    "#             print(f\"  Test  - Pos: {np.sum(y_test == 1)},  Neg: {np.sum(y_test == 0)}\")\n",
    "\n",
    "#             # ç±»åˆ«æƒé‡\n",
    "#             num_pos = np.sum(y_train == 1)\n",
    "#             num_neg = np.sum(y_train == 0)\n",
    "#             scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "\n",
    "#             # DMatrix\n",
    "#             dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "#             dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "#             dtest  = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "#             # å‚æ•°é…ç½®\n",
    "#             xgb_params = {\n",
    "#                 \"objective\": \"binary:logistic\",\n",
    "#                 \"eval_metric\": \"auc\",\n",
    "#                 \"scale_pos_weight\": scale_pos_weight,\n",
    "#                 \"seed\": seed,\n",
    "#                 \"verbosity\": 0,\n",
    "#                 **params\n",
    "#             }\n",
    "\n",
    "#             ## early_stopping_rounds é»˜è®¤åŸºäºæœ€åä¸€ä¸ª eval æ•°æ®é›†ï¼ˆå³ dvalï¼‰çš„æ€§èƒ½ï¼Œæ‰€ä»¥å³ä½¿åŒ…å« dtrainï¼Œearly stopping ä»ç„¶çœ‹çš„æ˜¯éªŒè¯é›†çš„è¡¨ç°ã€‚\n",
    "#             # early stopping è®­ç»ƒ\n",
    "#             # evals = [(dtrain, \"train\"), (dval, \"val\")]\n",
    "#             evals=[(dval, 'validation')]\n",
    "#             model = xgb.train(\n",
    "#                 xgb_params,\n",
    "#                 dtrain,\n",
    "#                 num_boost_round=params.get(\"n_estimators\", 300),\n",
    "#                 evals=evals,\n",
    "#                 early_stopping_rounds=early_stopping_rounds,\n",
    "#                 verbose_eval=False\n",
    "#             )\n",
    "\n",
    "#             # æµ‹è¯•é¢„æµ‹\n",
    "#             y_pred_prob = model.predict(dtest)\n",
    "\n",
    "#             # ä¿å­˜å½“å‰æŠ˜ç»“æœ\n",
    "#             fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "#             pd.DataFrame({\n",
    "#                 \"fold\": fold_idx,\n",
    "#                 \"y_test\": y_test,\n",
    "#                 \"y_pred\": y_pred_prob\n",
    "#             }).to_csv(fold_csv, index=False)\n",
    "#             all_results.append(pd.DataFrame({\"fold\": fold_idx,\"y_test\": y_test,\"y_pred\": y_pred_prob}))\n",
    "\n",
    "#             fold_idx += 1\n",
    "\n",
    "#         # åˆå¹¶æŠ˜ç»“æœ\n",
    "#         all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "#         all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "#         all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "#         # ===================== bootstrap è®¡ç®— AUROC/AUPRC =====================\n",
    "#         mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "#             all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "#         )\n",
    "#         mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "#             all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "#         )\n",
    "\n",
    "#         print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "#         print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "#         grid_results.append({\n",
    "#             \"params\": params,\n",
    "#             \"AUROC_mean\": mean_auroc,\n",
    "#             \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "#             \"AUPRC_mean\": mean_auprc,\n",
    "#             \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "#             \"AllResults\": all_results_df\n",
    "#         })\n",
    "\n",
    "#     # æœ€ä¼˜å‚æ•°\n",
    "#     best = max(grid_results, key=lambda x: x[\"AUROC_mean\"])\n",
    "#     print(f\"\\n===== æœ€ä¼˜å‚æ•° =====\\n{best['params']}\")\n",
    "#     print(f\"AUROC Mean={best['AUROC_mean']:.4f}, 95% CI={best['AUROC_CI']}\")\n",
    "#     print(f\"AUPRC Mean={best['AUPRC_mean']:.4f}, 95% CI={best['AUPRC_CI']}\")\n",
    "\n",
    "#     return grid_results, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be573b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"n_estimators\": [200, 300, 400],\n",
    "#     \"max_depth\": [4, 5, 6],\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 200} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.7810, 95% CI=(0.7163,0.8414)\n",
      "AUPRC: Mean=0.4256, 95% CI=(0.3122,0.5308)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.7810, 95% CI=(0.7163,0.8414)\n",
      "AUPRC: Mean=0.4256, 95% CI=(0.3122,0.5308)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 400} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.7810, 95% CI=(0.7163,0.8414)\n",
      "AUPRC: Mean=0.4256, 95% CI=(0.3122,0.5308)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8000, 95% CI=(0.7341,0.8596)\n",
      "AUPRC: Mean=0.4430, 95% CI=(0.3303,0.5476)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 300} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8000, 95% CI=(0.7341,0.8596)\n",
      "AUPRC: Mean=0.4430, 95% CI=(0.3303,0.5476)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 400} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8000, 95% CI=(0.7341,0.8596)\n",
      "AUPRC: Mean=0.4430, 95% CI=(0.3303,0.5476)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 200} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.7787, 95% CI=(0.7104,0.8404)\n",
      "AUPRC: Mean=0.4275, 95% CI=(0.3186,0.5305)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 300} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.7787, 95% CI=(0.7104,0.8404)\n",
      "AUPRC: Mean=0.4275, 95% CI=(0.3186,0.5305)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 400} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.7787, 95% CI=(0.7104,0.8404)\n",
      "AUPRC: Mean=0.4275, 95% CI=(0.3186,0.5305)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 200} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8202, 95% CI=(0.7592,0.8773)\n",
      "AUPRC: Mean=0.4714, 95% CI=(0.3604,0.5807)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8202, 95% CI=(0.7592,0.8773)\n",
      "AUPRC: Mean=0.4714, 95% CI=(0.3604,0.5807)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 400} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8202, 95% CI=(0.7592,0.8773)\n",
      "AUPRC: Mean=0.4714, 95% CI=(0.3604,0.5807)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8485, 95% CI=(0.7961,0.8955)\n",
      "AUPRC: Mean=0.4712, 95% CI=(0.3579,0.5834)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8485, 95% CI=(0.7961,0.8955)\n",
      "AUPRC: Mean=0.4712, 95% CI=(0.3579,0.5834)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8485, 95% CI=(0.7961,0.8955)\n",
      "AUPRC: Mean=0.4712, 95% CI=(0.3579,0.5834)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 200} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8366, 95% CI=(0.7864,0.8849)\n",
      "AUPRC: Mean=0.4253, 95% CI=(0.3141,0.5380)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 300} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8366, 95% CI=(0.7864,0.8849)\n",
      "AUPRC: Mean=0.4253, 95% CI=(0.3141,0.5380)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 400} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8366, 95% CI=(0.7864,0.8849)\n",
      "AUPRC: Mean=0.4253, 95% CI=(0.3141,0.5380)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 200} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8457, 95% CI=(0.7949,0.8927)\n",
      "AUPRC: Mean=0.4555, 95% CI=(0.3413,0.5696)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8457, 95% CI=(0.7949,0.8927)\n",
      "AUPRC: Mean=0.4555, 95% CI=(0.3413,0.5696)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 400} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8457, 95% CI=(0.7949,0.8927)\n",
      "AUPRC: Mean=0.4555, 95% CI=(0.3413,0.5696)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8455, 95% CI=(0.7942,0.8945)\n",
      "AUPRC: Mean=0.4779, 95% CI=(0.3648,0.5906)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8455, 95% CI=(0.7942,0.8945)\n",
      "AUPRC: Mean=0.4779, 95% CI=(0.3648,0.5906)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8455, 95% CI=(0.7942,0.8945)\n",
      "AUPRC: Mean=0.4779, 95% CI=(0.3648,0.5906)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8324, 95% CI=(0.7751,0.8866)\n",
      "AUPRC: Mean=0.4637, 95% CI=(0.3501,0.5742)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8324, 95% CI=(0.7751,0.8866)\n",
      "AUPRC: Mean=0.4637, 95% CI=(0.3501,0.5742)\n",
      "\n",
      "===== æµ‹è¯•å‚æ•°ç»„åˆ: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 400} =====\n",
      "Seed 8226, Fold: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "AUROC: Mean=0.8324, 95% CI=(0.7751,0.8866)\n",
      "AUPRC: Mean=0.4637, 95% CI=(0.3501,0.5742)\n",
      "\n",
      "===== æœ€ä¼˜å‚æ•° =====\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n",
      "AUROC Mean=0.8485, 95% CI=(np.float64(0.7961402575137099), np.float64(0.8954669147640918))\n",
      "AUPRC Mean=0.4712, 95% CI=(np.float64(0.35788342060344325), np.float64(0.5833624283317074))\n",
      "([{'params': {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.7809732479278595), 'AUROC_CI': (np.float64(0.7163115300758574), np.float64(0.8414206887023774)), 'AUPRC_mean': np.float64(0.4256050786126924), 'AUPRC_CI': (np.float64(0.3122018040231364), np.float64(0.5307975073883856)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.228275\n",
      "1       1       0  0.158136\n",
      "2       1       0  0.174746\n",
      "3       1       0  0.167749\n",
      "4       1       0  0.131931\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.382059\n",
      "967     5       0  0.378002\n",
      "968     5       0  0.444595\n",
      "969     5       0  0.390912\n",
      "970     5       0  0.418581\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300}, 'AUROC_mean': np.float64(0.7809732479278595), 'AUROC_CI': (np.float64(0.7163115300758574), np.float64(0.8414206887023774)), 'AUPRC_mean': np.float64(0.4256050786126924), 'AUPRC_CI': (np.float64(0.3122018040231364), np.float64(0.5307975073883856)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.228275\n",
      "1       1       0  0.158136\n",
      "2       1       0  0.174746\n",
      "3       1       0  0.167749\n",
      "4       1       0  0.131931\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.382059\n",
      "967     5       0  0.378002\n",
      "968     5       0  0.444595\n",
      "969     5       0  0.390912\n",
      "970     5       0  0.418581\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 400}, 'AUROC_mean': np.float64(0.7809732479278595), 'AUROC_CI': (np.float64(0.7163115300758574), np.float64(0.8414206887023774)), 'AUPRC_mean': np.float64(0.4256050786126924), 'AUPRC_CI': (np.float64(0.3122018040231364), np.float64(0.5307975073883856)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.228275\n",
      "1       1       0  0.158136\n",
      "2       1       0  0.174746\n",
      "3       1       0  0.167749\n",
      "4       1       0  0.131931\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.382059\n",
      "967     5       0  0.378002\n",
      "968     5       0  0.444595\n",
      "969     5       0  0.390912\n",
      "970     5       0  0.418581\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.8000369086754512), 'AUROC_CI': (np.float64(0.7340835568769924), np.float64(0.8595659383896073)), 'AUPRC_mean': np.float64(0.44302833252053275), 'AUPRC_CI': (np.float64(0.3302769927747848), np.float64(0.5476277200295293)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.344543\n",
      "1       1       0  0.267417\n",
      "2       1       0  0.318007\n",
      "3       1       0  0.302185\n",
      "4       1       0  0.267568\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.417334\n",
      "967     5       0  0.255978\n",
      "968     5       0  0.321736\n",
      "969     5       0  0.291280\n",
      "970     5       0  0.390251\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 300}, 'AUROC_mean': np.float64(0.8000369086754512), 'AUROC_CI': (np.float64(0.7340835568769924), np.float64(0.8595659383896073)), 'AUPRC_mean': np.float64(0.44302833252053275), 'AUPRC_CI': (np.float64(0.3302769927747848), np.float64(0.5476277200295293)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.344543\n",
      "1       1       0  0.267417\n",
      "2       1       0  0.318007\n",
      "3       1       0  0.302185\n",
      "4       1       0  0.267568\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.417334\n",
      "967     5       0  0.255978\n",
      "968     5       0  0.321736\n",
      "969     5       0  0.291280\n",
      "970     5       0  0.390251\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 400}, 'AUROC_mean': np.float64(0.8000369086754512), 'AUROC_CI': (np.float64(0.7340835568769924), np.float64(0.8595659383896073)), 'AUPRC_mean': np.float64(0.44302833252053275), 'AUPRC_CI': (np.float64(0.3302769927747848), np.float64(0.5476277200295293)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.344543\n",
      "1       1       0  0.267417\n",
      "2       1       0  0.318007\n",
      "3       1       0  0.302185\n",
      "4       1       0  0.267568\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.417334\n",
      "967     5       0  0.255978\n",
      "968     5       0  0.321736\n",
      "969     5       0  0.291280\n",
      "970     5       0  0.390251\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.778726430353661), 'AUROC_CI': (np.float64(0.710406300428158), np.float64(0.8404461056062849)), 'AUPRC_mean': np.float64(0.4274864671496628), 'AUPRC_CI': (np.float64(0.31858500743141177), np.float64(0.5304727326776867)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.329516\n",
      "1       1       0  0.313179\n",
      "2       1       0  0.338324\n",
      "3       1       0  0.313179\n",
      "4       1       0  0.313179\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.393286\n",
      "967     5       0  0.209406\n",
      "968     5       0  0.266237\n",
      "969     5       0  0.227741\n",
      "970     5       0  0.354102\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 300}, 'AUROC_mean': np.float64(0.778726430353661), 'AUROC_CI': (np.float64(0.710406300428158), np.float64(0.8404461056062849)), 'AUPRC_mean': np.float64(0.4274864671496628), 'AUPRC_CI': (np.float64(0.31858500743141177), np.float64(0.5304727326776867)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.329516\n",
      "1       1       0  0.313179\n",
      "2       1       0  0.338324\n",
      "3       1       0  0.313179\n",
      "4       1       0  0.313179\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.393286\n",
      "967     5       0  0.209406\n",
      "968     5       0  0.266237\n",
      "969     5       0  0.227741\n",
      "970     5       0  0.354102\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 400}, 'AUROC_mean': np.float64(0.778726430353661), 'AUROC_CI': (np.float64(0.710406300428158), np.float64(0.8404461056062849)), 'AUPRC_mean': np.float64(0.4274864671496628), 'AUPRC_CI': (np.float64(0.31858500743141177), np.float64(0.5304727326776867)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.329516\n",
      "1       1       0  0.313179\n",
      "2       1       0  0.338324\n",
      "3       1       0  0.313179\n",
      "4       1       0  0.313179\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.393286\n",
      "967     5       0  0.209406\n",
      "968     5       0  0.266237\n",
      "969     5       0  0.227741\n",
      "970     5       0  0.354102\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.8201708604485478), 'AUROC_CI': (np.float64(0.7592313099684503), np.float64(0.877298943314696)), 'AUPRC_mean': np.float64(0.4713666402221311), 'AUPRC_CI': (np.float64(0.36037190103331945), np.float64(0.5806863509959567)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.075728\n",
      "1       1       0  0.051450\n",
      "2       1       0  0.040317\n",
      "3       1       0  0.040658\n",
      "4       1       0  0.024228\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.405971\n",
      "967     5       0  0.023169\n",
      "968     5       0  0.075963\n",
      "969     5       0  0.043156\n",
      "970     5       0  0.125557\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300}, 'AUROC_mean': np.float64(0.8201708604485478), 'AUROC_CI': (np.float64(0.7592313099684503), np.float64(0.877298943314696)), 'AUPRC_mean': np.float64(0.4713666402221311), 'AUPRC_CI': (np.float64(0.36037190103331945), np.float64(0.5806863509959567)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.075728\n",
      "1       1       0  0.051450\n",
      "2       1       0  0.040317\n",
      "3       1       0  0.040658\n",
      "4       1       0  0.024228\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.405971\n",
      "967     5       0  0.023169\n",
      "968     5       0  0.075963\n",
      "969     5       0  0.043156\n",
      "970     5       0  0.125557\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 400}, 'AUROC_mean': np.float64(0.8201708604485478), 'AUROC_CI': (np.float64(0.7592313099684503), np.float64(0.877298943314696)), 'AUPRC_mean': np.float64(0.4713666402221311), 'AUPRC_CI': (np.float64(0.36037190103331945), np.float64(0.5806863509959567)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.075728\n",
      "1       1       0  0.051450\n",
      "2       1       0  0.040317\n",
      "3       1       0  0.040658\n",
      "4       1       0  0.024228\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.405971\n",
      "967     5       0  0.023169\n",
      "968     5       0  0.075963\n",
      "969     5       0  0.043156\n",
      "970     5       0  0.125557\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.8484887503242158), 'AUROC_CI': (np.float64(0.7961402575137099), np.float64(0.8954669147640918)), 'AUPRC_mean': np.float64(0.4711890699966884), 'AUPRC_CI': (np.float64(0.35788342060344325), np.float64(0.5833624283317074)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.015236\n",
      "1       1       0  0.006273\n",
      "2       1       0  0.012435\n",
      "3       1       0  0.010235\n",
      "4       1       0  0.006132\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.333872\n",
      "967     5       0  0.018177\n",
      "968     5       0  0.100516\n",
      "969     5       0  0.036619\n",
      "970     5       0  0.080217\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300}, 'AUROC_mean': np.float64(0.8484887503242158), 'AUROC_CI': (np.float64(0.7961402575137099), np.float64(0.8954669147640918)), 'AUPRC_mean': np.float64(0.4711890699966884), 'AUPRC_CI': (np.float64(0.35788342060344325), np.float64(0.5833624283317074)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.015236\n",
      "1       1       0  0.006273\n",
      "2       1       0  0.012435\n",
      "3       1       0  0.010235\n",
      "4       1       0  0.006132\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.333872\n",
      "967     5       0  0.018177\n",
      "968     5       0  0.100516\n",
      "969     5       0  0.036619\n",
      "970     5       0  0.080217\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 400}, 'AUROC_mean': np.float64(0.8484887503242158), 'AUROC_CI': (np.float64(0.7961402575137099), np.float64(0.8954669147640918)), 'AUPRC_mean': np.float64(0.4711890699966884), 'AUPRC_CI': (np.float64(0.35788342060344325), np.float64(0.5833624283317074)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.015236\n",
      "1       1       0  0.006273\n",
      "2       1       0  0.012435\n",
      "3       1       0  0.010235\n",
      "4       1       0  0.006132\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.333872\n",
      "967     5       0  0.018177\n",
      "968     5       0  0.100516\n",
      "969     5       0  0.036619\n",
      "970     5       0  0.080217\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.8366194044531482), 'AUROC_CI': (np.float64(0.7864444164500289), np.float64(0.8848735080691754)), 'AUPRC_mean': np.float64(0.4252778081222533), 'AUPRC_CI': (np.float64(0.31406273166102894), np.float64(0.5379732216346578)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.010078\n",
      "1       1       0  0.002988\n",
      "2       1       0  0.007666\n",
      "3       1       0  0.009841\n",
      "4       1       0  0.004051\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.212043\n",
      "967     5       0  0.013272\n",
      "968     5       0  0.058636\n",
      "969     5       0  0.016307\n",
      "970     5       0  0.045230\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 300}, 'AUROC_mean': np.float64(0.8366194044531482), 'AUROC_CI': (np.float64(0.7864444164500289), np.float64(0.8848735080691754)), 'AUPRC_mean': np.float64(0.4252778081222533), 'AUPRC_CI': (np.float64(0.31406273166102894), np.float64(0.5379732216346578)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.010078\n",
      "1       1       0  0.002988\n",
      "2       1       0  0.007666\n",
      "3       1       0  0.009841\n",
      "4       1       0  0.004051\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.212043\n",
      "967     5       0  0.013272\n",
      "968     5       0  0.058636\n",
      "969     5       0  0.016307\n",
      "970     5       0  0.045230\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 400}, 'AUROC_mean': np.float64(0.8366194044531482), 'AUROC_CI': (np.float64(0.7864444164500289), np.float64(0.8848735080691754)), 'AUPRC_mean': np.float64(0.4252778081222533), 'AUPRC_CI': (np.float64(0.31406273166102894), np.float64(0.5379732216346578)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.010078\n",
      "1       1       0  0.002988\n",
      "2       1       0  0.007666\n",
      "3       1       0  0.009841\n",
      "4       1       0  0.004051\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.212043\n",
      "967     5       0  0.013272\n",
      "968     5       0  0.058636\n",
      "969     5       0  0.016307\n",
      "970     5       0  0.045230\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.8457404266162416), 'AUROC_CI': (np.float64(0.7949486424193822), np.float64(0.8926942354562152)), 'AUPRC_mean': np.float64(0.4555198208642103), 'AUPRC_CI': (np.float64(0.34125278435758377), np.float64(0.5695913621093344)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.002417\n",
      "1       1       0  0.001078\n",
      "2       1       0  0.003128\n",
      "3       1       0  0.003436\n",
      "4       1       0  0.000515\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.416316\n",
      "967     5       0  0.013692\n",
      "968     5       0  0.049242\n",
      "969     5       0  0.023593\n",
      "970     5       0  0.071876\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300}, 'AUROC_mean': np.float64(0.8457404266162416), 'AUROC_CI': (np.float64(0.7949486424193822), np.float64(0.8926942354562152)), 'AUPRC_mean': np.float64(0.4555198208642103), 'AUPRC_CI': (np.float64(0.34125278435758377), np.float64(0.5695913621093344)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.002417\n",
      "1       1       0  0.001078\n",
      "2       1       0  0.003128\n",
      "3       1       0  0.003436\n",
      "4       1       0  0.000515\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.416316\n",
      "967     5       0  0.013692\n",
      "968     5       0  0.049242\n",
      "969     5       0  0.023593\n",
      "970     5       0  0.071876\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 400}, 'AUROC_mean': np.float64(0.8457404266162416), 'AUROC_CI': (np.float64(0.7949486424193822), np.float64(0.8926942354562152)), 'AUPRC_mean': np.float64(0.4555198208642103), 'AUPRC_CI': (np.float64(0.34125278435758377), np.float64(0.5695913621093344)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.002417\n",
      "1       1       0  0.001078\n",
      "2       1       0  0.003128\n",
      "3       1       0  0.003436\n",
      "4       1       0  0.000515\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.416316\n",
      "967     5       0  0.013692\n",
      "968     5       0  0.049242\n",
      "969     5       0  0.023593\n",
      "970     5       0  0.071876\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.8454968919693432), 'AUROC_CI': (np.float64(0.7941996348318933), np.float64(0.8944779451956832)), 'AUPRC_mean': np.float64(0.47787235512529075), 'AUPRC_CI': (np.float64(0.3647603380507934), np.float64(0.5905611585730743)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.009352\n",
      "1       1       0  0.002770\n",
      "2       1       0  0.008017\n",
      "3       1       0  0.006483\n",
      "4       1       0  0.003533\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.149219\n",
      "967     5       0  0.004107\n",
      "968     5       0  0.020535\n",
      "969     5       0  0.007497\n",
      "970     5       0  0.022174\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}, 'AUROC_mean': np.float64(0.8454968919693432), 'AUROC_CI': (np.float64(0.7941996348318933), np.float64(0.8944779451956832)), 'AUPRC_mean': np.float64(0.47787235512529075), 'AUPRC_CI': (np.float64(0.3647603380507934), np.float64(0.5905611585730743)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.009352\n",
      "1       1       0  0.002770\n",
      "2       1       0  0.008017\n",
      "3       1       0  0.006483\n",
      "4       1       0  0.003533\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.149219\n",
      "967     5       0  0.004107\n",
      "968     5       0  0.020535\n",
      "969     5       0  0.007497\n",
      "970     5       0  0.022174\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 400}, 'AUROC_mean': np.float64(0.8454968919693432), 'AUROC_CI': (np.float64(0.7941996348318933), np.float64(0.8944779451956832)), 'AUPRC_mean': np.float64(0.47787235512529075), 'AUPRC_CI': (np.float64(0.3647603380507934), np.float64(0.5905611585730743)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.009352\n",
      "1       1       0  0.002770\n",
      "2       1       0  0.008017\n",
      "3       1       0  0.006483\n",
      "4       1       0  0.003533\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.149219\n",
      "967     5       0  0.004107\n",
      "968     5       0  0.020535\n",
      "969     5       0  0.007497\n",
      "970     5       0  0.022174\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.8324103048062784), 'AUROC_CI': (np.float64(0.7750881927048633), np.float64(0.8866058318284599)), 'AUPRC_mean': np.float64(0.4637292459644698), 'AUPRC_CI': (np.float64(0.35011725016893636), np.float64(0.5741700662713486)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.010389\n",
      "1       1       0  0.002675\n",
      "2       1       0  0.007305\n",
      "3       1       0  0.015559\n",
      "4       1       0  0.003298\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.098085\n",
      "967     5       0  0.006676\n",
      "968     5       0  0.024894\n",
      "969     5       0  0.009167\n",
      "970     5       0  0.032462\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300}, 'AUROC_mean': np.float64(0.8324103048062784), 'AUROC_CI': (np.float64(0.7750881927048633), np.float64(0.8866058318284599)), 'AUPRC_mean': np.float64(0.4637292459644698), 'AUPRC_CI': (np.float64(0.35011725016893636), np.float64(0.5741700662713486)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.010389\n",
      "1       1       0  0.002675\n",
      "2       1       0  0.007305\n",
      "3       1       0  0.015559\n",
      "4       1       0  0.003298\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.098085\n",
      "967     5       0  0.006676\n",
      "968     5       0  0.024894\n",
      "969     5       0  0.009167\n",
      "970     5       0  0.032462\n",
      "\n",
      "[971 rows x 3 columns]}, {'params': {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 400}, 'AUROC_mean': np.float64(0.8324103048062784), 'AUROC_CI': (np.float64(0.7750881927048633), np.float64(0.8866058318284599)), 'AUPRC_mean': np.float64(0.4637292459644698), 'AUPRC_CI': (np.float64(0.35011725016893636), np.float64(0.5741700662713486)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.010389\n",
      "1       1       0  0.002675\n",
      "2       1       0  0.007305\n",
      "3       1       0  0.015559\n",
      "4       1       0  0.003298\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.098085\n",
      "967     5       0  0.006676\n",
      "968     5       0  0.024894\n",
      "969     5       0  0.009167\n",
      "970     5       0  0.032462\n",
      "\n",
      "[971 rows x 3 columns]}], {'params': {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.8484887503242158), 'AUROC_CI': (np.float64(0.7961402575137099), np.float64(0.8954669147640918)), 'AUPRC_mean': np.float64(0.4711890699966884), 'AUPRC_CI': (np.float64(0.35788342060344325), np.float64(0.5833624283317074)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.015236\n",
      "1       1       0  0.006273\n",
      "2       1       0  0.012435\n",
      "3       1       0  0.010235\n",
      "4       1       0  0.006132\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.333872\n",
      "967     5       0  0.018177\n",
      "968     5       0  0.100516\n",
      "969     5       0  0.036619\n",
      "970     5       0  0.080217\n",
      "\n",
      "[971 rows x 3 columns]})\n"
     ]
    }
   ],
   "source": [
    "# dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "# dataY = df_mapped_wide['Outcome_other']  # é¢„æµ‹æ˜¯å¦æ­»äº¡\n",
    "# results = train_xgboost_5fold_cv_gridsearch_save(dataX, dataY, param_grid=param_grid)  ## æ—§ç‰ˆxgboostä¸æ”¯æŒæŒ‡å®šç±»åˆ«ç‰¹å¾\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'/home/mailiyi/Poisoning_Prediction/ML/predict_death/xgboost_gridsearch_valid_test_5cv/learning_rate_0.05_max_depth_5_n_estimators_200'\n",
    "\n",
    "===== æœ€ä¼˜å‚æ•° =====\n",
    "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n",
    "AUROC Mean=0.8485, 95% CI=(np.float64(0.7961402575137099), np.float64(0.8954669147640918))\n",
    "AUPRC Mean=0.4712, 95% CI=(np.float64(0.35788342060344325), np.float64(0.5833624283317074))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fixed Hyperparameters\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import xgboost as xgb\n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# from xgboost import XGBClassifier\n",
    "import os\n",
    "\n",
    "# ===================== Bootstrap Calculation Function =====================\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "\n",
    "# ===================== 5-Fold XGBoost (Fixed Parameters) =====================\n",
    "def train_xgboost_5fold_cv_fixed(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death/xgboost_gridsearch_valid_test_5cv/',\n",
    "    seed=8226,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    X = dataX.copy()\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "    print(f\"\\n===== Using Fixed Parameters: {params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        # Step 1: Split train_val / test\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # Step 2: Split validation set\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        # Output the number of samples in train, test, and validation sets\n",
    "        print(f\"Seed {seed}, Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "        print(f\"  Train - Pos: {np.sum(y_train == 1)}, Neg: {np.sum(y_train == 0)}\")\n",
    "        print(f\"  Val   - Pos: {np.sum(y_val == 1)},   Neg: {np.sum(y_val == 0)}\")\n",
    "        print(f\"  Test  - Pos: {np.sum(y_test == 1)},  Neg: {np.sum(y_test == 0)}\")\n",
    "\n",
    "        # Class weights\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "\n",
    "        # DMatrix\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "        dtest  = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        # Parameter configuration\n",
    "        xgb_params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"auc\",\n",
    "            \"scale_pos_weight\": scale_pos_weight,\n",
    "            \"seed\": seed,\n",
    "            \"verbosity\": 0,\n",
    "            \"learning_rate\": params[\"learning_rate\"],\n",
    "            \"max_depth\": params[\"max_depth\"]\n",
    "        }\n",
    "\n",
    "        # Early stopping\n",
    "        evals = [(dval, \"validation\")]\n",
    "        model = xgb.train(\n",
    "            xgb_params,\n",
    "            dtrain,\n",
    "            num_boost_round=params.get(\"n_estimators\", 200),\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        # Test predictions\n",
    "        y_pred_prob = model.predict(dtest)\n",
    "\n",
    "\n",
    "        # Save current fold results\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "\n",
    "        fold_idx += 1\n",
    "\n",
    "    # Combine fold results\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    # ===================== Bootstrap Calculation of AUROC/AUPRC =====================\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"\\n===== Final Results (Fixed Parameters) =====\")\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afc109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ä½¿ç”¨å›ºå®šå‚æ•°: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200} =====\n",
      "Seed 8226, Fold=1: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n",
      "Seed 8226, Fold=2: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n",
      "Seed 8226, Fold=3: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold=4: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "Seed 8226, Fold=5: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n",
      "\n",
      "===== æœ€ç»ˆç»“æœ (å›ºå®šå‚æ•°) =====\n",
      "AUROC: Mean=0.8485, 95% CI=(0.7961,0.8955)\n",
      "AUPRC: Mean=0.4712, 95% CI=(0.3579,0.5834)\n",
      "{'params': {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.8484887503242158), 'AUROC_CI': (np.float64(0.7961402575137099), np.float64(0.8954669147640918)), 'AUPRC_mean': np.float64(0.4711890699966884), 'AUPRC_CI': (np.float64(0.35788342060344325), np.float64(0.5833624283317074)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.015236\n",
      "1       1       0  0.006273\n",
      "2       1       0  0.012435\n",
      "3       1       0  0.010235\n",
      "4       1       0  0.006132\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.333872\n",
      "967     5       0  0.018177\n",
      "968     5       0  0.100516\n",
      "969     5       0  0.036619\n",
      "970     5       0  0.080217\n",
      "\n",
      "[971 rows x 3 columns]}\n"
     ]
    }
   ],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome_other'] \n",
    "results = train_xgboost_5fold_cv_fixed(dataX, dataY)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d843f4",
   "metadata": {},
   "source": [
    "## Fixed Hyperparameter + 5-fold XGBoost + Calibration (Training Set Fit + Validation Set Calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fixed Hyperparameter + 5-fold XGBoost + Calibration (Training Set Fit + Validation Set Calibration)\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "\n",
    "# ===================== bootstrap calculation function =====================\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "\n",
    "# ===================== 5-fold XGBoostï¼ˆTraining Set Fit + Validation Set Calibrationï¼‰ =====================\n",
    "def train_xgboost_5fold_cv_calibrated(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death_calibration/xgboost/',\n",
    "    seed=8226,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'learning_rate':0.05, 'max_depth':5, 'n_estimators':200}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    X = dataX.copy()\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "    print(f\"\\n===== Use fixed parameters: {params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        print(f\"Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "        print(f\"  Train - Pos: {np.sum(y_train==1)}, Neg: {np.sum(y_train==0)}\")\n",
    "        print(f\"  Val   - Pos: {np.sum(y_val==1)},   Neg: {np.sum(y_val==0)}\")\n",
    "        print(f\"  Test  - Pos: {np.sum(y_test==1)},  Neg: {np.sum(y_test==0)}\")\n",
    "\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos,1)\n",
    "\n",
    "        # ---------------------- training-based model ----------------------\n",
    "        base_model = XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            learning_rate=params['learning_rate'],\n",
    "            max_depth=params['max_depth'],\n",
    "            n_estimators=params['n_estimators'],\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=seed,\n",
    "            verbosity=0,\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "\n",
    "        # Fit with Training Set\n",
    "        base_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            # early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # ---------------------- Validation Set Calibration ----------------------\n",
    "        calib_model = CalibratedClassifierCV(\n",
    "            estimator=base_model,\n",
    "            method='sigmoid',\n",
    "            cv='prefit'\n",
    "        )\n",
    "        calib_model.fit(X_val, y_val)\n",
    "        y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # ---------------------- Deposit book result ----------------------\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "\n",
    "        fold_idx += 1\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    # ===================== bootstrap calculate AUROC/AUPRC =====================\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"\\n===== Final results (fixed parameters + calibration) =====\")\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec3a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ä½¿ç”¨å›ºå®šå‚æ•°: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200} =====\n",
      "Fold=1: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 62, Neg: 617\n",
      "  Val   - Pos: 9,   Neg: 88\n",
      "  Test  - Pos: 11,  Neg: 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=2: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 20,  Neg: 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=3: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=4: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=5: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 57, Neg: 622\n",
      "  Val   - Pos: 8,   Neg: 90\n",
      "  Test  - Pos: 17,  Neg: 177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== æœ€ç»ˆç»“æœ (å›ºå®šå‚æ•° + æ ¡å‡†) =====\n",
      "AUROC: Mean=0.7826, 95% CI=(0.7177,0.8464)\n",
      "AUPRC: Mean=0.4438, 95% CI=(0.3284,0.5637)\n",
      "{'params': {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}, 'AUROC_mean': np.float64(0.7825862494740717), 'AUROC_CI': (np.float64(0.7177408328888394), np.float64(0.8463818331654189)), 'AUPRC_mean': np.float64(0.44378015815334926), 'AUPRC_CI': (np.float64(0.3283563987630393), np.float64(0.5637278679445397)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.068198\n",
      "1       1       0  0.067898\n",
      "2       1       0  0.068452\n",
      "3       1       0  0.068766\n",
      "4       1       0  0.068101\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.099009\n",
      "967     5       0  0.056913\n",
      "968     5       0  0.058103\n",
      "969     5       0  0.056976\n",
      "970     5       0  0.057476\n",
      "\n",
      "[971 rows x 3 columns]}\n"
     ]
    }
   ],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome_other'] \n",
    "results = train_xgboost_5fold_cv_calibrated(dataX, dataY)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7215c08",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fce8e4",
   "metadata": {},
   "source": [
    "- grid search optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn import metrics\n",
    "# from sklearn.model_selection import KFold, train_test_split, ParameterGrid\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import os\n",
    "\n",
    "# def train_randomforest_5fold_cv_gridsearch_save(\n",
    "#     dataX,\n",
    "#     dataY,\n",
    "#     save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death/randomforest_gridsearch_valid_test_5cv/',\n",
    "#     seed=42,\n",
    "#     param_grid=None\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     äº”æŠ˜äº¤å‰éªŒè¯ + è¶…å‚æ•°ç½‘æ ¼æœç´¢ + å†…éƒ¨éªŒè¯é›†åˆ’åˆ† + ä¿å­˜æ¯æŠ˜é¢„æµ‹ CSVï¼ˆæ—  early stoppingï¼‰\n",
    "#     param_grid: dictï¼Œä¾‹å¦‚\n",
    "#         param_grid = {\n",
    "#             \"n_estimators\": [200, 300, 400],\n",
    "#             \"max_depth\": [4, 5, 6],\n",
    "#             \"min_samples_split\": [2, 5, 10],\n",
    "#             \"min_samples_leaf\": [1, 2, 4],\n",
    "#             \"max_features\": [\"sqrt\", \"log2\"]\n",
    "#         }\n",
    "#     \"\"\"\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     from sklearn import metrics\n",
    "#     from sklearn.model_selection import KFold, train_test_split, ParameterGrid\n",
    "#     from sklearn.ensemble import RandomForestClassifier\n",
    "#     import os\n",
    "\n",
    "#     os.makedirs(save_path, exist_ok=True)\n",
    "#     X = dataX.copy()\n",
    "#     y = np.array(dataY)\n",
    "\n",
    "#     if param_grid is None:\n",
    "#         param_grid = {\n",
    "#             \"n_estimators\": [300],\n",
    "#             \"max_depth\": [5],\n",
    "#             \"min_samples_split\": [2],\n",
    "#             \"min_samples_leaf\": [1],\n",
    "#             \"max_features\": [\"sqrt\"]\n",
    "#         }\n",
    "\n",
    "#     grid_results = []\n",
    "\n",
    "#     for params in ParameterGrid(param_grid):\n",
    "#         param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "#         param_path = os.path.join(save_path, param_name)\n",
    "#         os.makedirs(param_path, exist_ok=True)\n",
    "#         print(f\"\\n===== æµ‹è¯•å‚æ•°ç»„åˆ: {params} =====\")\n",
    "\n",
    "#         kf = KFold(n_splits=5, shuffle=True, random_state=seed)  # ä¿®æ”¹ä¸ºäº”æŠ˜\n",
    "#         aurocs, auprcs = [], []\n",
    "#         all_results = []\n",
    "\n",
    "#         fold_idx = 1\n",
    "#         for train_val_index, test_index in kf.split(X):\n",
    "#             # Step1: æ‹†åˆ† 80% train_val, 20% test\n",
    "#             X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "#             y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "#             # Step2: ä» train_val ä¸­å†æ‹† 1/8 åšéªŒè¯é›†\n",
    "#             X_train, X_val, y_train, y_val = train_test_split(\n",
    "#                 X_train_val, y_train_val,\n",
    "#                 test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "#             )\n",
    "\n",
    "#             # ç±»åˆ«æƒé‡ï¼ˆä¸å¹³è¡¡å¤„ç†ï¼‰\n",
    "#             num_pos = np.sum(y_train == 1)\n",
    "#             num_neg = np.sum(y_train == 0)\n",
    "#             scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "#             class_weight = {0: 1.0, 1: scale_pos_weight}\n",
    "\n",
    "#             # è®­ç»ƒæ¨¡å‹\n",
    "#             model = RandomForestClassifier(\n",
    "#                 random_state=seed,\n",
    "#                 n_jobs=-1,\n",
    "#                 class_weight=class_weight,\n",
    "#                 **params\n",
    "#             )\n",
    "#             model.fit(X_train, y_train)\n",
    "\n",
    "#             # æµ‹è¯•é›†é¢„æµ‹\n",
    "#             y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "#             auroc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "#             auprc = metrics.average_precision_score(y_test, y_pred_prob)\n",
    "#             aurocs.append(auroc)\n",
    "#             auprcs.append(auprc)\n",
    "\n",
    "#             # ä¿å­˜æ¯æŠ˜ç»“æœ\n",
    "#             fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "#             pd.DataFrame({\n",
    "#                 \"fold\": fold_idx,\n",
    "#                 \"y_test\": y_test,\n",
    "#                 \"y_pred\": y_pred_prob\n",
    "#             }).to_csv(fold_csv, index=False)\n",
    "#             all_results.append(pd.DataFrame({\n",
    "#                 \"fold\": fold_idx,\n",
    "#                 \"y_test\": y_test,\n",
    "#                 \"y_pred\": y_pred_prob\n",
    "#             }))\n",
    "#             fold_idx += 1\n",
    "\n",
    "#         # æ±‡æ€»æ¯æŠ˜ç»“æœ\n",
    "#         all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "#         all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "#         all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "#         # è®¡ç®—å‡å€¼å’Œ95%CI\n",
    "#         def mean_ci_interval(data):\n",
    "#             mean = np.mean(data)\n",
    "#             std = np.std(data, ddof=1)\n",
    "#             ci95 = 1.96 * std / np.sqrt(len(data))\n",
    "#             return round(mean, 4), round(mean - ci95, 4), round(mean + ci95, 4)\n",
    "\n",
    "#         auroc_mean, auroc_lower, auroc_upper = mean_ci_interval(aurocs)\n",
    "#         auprc_mean, auprc_lower, auprc_upper = mean_ci_interval(auprcs)\n",
    "\n",
    "#         print(f\"AUROC: Mean={auroc_mean:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "#         print(f\"AUPRC: Mean={auprc_mean:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "#         grid_results.append({\n",
    "#             \"params\": params,\n",
    "#             \"AUROC_mean\": auroc_mean,\n",
    "#             \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "#             \"AUPRC_mean\": auprc_mean,\n",
    "#             \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "#             \"AllResults\": all_results_df\n",
    "#         })\n",
    "\n",
    "#     # é€‰æ‹©æœ€ä¼˜å‚æ•°ç»„åˆ\n",
    "#     best = max(grid_results, key=lambda x: x[\"AUROC_mean\"])\n",
    "#     print(f\"\\n===== æœ€ä¼˜å‚æ•° =====\\n{best['params']}\")\n",
    "#     print(f\"AUROC Mean={best['AUROC_mean']:.4f}, 95% CI={best['AUROC_CI']}\")\n",
    "#     print(f\"AUPRC Mean={best['AUPRC_mean']:.4f}, 95% CI={best['AUPRC_CI']}\")\n",
    "\n",
    "#     return grid_results, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"n_estimators\": [200, 300, 400],   # æ£®æ—ä¸­æ ‘çš„æ•°é‡\n",
    "#     \"max_depth\": [4, 5, 6],            # æ ‘çš„æœ€å¤§æ·±åº¦\n",
    "#     # \"min_samples_split\": [2, 5, 10],   # å†…éƒ¨èŠ‚ç‚¹å†åˆ’åˆ†æ‰€éœ€æœ€å°æ ·æœ¬æ•°\n",
    "#     # \"min_samples_leaf\": [1, 2, 4],     # å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°\n",
    "#     # \"max_features\": [\"sqrt\", \"log2\", None]  # æ¯æ¬¡åˆ’åˆ†æ—¶è€ƒè™‘çš„ç‰¹å¾æ•°\n",
    "# }  # RandomForestClassifier æ²¡æœ‰ learning_rate å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "# dataY = df_mapped_wide['Outcome_other']  # é¢„æµ‹æ˜¯å¦æ­»äº¡\n",
    "# results = train_randomforest_5fold_cv_gridsearch_save(dataX, dataY, param_grid=param_grid)  \n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f09ba9",
   "metadata": {},
   "source": [
    "Because RF fit process can not specify validation set, so RF validation set is equivalent to not used, so RF parameter adjustment is not important, super parameter setting is consistent with catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "'/home/mailiyi/Poisoning_Prediction/ML/predict_death/randomforest_gridsearch_valid_test_5cv/max_depth_5_n_estimators_200'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eaebc2",
   "metadata": {},
   "source": [
    "- probability calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc08b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import os\n",
    "\n",
    "def train_randomforest_5fold_cv_calibrated(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death_calibration/randomforest/',\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    äº”æŠ˜äº¤å‰éªŒè¯ï¼ˆå›ºå®šè¶…å‚æ•°ï¼‰+ å†…éƒ¨éªŒè¯é›†åˆ’åˆ† + æ ¡å‡† + ä¿å­˜æ¯æŠ˜é¢„æµ‹ç»“æœ\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    X = dataX.copy()\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    fixed_params = {\n",
    "        \"n_estimators\": 200,\n",
    "        \"max_depth\": 5,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"min_samples_leaf\": 1,\n",
    "        \"max_features\": \"sqrt\"\n",
    "    }\n",
    "\n",
    "    print(f\"\\n===== ä½¿ç”¨å›ºå®šå‚æ•°: {fixed_params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    aurocs, auprcs = [], []\n",
    "    all_results = []\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        print(f\"\\n===== Fold {fold_idx} =====\")\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # æ‹†åˆ†éªŒè¯é›†\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        # ç±»åˆ«æƒé‡\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "        class_weight = {0: 1.0, 1: scale_pos_weight}\n",
    "\n",
    "        # ---------------------- è®­ç»ƒåŸºæ¨¡å‹ ----------------------\n",
    "        base_model = RandomForestClassifier(\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            class_weight=class_weight,\n",
    "            **fixed_params\n",
    "        )\n",
    "        base_model.fit(X_train, y_train)\n",
    "\n",
    "        # ---------------------- éªŒè¯é›†æ ¡å‡† ----------------------\n",
    "        calib_model = CalibratedClassifierCV(\n",
    "            estimator=base_model,\n",
    "            method='sigmoid',\n",
    "            cv='prefit'  # base_model å·²ç»æ‹Ÿåˆ\n",
    "        )\n",
    "        calib_model.fit(X_val, y_val)\n",
    "\n",
    "        # ---------------------- æµ‹è¯•é›†é¢„æµ‹ ----------------------\n",
    "        y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "        auroc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "        auprc = metrics.average_precision_score(y_test, y_pred_prob)\n",
    "        aurocs.append(auroc)\n",
    "        auprcs.append(auprc)\n",
    "\n",
    "        print(f\"[Fold {fold_idx}] AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "        # ä¿å­˜æ¯æŠ˜ç»“æœ\n",
    "        fold_csv = os.path.join(save_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    # æ±‡æ€»æ‰€æœ‰æŠ˜ç»“æœ\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    # è®¡ç®—å‡å€¼å’Œ95%CI\n",
    "    def mean_ci_interval(data):\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data, ddof=1)\n",
    "        ci95 = 1.96 * std / np.sqrt(len(data))\n",
    "        return round(mean, 4), round(mean - ci95, 4), round(mean + ci95, 4)\n",
    "\n",
    "    auroc_mean, auroc_lower, auroc_upper = mean_ci_interval(aurocs)\n",
    "    auprc_mean, auprc_lower, auprc_upper = mean_ci_interval(auprcs)\n",
    "\n",
    "    print(\"\\n===== äº”æŠ˜äº¤å‰éªŒè¯ç»“æœ (æ ¡å‡†å) =====\")\n",
    "    print(f\"AUROC: Mean={auroc_mean:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={auprc_mean:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    results_summary = {\n",
    "        \"AUROC_mean\": auroc_mean,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": auprc_mean,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper)\n",
    "    }\n",
    "\n",
    "    return results_summary, all_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78e0c5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ä½¿ç”¨å›ºå®šå‚æ•°: {'n_estimators': 200, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'} =====\n",
      "\n",
      "===== Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] AUROC: 0.8331, AUPRC: 0.5141\n",
      "\n",
      "===== Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2] AUROC: 0.7806, AUPRC: 0.3484\n",
      "\n",
      "===== Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3] AUROC: 0.9349, AUPRC: 0.4333\n",
      "\n",
      "===== Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4] AUROC: 0.8694, AUPRC: 0.5235\n",
      "\n",
      "===== Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5] AUROC: 0.7676, AUPRC: 0.5396\n",
      "\n",
      "===== äº”æŠ˜äº¤å‰éªŒè¯ç»“æœ (æ ¡å‡†å) =====\n",
      "AUROC: Mean=0.8371, 95% CI=(0.7773,0.8970)\n",
      "AUPRC: Mean=0.4718, 95% CI=(0.4014,0.5421)\n",
      "({'AUROC_mean': np.float64(0.8371), 'AUROC_CI': (np.float64(0.7773), np.float64(0.897)), 'AUPRC_mean': np.float64(0.4718), 'AUPRC_CI': (np.float64(0.4014), np.float64(0.5421))},      fold  y_test    y_pred\n",
      "0       1       0  0.051723\n",
      "1       1       0  0.039058\n",
      "2       1       0  0.022700\n",
      "3       1       0  0.024287\n",
      "4       1       0  0.032955\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.024558\n",
      "967     5       0  0.097325\n",
      "968     5       0  0.071954\n",
      "969     5       0  0.035738\n",
      "970     5       0  0.026533\n",
      "\n",
      "[971 rows x 3 columns])\n"
     ]
    }
   ],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome_other']  # é¢„æµ‹æ˜¯å¦æ­»äº¡\n",
    "results = train_randomforest_5fold_cv_calibrated(dataX, dataY)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98684bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c67df31b",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import os \n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "# ===================== Bootstrap calculation function ===================== \n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42): \n",
    "    rng = np.random.RandomState(seed) \n",
    "    scores = [] \n",
    "    y_true = np.array(y_true) \n",
    "    y_pred = np.array(y_pred) \n",
    "    for _ in range(n_bootstrap): \n",
    "        idx = rng.randint(0, len(y_true), len(y_true)) \n",
    "        if len(np.unique(y_true[idx])) < 2: \n",
    "            continue \n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx])) \n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5) \n",
    " \n",
    " \n",
    "# ===================== Five-fold Logistic Regression ===================== \n",
    "def train_lr_5fold_cv( \n",
    "    dataX, \n",
    "    dataY, \n",
    "    num_features, \n",
    "    cat_features, \n",
    "    save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death/lr_valid_test_5cv/',\n",
    "    seed=9762 \n",
    "): \n",
    "    os.makedirs(save_path, exist_ok=True) \n",
    "    X = dataX.copy() \n",
    "    y = np.array(dataY) \n",
    " \n",
    "    # ===================== Feature preprocessing ===================== \n",
    "    # Fill missing values in continuous variables with the median \n",
    "    imputer = SimpleImputer(strategy='median') \n",
    "    X_num = pd.DataFrame(imputer.fit_transform(X[num_features]), columns=num_features, index=X.index) \n",
    "\n",
    "    # âœ… Normalize continuous variables (standardization)\n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = pd.DataFrame(scaler.fit_transform(X_num), columns=num_features, index=X.index)\n",
    " \n",
    "    \n",
    "    # For categorical variables: fill missing values with 'missing'\n",
    "    X_cat = X[cat_features].astype(str).fillna('missing')\n",
    "\n",
    "    # OneHot encoding (drop the first category to avoid collinearity)\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
    "    X_cat_encoded = pd.DataFrame(\n",
    "        encoder.fit_transform(X_cat),\n",
    "        columns=encoder.get_feature_names_out(cat_features),\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    # Combine the encoded features\n",
    "    X_processed = pd.concat([X_num_scaled, X_cat_encoded], axis=1)\n",
    "\n",
    "    print(f\"Final feature dimension: {X_processed.shape[1]}\") \n",
    " \n",
    "    # ===================== Five-fold cross-validation ===================== \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed) \n",
    "    all_results = [] \n",
    "    fold_idx = 1 \n",
    " \n",
    "    for train_val_index, test_index in kf.split(X_processed): \n",
    "        X_train_val, X_test = X_processed.iloc[train_val_index], X_processed.iloc[test_index] \n",
    "        y_train_val, y_test = y[train_val_index], y[test_index] \n",
    " \n",
    "        # Split validation set (1/8) \n",
    "        X_train, X_val, y_train, y_val = train_test_split( \n",
    "            X_train_val, y_train_val, \n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val \n",
    "        ) \n",
    " \n",
    "        print(f\"Fold {fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\") \n",
    " \n",
    "        # Check class balance \n",
    "        n_pos = np.sum(y_train == 1)\n",
    "        n_neg = np.sum(y_train == 0)\n",
    "        print(f\"  Train - Pos: {n_pos}, Neg: {n_neg}\") \n",
    "        print(f\"  Test  - Pos: {np.sum(y_test==1)}, Neg: {np.sum(y_test==0)}\") \n",
    " \n",
    "        # ===================== Manually set class_weight ===================== \n",
    "        w_pos = n_neg / max(n_pos, 1)\n",
    "        class_weight = {0: 1.0, 1: w_pos}\n",
    "        print(f\"  Using class_weight = {class_weight}\") \n",
    " \n",
    "        # Model definition (with weights)\n",
    "        model = LogisticRegression(\n",
    "            max_iter=1000, \n",
    "            solver='lbfgs',\n",
    "            class_weight=class_weight\n",
    "        ) \n",
    "        # model = LogisticRegression(\n",
    "        #     max_iter=200, \n",
    "        #     solver='lbfgs',\n",
    "        #     class_weight=class_weight\n",
    "        # ) \n",
    " \n",
    "        # Train \n",
    "        model.fit(X_train, y_train) \n",
    " \n",
    "        # Predict \n",
    "        y_pred_prob = model.predict_proba(X_test)[:, 1] \n",
    " \n",
    "        # Save results for the current fold \n",
    "        fold_csv = os.path.join(save_path, f\"fold_{fold_idx}_results.csv\") \n",
    "        pd.DataFrame({ \n",
    "            \"fold\": fold_idx, \n",
    "            \"y_test\": y_test, \n",
    "            \"y_pred\": y_pred_prob \n",
    "        }).to_csv(fold_csv, index=False) \n",
    " \n",
    "        all_results.append(pd.DataFrame({ \n",
    "            \"fold\": fold_idx, \n",
    "            \"y_test\": y_test, \n",
    "            \"y_pred\": y_pred_prob \n",
    "        })) \n",
    " \n",
    "        fold_idx += 1 \n",
    " \n",
    "    # ===================== Combine results from all folds ===================== \n",
    "    all_results_df = pd.concat(all_results, ignore_index=True) \n",
    "    all_csv = os.path.join(save_path, \"all_folds_results.csv\") \n",
    "    all_results_df.to_csv(all_csv, index=False) \n",
    " \n",
    "    # ===================== Bootstrap calculation for AUROC/AUPRC ===================== \n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci( \n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score \n",
    "    ) \n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci( \n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score \n",
    "    ) \n",
    " \n",
    "    print(f\"\\n===== Logistic Regression Results =====\") \n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f}, {auroc_upper:.4f})\") \n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f}, {auprc_upper:.4f})\") \n",
    " \n",
    "    return { \n",
    "        \"AUROC_mean\": mean_auroc, \n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper), \n",
    "        \"AUPRC_mean\": mean_auprc, \n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper), \n",
    "        \"AllResults\": all_results_df \n",
    "    } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb35856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€ç»ˆç‰¹å¾ç»´åº¦: 78\n",
      "Fold 1: Train=679, Val=97, Test=195\n",
      "  Train - Pos: 59, Neg: 620\n",
      "  Test  - Pos: 14, Neg: 181\n",
      "  ä½¿ç”¨ class_weight = {0: 1.0, 1: np.float64(10.508474576271187)}\n",
      "Fold 2: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 55, Neg: 624\n",
      "  Test  - Pos: 19, Neg: 175\n",
      "  ä½¿ç”¨ class_weight = {0: 1.0, 1: np.float64(11.345454545454546)}\n",
      "Fold 3: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 60, Neg: 619\n",
      "  Test  - Pos: 13, Neg: 181\n",
      "  ä½¿ç”¨ class_weight = {0: 1.0, 1: np.float64(10.316666666666666)}\n",
      "Fold 4: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 58, Neg: 621\n",
      "  Test  - Pos: 16, Neg: 178\n",
      "  ä½¿ç”¨ class_weight = {0: 1.0, 1: np.float64(10.706896551724139)}\n",
      "Fold 5: Train=679, Val=98, Test=194\n",
      "  Train - Pos: 54, Neg: 625\n",
      "  Test  - Pos: 20, Neg: 174\n",
      "  ä½¿ç”¨ class_weight = {0: 1.0, 1: np.float64(11.574074074074074)}\n",
      "\n",
      "===== Logistic Regression ç»“æœ =====\n",
      "AUROC: Mean=0.7893, 95% CI=(0.7313, 0.8447)\n",
      "AUPRC: Mean=0.3356, 95% CI=(0.2398, 0.4352)\n",
      "{'AUROC_mean': np.float64(0.7892811640996074), 'AUROC_CI': (np.float64(0.7313339869373903), np.float64(0.8446620900655896)), 'AUPRC_mean': np.float64(0.33557580007852694), 'AUPRC_CI': (np.float64(0.23980652151812515), np.float64(0.4351551788308765)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.813343\n",
      "1       1       0  0.295661\n",
      "2       1       0  0.004227\n",
      "3       1       0  0.008465\n",
      "4       1       0  0.001349\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.178524\n",
      "967     5       0  0.000586\n",
      "968     5       0  0.113394\n",
      "969     5       0  0.323689\n",
      "970     5       0  0.828924\n",
      "\n",
      "[971 rows x 3 columns]}\n"
     ]
    }
   ],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome_other'] \n",
    "results = train_lr_5fold_cv(dataX, dataY,num_features=features_continuous, cat_features=features_categorical)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c5185",
   "metadata": {},
   "source": [
    "- Calibrate models for probabilistic calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4028642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import os \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def train_lr_5fold_cv_calibrated(\n",
    "    dataX, \n",
    "    dataY, \n",
    "    num_features, \n",
    "    cat_features, \n",
    "    save_path='/home/mailiyi/Poisoning_Prediction/ML/predict_death_calibration/lr/',\n",
    "    seed=9762\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True) \n",
    "    X = dataX.copy() \n",
    "    y = np.array(dataY) \n",
    "\n",
    "    # Fill missing values for numerical variables + standardization\n",
    "    imputer = SimpleImputer(strategy='median') \n",
    "    X_num = pd.DataFrame(imputer.fit_transform(X[num_features]), columns=num_features, index=X.index) \n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = pd.DataFrame(scaler.fit_transform(X_num), columns=num_features, index=X.index)\n",
    "\n",
    "    # Fill missing values for categorical variables + OneHot encoding\n",
    "    X_cat = X[cat_features].astype(str).fillna('missing')\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
    "    X_cat_encoded = pd.DataFrame(\n",
    "        encoder.fit_transform(X_cat),\n",
    "        columns=encoder.get_feature_names_out(cat_features),\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    # Combine processed numerical and categorical features\n",
    "    X_processed = pd.concat([X_num_scaled, X_cat_encoded], axis=1)\n",
    "    print(f\"Final feature dimension: {X_processed.shape[1]}\") \n",
    "\n",
    "    # 5-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed) \n",
    "    all_results = [] \n",
    "    fold_idx = 1 \n",
    "\n",
    "    for train_val_index, test_index in kf.split(X_processed): \n",
    "        X_train_val, X_test = X_processed.iloc[train_val_index], X_processed.iloc[test_index] \n",
    "        y_train_val, y_test = y[train_val_index], y[test_index] \n",
    "\n",
    "        # Split validation set\n",
    "        X_train, X_val, y_train, y_val = train_test_split( \n",
    "            X_train_val, y_train_val, \n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val \n",
    "        ) \n",
    "\n",
    "        print(f\"Fold {fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\") \n",
    "\n",
    "        # Class weights\n",
    "        n_pos = np.sum(y_train == 1)\n",
    "        n_neg = np.sum(y_train == 0)\n",
    "        class_weight = {0: 1.0, 1: n_neg / max(n_pos, 1)}\n",
    "        print(f\"  Using class_weight = {class_weight}\") \n",
    "\n",
    "        # Train base model\n",
    "        base_model = LogisticRegression(max_iter=1000, solver='lbfgs', class_weight=class_weight)\n",
    "        base_model.fit(X_train, y_train)\n",
    "\n",
    "        # Calibration on validation set\n",
    "        calib_model = CalibratedClassifierCV(base_model, method='sigmoid', cv='prefit')\n",
    "        calib_model.fit(X_val, y_val)\n",
    "\n",
    "        # Predictions on test set\n",
    "        y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Save fold results\n",
    "        fold_csv = os.path.join(save_path, f\"fold_{fold_idx}_results.csv\") \n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx, \n",
    "            \"y_test\": y_test, \n",
    "            \"y_pred\": y_pred_prob \n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx, \n",
    "            \"y_test\": y_test, \n",
    "            \"y_pred\": y_pred_prob \n",
    "        })) \n",
    "        fold_idx += 1 \n",
    "\n",
    "    # Combine results from all folds\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True) \n",
    "    all_csv = os.path.join(save_path, \"all_folds_results.csv\") \n",
    "    all_results_df.to_csv(all_csv, index=False) \n",
    "\n",
    "    # Bootstrap to calculate AUROC/AUPRC\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score \n",
    "    ) \n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score \n",
    "    ) \n",
    "\n",
    "    print(f\"\\n===== Logistic Regression Calibration Results =====\") \n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f}, {auroc_upper:.4f})\") \n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f}, {auprc_upper:.4f})\") \n",
    "\n",
    "    return {\n",
    "        \"AUROC_mean\": mean_auroc, \n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper), \n",
    "        \"AUPRC_mean\": mean_auprc, \n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper), \n",
    "        \"AllResults\": all_results_df \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962cc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€ç»ˆç‰¹å¾ç»´åº¦: 78\n",
      "Fold 1: Train=679, Val=97, Test=195\n",
      "  ä½¿ç”¨ class_weight = {0: 1.0, 1: np.float64(10.508474576271187)}\n",
      "Fold 2: Train=679, Val=98, Test=194\n",
      "  ä½¿ç”¨ class_weight = {0: 1.0, 1: np.float64(11.345454545454546)}\n",
      "Fold 3: Train=679, Val=98, Test=194\n",
      "  ä½¿ç”¨ class_weight = {0: 1.0, 1: np.float64(10.316666666666666)}\n",
      "Fold 4: Train=679, Val=98, Test=194\n",
      "  ä½¿ç”¨ class_weight = {0: 1.0, 1: np.float64(10.706896551724139)}\n",
      "Fold 5: Train=679, Val=98, Test=194\n",
      "  ä½¿ç”¨ class_weight = {0: 1.0, 1: np.float64(11.574074074074074)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/home/mailiyi/.conda/envs/ml_env/lib/python3.11/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Logistic Regression æ ¡å‡†ç»“æœ =====\n",
      "AUROC: Mean=0.8080, 95% CI=(0.7529, 0.8594)\n",
      "AUPRC: Mean=0.3792, 95% CI=(0.2740, 0.4916)\n",
      "{'AUROC_mean': np.float64(0.8079921624354208), 'AUROC_CI': (np.float64(0.752923285310382), np.float64(0.8594135799228018)), 'AUPRC_mean': np.float64(0.37915846520377694), 'AUPRC_CI': (np.float64(0.27398175044700224), np.float64(0.4916145327736861)), 'AllResults':      fold  y_test    y_pred\n",
      "0       1       0  0.160134\n",
      "1       1       0  0.111295\n",
      "2       1       0  0.052013\n",
      "3       1       0  0.058563\n",
      "4       1       0  0.042760\n",
      "..    ...     ...       ...\n",
      "966     5       0  0.094177\n",
      "967     5       0  0.021069\n",
      "968     5       0  0.082806\n",
      "969     5       0  0.113704\n",
      "970     5       0  0.191994\n",
      "\n",
      "[971 rows x 3 columns]}\n"
     ]
    }
   ],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome_other']\n",
    "results = train_lr_5fold_cv_calibrated(dataX, dataY,num_features=features_continuous, cat_features=features_categorical)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90347d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
