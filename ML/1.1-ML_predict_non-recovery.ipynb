{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d8b5c7",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdbd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_albumin_clean = pd.read_excel('./Poisoning_Prediction/all_poisoning_data_wide_clean_albumin_20251106.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222dacc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_categorical = ['Gender',\n",
    " 'Education Level',\n",
    " 'Type of Poisoning',\n",
    " 'Hypertension',\n",
    " 'Hyperlipidemia',\n",
    " 'Diabetes Mellitus',\n",
    " 'Cerebrovascular Disease',\n",
    " 'Heart Disease',\n",
    " 'Allergy History',\n",
    " 'Cancer',\n",
    " 'Poisoning',\n",
    " 'degree of poisoning',\n",
    " 'Smoking Status',\n",
    " 'Alcohol Consumption Status',\n",
    " 'Shortness of Breath',\n",
    " 'Chest Pain',\n",
    " 'Cough',\n",
    " 'Pre-syncope',\n",
    " 'Altered Consciousness or Syncope',\n",
    " 'Sore Throat',\n",
    " 'Fever',\n",
    " 'Fatigue',\n",
    " 'Lower Limb Edema',\n",
    " 'Palpitations',\n",
    " 'Vomiting',\n",
    " 'Nausea',\n",
    " 'Weakness',\n",
    " 'Headache',\n",
    " 'Residence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9e60be",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_mappings_en = {\n",
    "    \"Gender\": {\n",
    "        1: \"Male\",\n",
    "        0: \"Female\"\n",
    "    },\n",
    "    \"Education Level\": {\n",
    "        1: \"Illiterate\",\n",
    "        2: \"Primary School\",\n",
    "        3: \"Junior High School\",\n",
    "        4: \"Senior High School\",\n",
    "        5: \"University Degree\"\n",
    "    },\n",
    "    \"Type of Poisoning\": {\n",
    "        1: \"Industrial\",\n",
    "        2: \"Pharmaceutical\",\n",
    "        3: \"Pesticide\",\n",
    "        4: \"Alcohol\",\n",
    "        0: \"Uncertain\"\n",
    "    },\n",
    "    \"Hypertension\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Hyperlipidemia\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Diabetes Mellitus\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Cerebrovascular Disease\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Heart Disease\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Allergy History\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Cancer\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Poisoning\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"degree of poisoning\": {\n",
    "        0: \"Undetermined\",\n",
    "        1: \"Low\",\n",
    "        2: \"Moderate\",\n",
    "        3: \"High\"\n",
    "    },\n",
    "    \"Smoking\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Alcohol Consumption Status\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Shortness of Breath\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Chest Pain\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Cough\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Pre-syncope\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Altered Mental Status or Syncope(AMS or Sync)\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Sore Throat\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Fever\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Fatigue\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Lower Limb Edema\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Palpitations\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Vomiting\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Nausea\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Weakness\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Headache\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    \"Residence\": {\n",
    "        1: \"Rural\",\n",
    "        2: \"Urban\"\n",
    "    },\n",
    "    \"Smoking Status\": {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },\n",
    "    'Altered Consciousness or Syncope': {\n",
    "        1: \"Yes\",\n",
    "        0: \"No\"\n",
    "    },    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inverse mapping (English label → numerical value)\n",
    "df_mapped_wide = df_albumin_clean.copy()\n",
    "for col in features_categorical:\n",
    "    if col in value_mappings_en and col in df_mapped_wide.columns:\n",
    "        inv_map = {v: k for k, v in value_mappings_en[col].items()}\n",
    "        df_mapped_wide[col] = df_mapped_wide[col].map(inv_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b63a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the distribution of each categorical variable\n",
    "category_distributions = {}\n",
    "\n",
    "for col in features_categorical:\n",
    "    if col in df_mapped_wide.columns:\n",
    "        counts = df_mapped_wide[col].value_counts(dropna=False)\n",
    "        category_distributions[col] = counts\n",
    "        print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79312ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics Distribution of Outcome_other and Outcome\n",
    "print(df_mapped_wide[\"Outcome_other\"].value_counts(dropna=False))\n",
    "print(df_mapped_wide[\"Outcome\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58352688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9aa9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate missing proportions for continuous variables\n",
    "\n",
    "features_continuous = ['Age',\n",
    " 'Length of Stay',\n",
    " 'Weight',\n",
    " 'Systolic Blood Pressure',\n",
    " 'Diastolic Blood Pressure',\n",
    " 'Respiratory Rate',\n",
    " 'Heart Rate',\n",
    " 'White Blood Cell Count',\n",
    " 'Red Blood Cell Count',\n",
    " 'Hemoglobin Concentration',\n",
    " 'Mean Corpuscular Volume',\n",
    " 'Mean Corpuscular Hemoglobin',\n",
    " 'Mean Corpuscular Hemoglobin Concentration',\n",
    " 'Platelet Count',\n",
    " 'Mean Platelet Volume',\n",
    " 'Alanine Aminotransferase (ALT)',\n",
    " 'Total Bilirubin',\n",
    " 'Direct Bilirubin',\n",
    " 'Lactate Dehydrogenase (LDH)',\n",
    " 'Urea',\n",
    " 'Serum Creatinine',\n",
    " 'Uric Acid',\n",
    " 'Creatine Kinase (CK)',\n",
    " 'Creatine Kinase-MB Isoenzyme',\n",
    " 'Troponin I',\n",
    " 'High-Sensitivity C-Reactive Protein (hs-CRP)',\n",
    " 'Homocysteine',\n",
    " 'Potassium',\n",
    " 'Sodium',\n",
    " 'Chloride',\n",
    " 'Carbon Dioxide',\n",
    " 'Prothrombin Time',\n",
    " 'D-Dimer',\n",
    " 'Lactate',\n",
    " 'Blood Cholinesterase Test Results',\n",
    " 'Albumin (First Measurement)',\n",
    " 'Albumin (Last Measurement)',\n",
    " 'Number of Hemoperfusion Sessions',\n",
    " 'Number of Blood Purification Sessions',\n",
    " 'Hyperbaric Oxygen Therapy Duration and Frequency',\n",
    " 'Atropine Dosage',\n",
    " 'Long-acting Nitroglycerin Dosage',\n",
    " 'Pralidoxime Dosage',\n",
    " ] \n",
    "\n",
    "missing_ratios = df_mapped_wide[features_continuous].isnull().mean()\n",
    "missing_summary = (missing_ratios * 100).round(2).sort_values(ascending=False)\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d58cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names screened for deletion rates> 90%\n",
    "high_missing_features = missing_ratios[missing_ratios > 0.90].index.tolist()\n",
    "for feat in high_missing_features:\n",
    "    print(f\"{feat}: {missing_ratios[feat]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c950c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(features_continuous))\n",
    "print(len(high_missing_features))\n",
    "features_continuous = [feat for feat in features_continuous if feat not in high_missing_features]\n",
    "print(len(features_continuous))\n",
    "\n",
    "df_mapped_wide = df_mapped_wide.drop(columns=high_missing_features)\n",
    "print(df_mapped_wide.shape)\n",
    "\n",
    "print('number of features：',len(features_categorical + features_continuous))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104809af",
   "metadata": {},
   "source": [
    "##### 5-fold cross validation: Divide 1/8 of the training set into validation sets (i.e. 70% training set, 10% validation set, 20% test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16489a72",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489aedf6",
   "metadata": {},
   "source": [
    "- Use the new fixed random seed number + the previous optimal hyperparameter (the same hyperparameter used to predict whether or not to die)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2aff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import os\n",
    "\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "def train_catboost_5fold_cv_fixed(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    cat_features=None,\n",
    "    save_path='./Poisoning_Prediction/ML/predict_non-recovery/catboost_fixed_valid_test_5cv/',\n",
    "    seed=42,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataX = dataX.copy()\n",
    "    if cat_features is not None:\n",
    "        for c in cat_features:\n",
    "            dataX[c] = dataX[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "    X = dataX\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        # Step1: train_val / test\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # Step2: train / val\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        print(f\"Seed {seed}, Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "        print(f\"  Train - Pos: {np.sum(y_train == 1)}, Neg: {np.sum(y_train == 0)}\")\n",
    "        print(f\"  Val   - Pos: {np.sum(y_val == 1)},   Neg: {np.sum(y_val == 0)}\")\n",
    "        print(f\"  Test  - Pos: {np.sum(y_test == 1)},  Neg: {np.sum(y_test == 0)}\")\n",
    "\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "\n",
    "        # Pool\n",
    "        train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "        val_pool = Pool(X_val, label=y_val, cat_features=cat_features)\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            **params,\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "        y_pred_prob = model.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23774be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome'] \n",
    "results = train_catboost_5fold_cv_fixed(dataX, dataY,cat_features=features_categorical)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f5fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the optimal model \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import os\n",
    "\n",
    "def find_best_threshold_by_youden(y_true, y_pred_prob):\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred_prob)\n",
    "    specificity = 1 - fpr\n",
    "    youden_index = tpr + specificity - 1\n",
    "    best_idx = np.argmax(youden_index)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "\n",
    "    return best_threshold, youden_index[best_idx], tpr[best_idx], specificity[best_idx]\n",
    "\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "def train_catboost_5fold_cv_best_save(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    cat_features=None,\n",
    "    save_path='./Poisoning_Prediction/ML/predict_non-recovery_best_model/catboost/',\n",
    "    seed=42,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataX = dataX.copy()\n",
    "    if cat_features is not None:\n",
    "        for c in cat_features:\n",
    "            dataX[c] = dataX[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "    X = dataX\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    # ⭐ Record the best model for the five-way validation set AUC\n",
    "    best_fold_auc = -np.inf\n",
    "    best_fold_model_path = None\n",
    "    best_fold_idx = None\n",
    "\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "\n",
    "        # Step1: train_val / test\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # Step2: train / val\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        print(f\"Seed {seed}, Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "\n",
    "        # Pool\n",
    "        train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "        val_pool = Pool(X_val, label=y_val, cat_features=cat_features)\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            **params,\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "        \n",
    "        # ============================ AUC 输出 ============================\n",
    "\n",
    "        # Validation AUC\n",
    "        val_pred = model.predict_proba(val_pool)[:, 1]\n",
    "        fold_val_auc = metrics.roc_auc_score(y_val, val_pred)\n",
    "        print(f\"Fold {fold_idx} - Validation AUC = {fold_val_auc:.4f}\")\n",
    "\n",
    "        # Test AUC\n",
    "        y_pred_prob = model.predict_proba(test_pool)[:, 1]\n",
    "        fold_test_auc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "        print(f\"Fold {fold_idx} - Test AUC = {fold_test_auc:.4f}\")\n",
    "\n",
    "        # ================================================================\n",
    "\n",
    "        # ------------------------------\n",
    "        # ⭐ Save the best model of the fold\n",
    "        # ------------------------------\n",
    "        fold_model_path = os.path.join(param_path, f\"best_model_fold_{fold_idx}.cbm\")\n",
    "        model.save_model(fold_model_path)\n",
    "        print(f\"Fold {fold_idx} Optimal model saved: {fold_model_path}\")\n",
    "\n",
    "        val_pred = model.predict_proba(val_pool)[:, 1]\n",
    "        fold_val_auc = metrics.roc_auc_score(y_val, val_pred)\n",
    "\n",
    "        if fold_val_auc > best_fold_auc:\n",
    "            best_fold_auc = fold_val_auc\n",
    "            best_fold_model_path = fold_model_path\n",
    "            best_fold_idx = fold_idx  \n",
    "\n",
    "        y_pred_prob = model.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # ⭐ Copy the best model of the five-way to the main directory\n",
    "    # ----------------------------------------\n",
    "    best_overall_path = os.path.join(param_path, \"best_overall_model.cbm\")\n",
    "    if best_fold_model_path is not None:\n",
    "        import shutil\n",
    "        shutil.copy(best_fold_model_path, best_overall_path)\n",
    "        print(f\"\\n===== Five-fold optimal model: Fold {best_fold_idx}, \"f\"Validation AUC={best_fold_auc:.4f} =====\")\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    best_thresh, best_youden, best_sen, best_spec = find_best_threshold_by_youden(\n",
    "        all_results_df[\"y_test\"], \n",
    "        all_results_df[\"y_pred\"]\n",
    "    )\n",
    "\n",
    "    print(f\"optimal threshold (cut-off) = {best_thresh:.4f}\")\n",
    "    print(f\"Youden Index = {best_youden:.4f}\")\n",
    "    print(f\"Sensitivity  = {best_sen:.4f}\")\n",
    "    print(f\"Specificity  = {best_spec:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df,\n",
    "        \"BestModelPath\": best_overall_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cb63325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome\n",
       "0    731\n",
       "1    240\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mapped_wide['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca80e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2bbff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataX = dataX.drop(columns=['Residence','Hypertension'])\n",
    "cat_features = [c for c in features_categorical if c in dataX.columns]\n",
    "dataY = df_mapped_wide['Outcome']\n",
    "# results = train_catboost_5fold_cv_best_save(dataX, dataY,cat_features=features_categorical)\n",
    "results = train_catboost_5fold_cv_best_save(dataX, dataY,cat_features=cat_features)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee97af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the optimal model (Probabilistic Calibration Model) ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import shutil\n",
    "\n",
    "def find_best_threshold_by_youden(y_true, y_pred_prob):\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred_prob)\n",
    "    specificity = 1 - fpr\n",
    "    youden_index = tpr + specificity - 1\n",
    "    best_idx = np.argmax(youden_index)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    return best_threshold, youden_index[best_idx], tpr[best_idx], specificity[best_idx]\n",
    "\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "def train_catboost_5fold_cv_best_save(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    cat_features=None,\n",
    "    save_path='./Poisoning_Prediction/ML/predict_non-recovery_best_model/catboost_calibration/',\n",
    "    seed=42,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataX = dataX.copy()\n",
    "    if cat_features is not None:\n",
    "        for c in cat_features:\n",
    "            dataX[c] = dataX[c].astype(str).fillna(\"missing\")\n",
    "    X = dataX\n",
    "    y = np.array(dataY)\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    # ⭐ Record the best model for the five-way validation set AUC\n",
    "    best_fold_auc = -np.inf\n",
    "    best_fold_model_path = None\n",
    "    best_fold_idx = None\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        # Step1: train_val / test\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # Step2: train / val\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        print(f\"Seed {seed}, Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "\n",
    "        # Pool\n",
    "        train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "        val_pool = Pool(X_val, label=y_val, cat_features=cat_features)\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "        cat_model = CatBoostClassifier(\n",
    "            **params,\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "        cat_model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "        calib_model = CalibratedClassifierCV(\n",
    "            estimator=cat_model,\n",
    "            method='sigmoid',\n",
    "            cv='prefit'\n",
    "        )\n",
    "        calib_model.fit(X_val, y_val)\n",
    "\n",
    "        y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Validation AUC\n",
    "        val_pred = calib_model.predict_proba(X_val)[:, 1]\n",
    "        fold_val_auc = metrics.roc_auc_score(y_val, val_pred)\n",
    "        fold_test_auc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "        print(f\"Fold {fold_idx} - Validation AUC = {fold_val_auc:.4f}\")\n",
    "        print(f\"Fold {fold_idx} - Test AUC       = {fold_test_auc:.4f}\")\n",
    "\n",
    "        # ------------------------------\n",
    "        # ⭐ Save the calibration model\n",
    "        # ------------------------------\n",
    "        fold_model_path = os.path.join(param_path, f\"best_model_fold_{fold_idx}.pkl\")\n",
    "        joblib.dump(calib_model, fold_model_path)\n",
    "        print(f\"Fold {fold_idx} Calibration model saved: {fold_model_path}\")\n",
    "\n",
    "        if fold_val_auc > best_fold_auc:\n",
    "            best_fold_auc = fold_val_auc\n",
    "            best_fold_model_path = fold_model_path\n",
    "            best_fold_idx = fold_idx\n",
    "            \n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    best_overall_path = os.path.join(param_path, \"best_overall_model.pkl\")\n",
    "    if best_fold_model_path is not None:\n",
    "        shutil.copy(best_fold_model_path, best_overall_path)\n",
    "        print(f\"Model saved to: {best_overall_path}\")\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    best_thresh, best_youden, best_sen, best_spec = find_best_threshold_by_youden(\n",
    "        all_results_df[\"y_test\"], \n",
    "        all_results_df[\"y_pred\"]\n",
    "    )\n",
    "\n",
    "    print(f\"optimal threshold (cut-off) = {best_thresh:.4f}\")\n",
    "    print(f\"Youden Index = {best_youden:.4f}\")\n",
    "    print(f\"Sensitivity  = {best_sen:.4f}\")\n",
    "    print(f\"Specificity  = {best_spec:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df,\n",
    "        \"BestModelPath\": best_overall_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataX = dataX.drop(columns=['Residence','Hypertension'])  \n",
    "cat_features = [c for c in features_categorical if c in dataX.columns]\n",
    "dataY = df_mapped_wide['Outcome']  \n",
    "# results = train_catboost_5fold_cv_best_save(dataX, dataY,cat_features=features_categorical)\n",
    "results = train_catboost_5fold_cv_best_save(dataX, dataY,cat_features=cat_features)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62643609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Probabilistic calibrator using sklearn ###\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import os\n",
    "\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "def train_catboost_5fold_cv_calibration(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    cat_features=None,\n",
    "    save_path='./Poisoning_Prediction/ML/predict_non-recovery_calibration/catboost/',\n",
    "    seed=42,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'depth': 5, 'iterations': 200, 'learning_rate': 0.05}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataX = dataX.copy()\n",
    "    if cat_features is not None:\n",
    "        for c in cat_features:\n",
    "            dataX[c] = dataX[c].astype(str).fillna(\"missing\")\n",
    "\n",
    "    X = dataX\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        # Step1: train_val / test\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # Step2: train / val\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        print(f\"Seed {seed}, Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "        print(f\"  Train - Pos: {np.sum(y_train == 1)}, Neg: {np.sum(y_train == 0)}\")\n",
    "        print(f\"  Val   - Pos: {np.sum(y_val == 1)},   Neg: {np.sum(y_val == 0)}\")\n",
    "        print(f\"  Test  - Pos: {np.sum(y_test == 1)},  Neg: {np.sum(y_test == 0)}\")\n",
    "        \n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "        print('scale_pos_weight:',scale_pos_weight)\n",
    "\n",
    "        # Pool\n",
    "        train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "        val_pool = Pool(X_val, label=y_val, cat_features=cat_features)\n",
    "        test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "        from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "        cat_model = CatBoostClassifier(\n",
    "            **params,\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        calib_model = CalibratedClassifierCV(\n",
    "            estimator=cat_model,\n",
    "            method='sigmoid',\n",
    "            cv='prefit'\n",
    "        )\n",
    "\n",
    "        cat_model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "        calib_model.fit(X_val, y_val)  \n",
    "        y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732bd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome']\n",
    "results = train_catboost_5fold_cv_calibration(dataX, dataY,cat_features=features_categorical)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc8cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdc78740",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e438df",
   "metadata": {},
   "source": [
    "Use the new fixed random seed number + the previous optimal hyperparameter (the same hyperparameter used to predict whether or not to die)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "def train_xgboost_5fold_cv_fixed(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    save_path='./Poisoning_Prediction/ML/predict_non-recovery/xgboost_fixed_valid_test_5cv/',\n",
    "    seed=42,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    X = dataX.copy()\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "    print(f\"\\n===== fixed params: {params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        # Step1: train_val / test\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # Step2: train / val\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        print(f\"Seed {seed}, Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "        print(f\"  Train - Pos: {np.sum(y_train == 1)}, Neg: {np.sum(y_train == 0)}\")\n",
    "        print(f\"  Val   - Pos: {np.sum(y_val == 1)},   Neg: {np.sum(y_val == 0)}\")\n",
    "        print(f\"  Test  - Pos: {np.sum(y_test == 1)},  Neg: {np.sum(y_test == 0)}\")\n",
    "\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "        dtest  = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        xgb_params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"auc\",\n",
    "            \"scale_pos_weight\": scale_pos_weight,\n",
    "            \"seed\": seed,\n",
    "            \"verbosity\": 0,\n",
    "            \"learning_rate\": params[\"learning_rate\"],\n",
    "            \"max_depth\": params[\"max_depth\"]\n",
    "        }\n",
    "\n",
    "        # early stopping\n",
    "        evals = [(dval, \"validation\")]\n",
    "        model = xgb.train(\n",
    "            xgb_params,\n",
    "            dtrain,\n",
    "            num_boost_round=params.get(\"n_estimators\", 200),\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        y_pred_prob = model.predict(dtest)\n",
    "\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "\n",
    "        fold_idx += 1\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb558f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome'] \n",
    "results = train_xgboost_5fold_cv_fixed(dataX, dataY)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe4c041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafdaffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fixed Hyperparameter + 5-fold XGBoost + Calibration (Training Set Fit + Validation Set Calibration)\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "\n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n",
    "\n",
    "def train_xgboost_5fold_cv_calibrated(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    save_path='./Poisoning_Prediction/ML/predict_non-recovery_calibration/xgboost/',\n",
    "    seed=42,\n",
    "    early_stopping_rounds=30,\n",
    "    params={'learning_rate':0.05, 'max_depth':5, 'n_estimators':200}\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    X = dataX.copy()\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    param_name = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n",
    "    param_path = os.path.join(save_path, param_name)\n",
    "    os.makedirs(param_path, exist_ok=True)\n",
    "    print(f\"\\n===== fixed params: {params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_results = []\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        # ---------------------- train_val / test ----------------------\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # train / val\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        print(f\"Fold={fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\")\n",
    "        print(f\"  Train - Pos: {np.sum(y_train==1)}, Neg: {np.sum(y_train==0)}\")\n",
    "        print(f\"  Val   - Pos: {np.sum(y_val==1)},   Neg: {np.sum(y_val==0)}\")\n",
    "        print(f\"  Test  - Pos: {np.sum(y_test==1)},  Neg: {np.sum(y_test==0)}\")\n",
    "\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos,1)\n",
    "\n",
    "        base_model = XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            learning_rate=params['learning_rate'],\n",
    "            max_depth=params['max_depth'],\n",
    "            n_estimators=params['n_estimators'],\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=seed,\n",
    "            verbosity=0,\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "\n",
    "        base_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            # early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        calib_model = CalibratedClassifierCV(\n",
    "            estimator=base_model,\n",
    "            method='sigmoid',\n",
    "            cv='prefit' \n",
    "        )\n",
    "        calib_model.fit(X_val, y_val)\n",
    "        y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        fold_csv = os.path.join(param_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "\n",
    "        fold_idx += 1\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(param_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score\n",
    "    )\n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score\n",
    "    )\n",
    "\n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"AUROC_mean\": mean_auroc,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": mean_auprc,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper),\n",
    "        \"AllResults\": all_results_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome'] \n",
    "results = train_xgboost_5fold_cv_calibrated(dataX, dataY)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fe5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62c00bf6",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d403e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "\n",
    "def train_randomforest_5fold_cv_fixed(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    save_path='./Poisoning_Prediction/ML/predict_non-recovery/randomforest_fixed_valid_test_5cv/',\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "        n_estimators=200\n",
    "        max_depth=5\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    X = dataX.copy()\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    fixed_params = {\n",
    "        \"n_estimators\": 200,\n",
    "        \"max_depth\": 5,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"min_samples_leaf\": 1,\n",
    "        \"max_features\": \"sqrt\"\n",
    "    }\n",
    "\n",
    "    print(f\"\\n===== fixed params: {fixed_params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    aurocs, auprcs = [], []\n",
    "    all_results = []\n",
    "\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        print(f\"\\n===== Fold {fold_idx} =====\")\n",
    "        # Step1: 80% train_val, 20% test\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        # Step2: train_val / val\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        # class weight\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "        class_weight = {0: 1.0, 1: scale_pos_weight}\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            class_weight=class_weight,\n",
    "            **fixed_params\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "        auroc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "        auprc = metrics.average_precision_score(y_test, y_pred_prob)\n",
    "        aurocs.append(auroc)\n",
    "        auprcs.append(auprc)\n",
    "\n",
    "        print(f\"[Fold {fold_idx}] AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "        fold_csv = os.path.join(save_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    def mean_ci_interval(data):\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data, ddof=1)\n",
    "        ci95 = 1.96 * std / np.sqrt(len(data))\n",
    "        return round(mean, 4), round(mean - ci95, 4), round(mean + ci95, 4)\n",
    "\n",
    "    auroc_mean, auroc_lower, auroc_upper = mean_ci_interval(aurocs)\n",
    "    auprc_mean, auprc_lower, auprc_upper = mean_ci_interval(auprcs)\n",
    "\n",
    "    print(f\"AUROC: Mean={auroc_mean:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={auprc_mean:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "\n",
    "    results_summary = {\n",
    "        \"AUROC_mean\": auroc_mean,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": auprc_mean,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper)\n",
    "    }\n",
    "\n",
    "    return results_summary, all_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome'] \n",
    "results = train_randomforest_5fold_cv_fixed(dataX, dataY)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8b513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### probability calibration ####\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import os\n",
    "\n",
    "def train_randomforest_5fold_cv_calibrated(\n",
    "    dataX,\n",
    "    dataY,\n",
    "    save_path='./Poisoning_Prediction/ML/predict_non-recovery_calibration/randomforest/',\n",
    "    seed=42\n",
    "):\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    X = dataX.copy()\n",
    "    y = np.array(dataY)\n",
    "\n",
    "    fixed_params = {\n",
    "        \"n_estimators\": 200,\n",
    "        \"max_depth\": 5,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"min_samples_leaf\": 1,\n",
    "        \"max_features\": \"sqrt\"\n",
    "    }\n",
    "\n",
    "    print(f\"\\n===== Use fixed parameters: {fixed_params} =====\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    aurocs, auprcs = [], []\n",
    "    all_results = []\n",
    "    fold_idx = 1\n",
    "    for train_val_index, test_index in kf.split(X):\n",
    "        print(f\"\\n===== Fold {fold_idx} =====\")\n",
    "        X_train_val, X_test = X.iloc[train_val_index], X.iloc[test_index]\n",
    "        y_train_val, y_test = y[train_val_index], y[test_index]\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        num_pos = np.sum(y_train == 1)\n",
    "        num_neg = np.sum(y_train == 0)\n",
    "        scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "        class_weight = {0: 1.0, 1: scale_pos_weight}\n",
    "\n",
    "        base_model = RandomForestClassifier(\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            class_weight=class_weight,\n",
    "            **fixed_params\n",
    "        )\n",
    "        base_model.fit(X_train, y_train)\n",
    "        calib_model = CalibratedClassifierCV(\n",
    "            estimator=base_model,\n",
    "            method='sigmoid',\n",
    "            cv='prefit' \n",
    "        )\n",
    "        calib_model.fit(X_val, y_val)\n",
    "        y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "        auroc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "        auprc = metrics.average_precision_score(y_test, y_pred_prob)\n",
    "        aurocs.append(auroc)\n",
    "        auprcs.append(auprc)\n",
    "        print(f\"[Fold {fold_idx}] AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "        fold_csv = os.path.join(save_path, f\"fold_{fold_idx}_results.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred_prob\n",
    "        }))\n",
    "        fold_idx += 1\n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_csv = os.path.join(save_path, \"all_folds_results.csv\")\n",
    "    all_results_df.to_csv(all_csv, index=False)\n",
    "\n",
    "    def mean_ci_interval(data):\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data, ddof=1)\n",
    "        ci95 = 1.96 * std / np.sqrt(len(data))\n",
    "        return round(mean, 4), round(mean - ci95, 4), round(mean + ci95, 4)\n",
    "    auroc_mean, auroc_lower, auroc_upper = mean_ci_interval(aurocs)\n",
    "    auprc_mean, auprc_lower, auprc_upper = mean_ci_interval(auprcs)\n",
    "    print(f\"AUROC: Mean={auroc_mean:.4f}, 95% CI=({auroc_lower:.4f},{auroc_upper:.4f})\")\n",
    "    print(f\"AUPRC: Mean={auprc_mean:.4f}, 95% CI=({auprc_lower:.4f},{auprc_upper:.4f})\")\n",
    "    results_summary = {\n",
    "        \"AUROC_mean\": auroc_mean,\n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper),\n",
    "        \"AUPRC_mean\": auprc_mean,\n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper)\n",
    "    }\n",
    "    return results_summary, all_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "196671d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome\n",
       "0    731\n",
       "1    240\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mapped_wide['Outcome'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome']\n",
    "results = train_randomforest_5fold_cv_calibrated(dataX, dataY)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfc6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f67d4849",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6404a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import os \n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "def bootstrap_metric_ci(y_true, y_pred, metric_fn, n_bootstrap=2000, seed=42): \n",
    "    rng = np.random.RandomState(seed) \n",
    "    scores = [] \n",
    "    y_true = np.array(y_true) \n",
    "    y_pred = np.array(y_pred) \n",
    "    for _ in range(n_bootstrap): \n",
    "        idx = rng.randint(0, len(y_true), len(y_true)) \n",
    "        if len(np.unique(y_true[idx])) < 2: \n",
    "            continue \n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx])) \n",
    "    return np.mean(scores), np.percentile(scores, 2.5), np.percentile(scores, 97.5) \n",
    " \n",
    "def train_lr_5fold_cv( \n",
    "    dataX, \n",
    "    dataY, \n",
    "    num_features, \n",
    "    cat_features, \n",
    "    save_path='./Poisoning_Prediction/ML/predict_non-recovery/lr_valid_test_5cv/',\n",
    "    seed=42 \n",
    "): \n",
    "    os.makedirs(save_path, exist_ok=True) \n",
    "    X = dataX.copy() \n",
    "    y = np.array(dataY) \n",
    " \n",
    "    # ===================== feature preprocessing ===================== \n",
    "    imputer = SimpleImputer(strategy='median') \n",
    "    X_num = pd.DataFrame(imputer.fit_transform(X[num_features]), columns=num_features, index=X.index) \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = pd.DataFrame(scaler.fit_transform(X_num), columns=num_features, index=X.index)\n",
    "\n",
    "    X_cat = X[cat_features].astype(str).fillna('missing')\n",
    "\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
    "    X_cat_encoded = pd.DataFrame(\n",
    "        encoder.fit_transform(X_cat),\n",
    "        columns=encoder.get_feature_names_out(cat_features),\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    X_processed = pd.concat([X_num_scaled, X_cat_encoded], axis=1)\n",
    "\n",
    "    print(f\"final feature dimension: {X_processed.shape[1]}\") \n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed) \n",
    "    all_results = [] \n",
    "    fold_idx = 1 \n",
    " \n",
    "    for train_val_index, test_index in kf.split(X_processed): \n",
    "        X_train_val, X_test = X_processed.iloc[train_val_index], X_processed.iloc[test_index] \n",
    "        y_train_val, y_test = y[train_val_index], y[test_index] \n",
    " \n",
    "        X_train, X_val, y_train, y_val = train_test_split( \n",
    "            X_train_val, y_train_val, \n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val \n",
    "        ) \n",
    " \n",
    "        print(f\"Fold {fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\") \n",
    "\n",
    "        n_pos = np.sum(y_train == 1)\n",
    "        n_neg = np.sum(y_train == 0)\n",
    "        print(f\"  Train - Pos: {n_pos}, Neg: {n_neg}\") \n",
    "        print(f\"  Test  - Pos: {np.sum(y_test==1)}, Neg: {np.sum(y_test==0)}\") \n",
    " \n",
    "        # ===================== manual setting class_weight ===================== \n",
    "        w_pos = n_neg / max(n_pos, 1)\n",
    "        class_weight = {0: 1.0, 1: w_pos}\n",
    "        print(f\"  use class_weight = {class_weight}\") \n",
    " \n",
    "        model = LogisticRegression(\n",
    "            max_iter=1000, \n",
    "            solver='lbfgs',\n",
    "            class_weight=class_weight\n",
    "        ) \n",
    "        # model = LogisticRegression(\n",
    "        #     max_iter=200, \n",
    "        #     solver='lbfgs',\n",
    "        #     class_weight=class_weight\n",
    "        # ) \n",
    " \n",
    "        model.fit(X_train, y_train) \n",
    "\n",
    "        y_pred_prob = model.predict_proba(X_test)[:, 1] \n",
    "\n",
    "        fold_csv = os.path.join(save_path, f\"fold_{fold_idx}_results.csv\") \n",
    "        pd.DataFrame({ \n",
    "            \"fold\": fold_idx, \n",
    "            \"y_test\": y_test, \n",
    "            \"y_pred\": y_pred_prob \n",
    "        }).to_csv(fold_csv, index=False) \n",
    " \n",
    "        all_results.append(pd.DataFrame({ \n",
    "            \"fold\": fold_idx, \n",
    "            \"y_test\": y_test, \n",
    "            \"y_pred\": y_pred_prob \n",
    "        })) \n",
    " \n",
    "        fold_idx += 1 \n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True) \n",
    "    all_csv = os.path.join(save_path, \"all_folds_results.csv\") \n",
    "    all_results_df.to_csv(all_csv, index=False) \n",
    "\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci( \n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score \n",
    "    ) \n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci( \n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score \n",
    "    ) \n",
    " \n",
    "    print(f\"\\n===== Logistic Regression 结果 =====\") \n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f}, {auroc_upper:.4f})\") \n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f}, {auprc_upper:.4f})\") \n",
    " \n",
    "    return { \n",
    "        \"AUROC_mean\": mean_auroc, \n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper), \n",
    "        \"AUPRC_mean\": mean_auprc, \n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper), \n",
    "        \"AllResults\": all_results_df \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome']\n",
    "results = train_lr_5fold_cv(dataX, dataY,num_features=features_continuous, cat_features=features_categorical)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf09be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf3df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calibrate models for probabilistic calibration\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import os \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def train_lr_5fold_cv_calibrated(\n",
    "    dataX, \n",
    "    dataY, \n",
    "    num_features, \n",
    "    cat_features, \n",
    "    save_path='./Poisoning_Prediction/ML/predict_non-recovery_calibration/lr/',\n",
    "    seed=42\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True) \n",
    "    X = dataX.copy() \n",
    "    y = np.array(dataY) \n",
    "\n",
    "    imputer = SimpleImputer(strategy='median') \n",
    "    X_num = pd.DataFrame(imputer.fit_transform(X[num_features]), columns=num_features, index=X.index) \n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = pd.DataFrame(scaler.fit_transform(X_num), columns=num_features, index=X.index)\n",
    "\n",
    "    X_cat = X[cat_features].astype(str).fillna('missing')\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
    "    X_cat_encoded = pd.DataFrame(\n",
    "        encoder.fit_transform(X_cat),\n",
    "        columns=encoder.get_feature_names_out(cat_features),\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    X_processed = pd.concat([X_num_scaled, X_cat_encoded], axis=1)\n",
    "    print(f\"final feature dimension: {X_processed.shape[1]}\") \n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed) \n",
    "    all_results = [] \n",
    "    fold_idx = 1 \n",
    "\n",
    "    for train_val_index, test_index in kf.split(X_processed): \n",
    "        X_train_val, X_test = X_processed.iloc[train_val_index], X_processed.iloc[test_index] \n",
    "        y_train_val, y_test = y[train_val_index], y[test_index] \n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split( \n",
    "            X_train_val, y_train_val, \n",
    "            test_size=1/8, random_state=seed, stratify=y_train_val \n",
    "        ) \n",
    "\n",
    "        print(f\"Fold {fold_idx}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}\") \n",
    "\n",
    "        n_pos = np.sum(y_train == 1)\n",
    "        n_neg = np.sum(y_train == 0)\n",
    "        class_weight = {0: 1.0, 1: n_neg / max(n_pos, 1)}\n",
    "        print(f\"  class_weight = {class_weight}\") \n",
    "\n",
    "        base_model = LogisticRegression(max_iter=1000, solver='lbfgs', class_weight=class_weight)\n",
    "        base_model.fit(X_train, y_train)\n",
    "\n",
    "        calib_model = CalibratedClassifierCV(base_model, method='sigmoid', cv='prefit')\n",
    "        calib_model.fit(X_val, y_val)\n",
    "\n",
    "        y_pred_prob = calib_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        fold_csv = os.path.join(save_path, f\"fold_{fold_idx}_results.csv\") \n",
    "        pd.DataFrame({\n",
    "            \"fold\": fold_idx, \n",
    "            \"y_test\": y_test, \n",
    "            \"y_pred\": y_pred_prob \n",
    "        }).to_csv(fold_csv, index=False)\n",
    "\n",
    "        all_results.append(pd.DataFrame({\n",
    "            \"fold\": fold_idx, \n",
    "            \"y_test\": y_test, \n",
    "            \"y_pred\": y_pred_prob \n",
    "        })) \n",
    "        fold_idx += 1 \n",
    "\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True) \n",
    "    all_csv = os.path.join(save_path, \"all_folds_results.csv\") \n",
    "    all_results_df.to_csv(all_csv, index=False) \n",
    "\n",
    "    mean_auroc, auroc_lower, auroc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.roc_auc_score \n",
    "    ) \n",
    "    mean_auprc, auprc_lower, auprc_upper = bootstrap_metric_ci(\n",
    "        all_results_df[\"y_test\"], all_results_df[\"y_pred\"], metrics.average_precision_score \n",
    "    ) \n",
    "\n",
    "    print(f\"\\n===== Logistic Regression =====\") \n",
    "    print(f\"AUROC: Mean={mean_auroc:.4f}, 95% CI=({auroc_lower:.4f}, {auroc_upper:.4f})\") \n",
    "    print(f\"AUPRC: Mean={mean_auprc:.4f}, 95% CI=({auprc_lower:.4f}, {auprc_upper:.4f})\") \n",
    "\n",
    "    return {\n",
    "        \"AUROC_mean\": mean_auroc, \n",
    "        \"AUROC_CI\": (auroc_lower, auroc_upper), \n",
    "        \"AUPRC_mean\": mean_auprc, \n",
    "        \"AUPRC_CI\": (auprc_lower, auprc_upper), \n",
    "        \"AllResults\": all_results_df \n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "125f89a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome\n",
       "0    731\n",
       "1    240\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mapped_wide['Outcome'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df_mapped_wide[features_categorical + features_continuous]\n",
    "dataY = df_mapped_wide['Outcome']\n",
    "results = train_lr_5fold_cv_calibrated(dataX, dataY,num_features=features_continuous, cat_features=features_categorical)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fa1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
